{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "import gym\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent and Environment Description\n",
    "\n",
    "* observation:  \n",
    "    (Cart Position ,  Cart Velocity ,  Pole Angle ,  Pole Velocity At Tip)\n",
    " \n",
    "* Actions:  \n",
    "    0 Push cart to the left; 1 Push cart to the right\n",
    "\n",
    "* Reward:  \n",
    "    Reward is for every step taken ,  including the termination step\n",
    "\n",
    "* Starting State:  \n",
    "    All observations are assigned a uniform random value between +-0.05\n",
    "\n",
    "* Episode Termination:  \n",
    "    Pole Angle is more than ±12°\n",
    "    Cart Position is more than ± 2.4 \n",
    "    (center of the cart reaches the edge of the display)\n",
    "    Episode length is greater than 200\n",
    "\n",
    "* reference:  \n",
    "    http:  //neuro-educator.com/rl1/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# input: (n x 4) ( Cart Position, Cart Vel, Pole Angle, Pole Vel)\n",
    "model.add(Flatten(input_shape=(1, env.observation_space.shape[0])))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# output: (n x 3) (left, no, right)\n",
    "model.add(Dense(env.action_space.n))\n",
    "model.add(Activation('linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_3 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2)                 34        \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 658\n",
      "Trainable params: 658\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory replay\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "\n",
    "# epsilon greedy algorithm\n",
    "policy = EpsGreedyQPolicy(eps=0.001)\n",
    "\n",
    "dqn = DQNAgent(model=model, nb_actions=env.action_space.n, gamma=0.99, memory=memory,\n",
    "              nb_steps_warmup=10, target_model_update=1e-2, policy=policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/syuntoku14/.pyenv/versions/anaconda3-5.1.0/envs/coursera/lib/python3.6/site-packages/rl/memory.py:29: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   141/100000: episode: 1, duration: 1.329s, episode steps: 141, steps per second: 106, episode reward: 141.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.496 [0.000, 1.000], mean observation: 0.118 [-0.627, 0.709], loss: 0.175437, mean_absolute_error: 0.412029, mean_q: 0.655545\n",
      "   200/100000: episode: 2, duration: 0.139s, episode steps: 59, steps per second: 423, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.100 [-0.746, 0.248], loss: 0.019771, mean_absolute_error: 0.600869, mean_q: 1.502709\n",
      "   262/100000: episode: 3, duration: 0.160s, episode steps: 62, steps per second: 387, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.001 [-0.860, 0.357], loss: 0.015196, mean_absolute_error: 0.884652, mean_q: 1.923544\n",
      "   291/100000: episode: 4, duration: 0.064s, episode steps: 29, steps per second: 454, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: -0.074 [-1.445, 0.615], loss: 0.012124, mean_absolute_error: 1.100889, mean_q: 2.246168\n",
      "   325/100000: episode: 5, duration: 0.073s, episode steps: 34, steps per second: 468, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.052 [-1.412, 0.648], loss: 0.015149, mean_absolute_error: 1.221427, mean_q: 2.442530\n",
      "   343/100000: episode: 6, duration: 0.040s, episode steps: 18, steps per second: 450, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.097 [-2.203, 1.157], loss: 0.018800, mean_absolute_error: 1.321335, mean_q: 2.642989\n",
      "   409/100000: episode: 7, duration: 0.157s, episode steps: 66, steps per second: 421, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.052 [-0.765, 1.129], loss: 0.028638, mean_absolute_error: 1.485823, mean_q: 2.971891\n",
      "   424/100000: episode: 8, duration: 0.091s, episode steps: 15, steps per second: 165, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.080 [-0.939, 1.385], loss: 0.047689, mean_absolute_error: 1.648128, mean_q: 3.235927\n",
      "   437/100000: episode: 9, duration: 0.049s, episode steps: 13, steps per second: 267, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.114 [-1.782, 2.894], loss: 0.054947, mean_absolute_error: 1.738796, mean_q: 3.399823\n",
      "   462/100000: episode: 10, duration: 0.057s, episode steps: 25, steps per second: 435, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.280 [0.000, 1.000], mean observation: 0.048 [-2.132, 3.284], loss: 0.059686, mean_absolute_error: 1.823024, mean_q: 3.602594\n",
      "   475/100000: episode: 11, duration: 0.044s, episode steps: 13, steps per second: 295, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.109 [-2.360, 1.418], loss: 0.172629, mean_absolute_error: 1.942946, mean_q: 3.759284\n",
      "   491/100000: episode: 12, duration: 0.036s, episode steps: 16, steps per second: 443, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.069 [-1.014, 1.481], loss: 0.141465, mean_absolute_error: 1.995574, mean_q: 3.860466\n",
      "   508/100000: episode: 13, duration: 0.038s, episode steps: 17, steps per second: 451, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.235 [0.000, 1.000], mean observation: 0.062 [-1.807, 2.798], loss: 0.113247, mean_absolute_error: 2.078580, mean_q: 4.044520\n",
      "   517/100000: episode: 14, duration: 0.020s, episode steps: 9, steps per second: 446, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.168 [-1.523, 2.482], loss: 0.116578, mean_absolute_error: 2.051282, mean_q: 3.980325\n",
      "   535/100000: episode: 15, duration: 0.043s, episode steps: 18, steps per second: 420, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.047 [-2.352, 3.418], loss: 0.207775, mean_absolute_error: 2.214707, mean_q: 4.279286\n",
      "   545/100000: episode: 16, duration: 0.022s, episode steps: 10, steps per second: 451, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.120 [-1.995, 3.019], loss: 0.119345, mean_absolute_error: 2.231929, mean_q: 4.380437\n",
      "   562/100000: episode: 17, duration: 0.037s, episode steps: 17, steps per second: 462, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.235 [0.000, 1.000], mean observation: 0.083 [-1.749, 2.824], loss: 0.300722, mean_absolute_error: 2.323840, mean_q: 4.484412\n",
      "   581/100000: episode: 18, duration: 0.058s, episode steps: 19, steps per second: 326, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.211 [0.000, 1.000], mean observation: 0.033 [-2.139, 3.088], loss: 0.192815, mean_absolute_error: 2.417932, mean_q: 4.657057\n",
      "   590/100000: episode: 19, duration: 0.021s, episode steps: 9, steps per second: 420, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.154 [-1.715, 2.744], loss: 0.152377, mean_absolute_error: 2.435453, mean_q: 4.796130\n",
      "   602/100000: episode: 20, duration: 0.028s, episode steps: 12, steps per second: 435, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.108 [-1.939, 2.953], loss: 0.221107, mean_absolute_error: 2.511200, mean_q: 4.947770\n",
      "   636/100000: episode: 21, duration: 0.079s, episode steps: 34, steps per second: 429, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.324 [0.000, 1.000], mean observation: 0.055 [-2.333, 3.447], loss: 0.296366, mean_absolute_error: 2.590683, mean_q: 4.972474\n",
      "   646/100000: episode: 22, duration: 0.024s, episode steps: 10, steps per second: 424, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.139 [-1.953, 3.039], loss: 0.344333, mean_absolute_error: 2.708354, mean_q: 5.242631\n",
      "   655/100000: episode: 23, duration: 0.021s, episode steps: 9, steps per second: 427, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.167 [-1.735, 2.891], loss: 0.268751, mean_absolute_error: 2.668949, mean_q: 5.201833\n",
      "   664/100000: episode: 24, duration: 0.021s, episode steps: 9, steps per second: 438, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.167 [-1.741, 2.830], loss: 0.164594, mean_absolute_error: 2.767761, mean_q: 5.432075\n",
      "   673/100000: episode: 25, duration: 0.036s, episode steps: 9, steps per second: 253, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.146 [-1.716, 2.807], loss: 0.330373, mean_absolute_error: 2.809637, mean_q: 5.450913\n",
      "   682/100000: episode: 26, duration: 0.038s, episode steps: 9, steps per second: 239, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.145 [-1.571, 2.494], loss: 0.297847, mean_absolute_error: 2.822865, mean_q: 5.468284\n",
      "   695/100000: episode: 27, duration: 0.030s, episode steps: 13, steps per second: 434, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.092 [-1.615, 2.508], loss: 0.297685, mean_absolute_error: 2.919373, mean_q: 5.662529\n",
      "   704/100000: episode: 28, duration: 0.021s, episode steps: 9, steps per second: 426, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.117 [-1.185, 1.860], loss: 0.477948, mean_absolute_error: 2.953923, mean_q: 5.555336\n",
      "   722/100000: episode: 29, duration: 0.042s, episode steps: 18, steps per second: 433, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.054 [-1.589, 2.256], loss: 0.324350, mean_absolute_error: 2.998573, mean_q: 5.746031\n",
      "   731/100000: episode: 30, duration: 0.022s, episode steps: 9, steps per second: 403, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.144 [-1.144, 1.821], loss: 0.327938, mean_absolute_error: 3.027209, mean_q: 5.835251\n",
      "   740/100000: episode: 31, duration: 0.033s, episode steps: 9, steps per second: 271, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.145 [-1.151, 1.940], loss: 0.209679, mean_absolute_error: 3.042327, mean_q: 5.975354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   749/100000: episode: 32, duration: 0.046s, episode steps: 9, steps per second: 194, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.123 [-1.209, 1.815], loss: 0.283052, mean_absolute_error: 3.096834, mean_q: 6.037265\n",
      "   758/100000: episode: 33, duration: 0.023s, episode steps: 9, steps per second: 383, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.146 [-1.136, 1.916], loss: 0.473171, mean_absolute_error: 3.142061, mean_q: 5.984330\n",
      "   768/100000: episode: 34, duration: 0.024s, episode steps: 10, steps per second: 421, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.128 [-1.350, 2.137], loss: 0.402748, mean_absolute_error: 3.182236, mean_q: 6.166202\n",
      "   777/100000: episode: 35, duration: 0.021s, episode steps: 9, steps per second: 425, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.168 [-1.752, 2.885], loss: 0.361621, mean_absolute_error: 3.168847, mean_q: 6.110140\n",
      "   786/100000: episode: 36, duration: 0.028s, episode steps: 9, steps per second: 319, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.148 [-1.729, 2.830], loss: 0.320714, mean_absolute_error: 3.206836, mean_q: 6.284680\n",
      "   796/100000: episode: 37, duration: 0.030s, episode steps: 10, steps per second: 337, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.120 [-1.788, 2.680], loss: 0.379619, mean_absolute_error: 3.270895, mean_q: 6.293075\n",
      "   805/100000: episode: 38, duration: 0.033s, episode steps: 9, steps per second: 270, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.170 [-1.750, 2.871], loss: 0.267644, mean_absolute_error: 3.244888, mean_q: 6.328799\n",
      "   814/100000: episode: 39, duration: 0.026s, episode steps: 9, steps per second: 343, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.162 [-1.781, 2.893], loss: 0.357770, mean_absolute_error: 3.285355, mean_q: 6.499798\n",
      "   824/100000: episode: 40, duration: 0.043s, episode steps: 10, steps per second: 233, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.117 [-1.980, 3.032], loss: 0.198080, mean_absolute_error: 3.278571, mean_q: 6.517842\n",
      "   833/100000: episode: 41, duration: 0.026s, episode steps: 9, steps per second: 351, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.150 [-1.712, 2.780], loss: 0.358703, mean_absolute_error: 3.345331, mean_q: 6.516614\n",
      "   842/100000: episode: 42, duration: 0.025s, episode steps: 9, steps per second: 362, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.148 [-1.749, 2.761], loss: 0.264867, mean_absolute_error: 3.386842, mean_q: 6.630331\n",
      "   852/100000: episode: 43, duration: 0.028s, episode steps: 10, steps per second: 356, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.139 [-1.992, 3.133], loss: 0.309118, mean_absolute_error: 3.430342, mean_q: 6.699877\n",
      "   862/100000: episode: 44, duration: 0.026s, episode steps: 10, steps per second: 385, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.142 [-1.970, 3.068], loss: 0.332213, mean_absolute_error: 3.371807, mean_q: 6.559846\n",
      "   872/100000: episode: 45, duration: 0.026s, episode steps: 10, steps per second: 386, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.135 [-1.911, 3.003], loss: 0.240107, mean_absolute_error: 3.465646, mean_q: 6.886004\n",
      "   882/100000: episode: 46, duration: 0.028s, episode steps: 10, steps per second: 351, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.125 [-1.954, 3.039], loss: 0.295845, mean_absolute_error: 3.436539, mean_q: 6.721217\n",
      "   898/100000: episode: 47, duration: 0.035s, episode steps: 16, steps per second: 453, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.096 [-1.576, 2.682], loss: 0.233093, mean_absolute_error: 3.492687, mean_q: 6.862580\n",
      "   908/100000: episode: 48, duration: 0.035s, episode steps: 10, steps per second: 286, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.142 [-1.993, 3.097], loss: 0.222275, mean_absolute_error: 3.547218, mean_q: 7.074161\n",
      "   918/100000: episode: 49, duration: 0.026s, episode steps: 10, steps per second: 382, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.143 [-1.991, 3.126], loss: 0.292112, mean_absolute_error: 3.581160, mean_q: 6.927766\n",
      "   928/100000: episode: 50, duration: 0.033s, episode steps: 10, steps per second: 306, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.116 [-1.972, 3.017], loss: 0.231486, mean_absolute_error: 3.634944, mean_q: 7.124898\n",
      "  1034/100000: episode: 51, duration: 0.289s, episode steps: 106, steps per second: 366, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.547 [0.000, 1.000], mean observation: 0.237 [-0.255, 1.809], loss: 0.193558, mean_absolute_error: 3.738039, mean_q: 7.336011\n",
      "  1076/100000: episode: 52, duration: 0.196s, episode steps: 42, steps per second: 215, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.095 [-0.642, 0.163], loss: 0.168168, mean_absolute_error: 4.006977, mean_q: 7.957038\n",
      "  1123/100000: episode: 53, duration: 0.261s, episode steps: 47, steps per second: 180, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.140 [-1.063, 0.357], loss: 0.298323, mean_absolute_error: 4.186881, mean_q: 8.281255\n",
      "  1195/100000: episode: 54, duration: 0.313s, episode steps: 72, steps per second: 230, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.205 [-1.117, 0.292], loss: 0.360946, mean_absolute_error: 4.431034, mean_q: 8.785374\n",
      "  1292/100000: episode: 55, duration: 0.227s, episode steps: 97, steps per second: 427, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.080 [-1.162, 0.846], loss: 0.376472, mean_absolute_error: 4.763579, mean_q: 9.435302\n",
      "  1363/100000: episode: 56, duration: 0.151s, episode steps: 71, steps per second: 470, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.215 [-1.042, 0.569], loss: 0.537254, mean_absolute_error: 5.054942, mean_q: 10.008756\n",
      "  1434/100000: episode: 57, duration: 0.198s, episode steps: 71, steps per second: 359, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.451 [0.000, 1.000], mean observation: -0.178 [-1.219, 0.265], loss: 0.359079, mean_absolute_error: 5.340862, mean_q: 10.678174\n",
      "  1601/100000: episode: 58, duration: 0.547s, episode steps: 167, steps per second: 305, episode reward: 167.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: 0.056 [-0.710, 0.427], loss: 0.435177, mean_absolute_error: 5.747879, mean_q: 11.523338\n",
      "  1716/100000: episode: 59, duration: 0.332s, episode steps: 115, steps per second: 347, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.248 [-1.314, 0.328], loss: 0.639374, mean_absolute_error: 6.299490, mean_q: 12.573858\n",
      "  1839/100000: episode: 60, duration: 0.376s, episode steps: 123, steps per second: 327, episode reward: 123.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.285 [-1.684, 0.260], loss: 0.441216, mean_absolute_error: 6.705626, mean_q: 13.514446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1986/100000: episode: 61, duration: 0.328s, episode steps: 147, steps per second: 448, episode reward: 147.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.322 [-1.960, 0.345], loss: 0.594766, mean_absolute_error: 7.157436, mean_q: 14.390363\n",
      "  2099/100000: episode: 62, duration: 0.366s, episode steps: 113, steps per second: 309, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.451 [0.000, 1.000], mean observation: -0.335 [-2.047, 0.308], loss: 0.636369, mean_absolute_error: 7.618752, mean_q: 15.343749\n",
      "  2233/100000: episode: 63, duration: 0.651s, episode steps: 134, steps per second: 206, episode reward: 134.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.368 [-1.884, 0.328], loss: 0.572585, mean_absolute_error: 8.075066, mean_q: 16.271688\n",
      "  2368/100000: episode: 64, duration: 0.308s, episode steps: 135, steps per second: 439, episode reward: 135.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.303 [-1.836, 0.316], loss: 0.806146, mean_absolute_error: 8.484375, mean_q: 17.086740\n",
      "  2487/100000: episode: 65, duration: 0.344s, episode steps: 119, steps per second: 346, episode reward: 119.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.454 [0.000, 1.000], mean observation: -0.343 [-1.975, 0.275], loss: 0.704692, mean_absolute_error: 8.793533, mean_q: 17.748688\n",
      "  2624/100000: episode: 66, duration: 0.319s, episode steps: 137, steps per second: 429, episode reward: 137.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.339 [-1.960, 0.328], loss: 0.924107, mean_absolute_error: 9.205802, mean_q: 18.583963\n",
      "  2738/100000: episode: 67, duration: 0.266s, episode steps: 114, steps per second: 429, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.339 [-1.805, 0.305], loss: 0.696003, mean_absolute_error: 9.618012, mean_q: 19.459343\n",
      "  2911/100000: episode: 68, duration: 0.652s, episode steps: 173, steps per second: 265, episode reward: 173.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.239 [-2.006, 0.274], loss: 0.606463, mean_absolute_error: 10.039804, mean_q: 20.370218\n",
      "  3085/100000: episode: 69, duration: 0.406s, episode steps: 174, steps per second: 429, episode reward: 174.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.277 [-1.840, 0.363], loss: 0.783241, mean_absolute_error: 10.553464, mean_q: 21.327065\n",
      "  3237/100000: episode: 70, duration: 0.420s, episode steps: 152, steps per second: 362, episode reward: 152.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.306 [-1.809, 0.354], loss: 0.839384, mean_absolute_error: 11.045082, mean_q: 22.417894\n",
      "  3388/100000: episode: 71, duration: 0.353s, episode steps: 151, steps per second: 427, episode reward: 151.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.326 [-2.005, 0.290], loss: 0.631634, mean_absolute_error: 11.535575, mean_q: 23.528870\n",
      "  3527/100000: episode: 72, duration: 0.365s, episode steps: 139, steps per second: 381, episode reward: 139.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.345 [-1.989, 0.320], loss: 0.920522, mean_absolute_error: 11.898634, mean_q: 24.198320\n",
      "  3651/100000: episode: 73, duration: 0.368s, episode steps: 124, steps per second: 337, episode reward: 124.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.355 [-1.855, 0.282], loss: 0.735043, mean_absolute_error: 12.213391, mean_q: 24.850868\n",
      "  3811/100000: episode: 74, duration: 0.363s, episode steps: 160, steps per second: 441, episode reward: 160.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.286 [-1.851, 0.279], loss: 0.668963, mean_absolute_error: 12.588678, mean_q: 25.678396\n",
      "  3946/100000: episode: 75, duration: 0.327s, episode steps: 135, steps per second: 413, episode reward: 135.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.332 [-2.004, 0.498], loss: 0.668009, mean_absolute_error: 13.010093, mean_q: 26.586939\n",
      "  4095/100000: episode: 76, duration: 0.337s, episode steps: 149, steps per second: 442, episode reward: 149.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.314 [-1.959, 0.333], loss: 0.528895, mean_absolute_error: 13.414968, mean_q: 27.407236\n",
      "  4243/100000: episode: 77, duration: 0.361s, episode steps: 148, steps per second: 410, episode reward: 148.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.322 [-1.864, 0.300], loss: 0.596297, mean_absolute_error: 13.761081, mean_q: 28.222982\n",
      "  4378/100000: episode: 78, duration: 0.442s, episode steps: 135, steps per second: 306, episode reward: 135.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.347 [-2.004, 0.276], loss: 0.578268, mean_absolute_error: 14.184237, mean_q: 29.054688\n",
      "  4503/100000: episode: 79, duration: 0.296s, episode steps: 125, steps per second: 422, episode reward: 125.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.377 [-2.045, 0.278], loss: 0.768041, mean_absolute_error: 14.484801, mean_q: 29.608210\n",
      "  4650/100000: episode: 80, duration: 0.326s, episode steps: 147, steps per second: 451, episode reward: 147.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.322 [-2.042, 0.332], loss: 0.758721, mean_absolute_error: 14.820829, mean_q: 30.283854\n",
      "  4794/100000: episode: 81, duration: 0.434s, episode steps: 144, steps per second: 332, episode reward: 144.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.309 [-1.790, 0.357], loss: 0.598010, mean_absolute_error: 15.081295, mean_q: 30.891951\n",
      "  4994/100000: episode: 82, duration: 0.485s, episode steps: 200, steps per second: 413, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.135 [-1.274, 0.390], loss: 0.625876, mean_absolute_error: 15.483782, mean_q: 31.781288\n",
      "  5130/100000: episode: 83, duration: 0.391s, episode steps: 136, steps per second: 347, episode reward: 136.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.323 [-1.828, 0.293], loss: 0.403245, mean_absolute_error: 15.974251, mean_q: 32.797554\n",
      "  5320/100000: episode: 84, duration: 0.705s, episode steps: 190, steps per second: 270, episode reward: 190.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.245 [-1.988, 0.398], loss: 0.603709, mean_absolute_error: 16.431108, mean_q: 33.705868\n",
      "  5437/100000: episode: 85, duration: 0.342s, episode steps: 117, steps per second: 342, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.401 [-1.993, 0.297], loss: 0.434005, mean_absolute_error: 16.611416, mean_q: 34.110336\n",
      "  5637/100000: episode: 86, duration: 0.471s, episode steps: 200, steps per second: 424, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.182 [-1.620, 0.404], loss: 0.553817, mean_absolute_error: 16.928255, mean_q: 34.780373\n",
      "  5763/100000: episode: 87, duration: 0.337s, episode steps: 126, steps per second: 374, episode reward: 126.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.371 [-1.854, 0.321], loss: 0.959831, mean_absolute_error: 17.359488, mean_q: 35.540012\n",
      "  5916/100000: episode: 88, duration: 0.332s, episode steps: 153, steps per second: 461, episode reward: 153.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.309 [-1.957, 0.311], loss: 0.615000, mean_absolute_error: 17.603075, mean_q: 36.055164\n",
      "  6079/100000: episode: 89, duration: 0.399s, episode steps: 163, steps per second: 408, episode reward: 163.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.271 [-2.011, 0.330], loss: 0.545282, mean_absolute_error: 17.850407, mean_q: 36.630196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6218/100000: episode: 90, duration: 0.343s, episode steps: 139, steps per second: 406, episode reward: 139.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.314 [-2.025, 0.277], loss: 0.426225, mean_absolute_error: 18.209860, mean_q: 37.432968\n",
      "  6340/100000: episode: 91, duration: 0.293s, episode steps: 122, steps per second: 416, episode reward: 122.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.365 [-1.833, 0.349], loss: 0.466377, mean_absolute_error: 18.513510, mean_q: 38.006458\n",
      "  6477/100000: episode: 92, duration: 0.299s, episode steps: 137, steps per second: 458, episode reward: 137.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.350 [-1.954, 0.318], loss: 0.814401, mean_absolute_error: 18.672670, mean_q: 38.293419\n",
      "  6677/100000: episode: 93, duration: 0.780s, episode steps: 200, steps per second: 257, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.227 [-2.015, 0.370], loss: 0.782395, mean_absolute_error: 18.897228, mean_q: 38.806847\n",
      "  6822/100000: episode: 94, duration: 0.342s, episode steps: 145, steps per second: 424, episode reward: 145.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.339 [-2.004, 0.359], loss: 0.586233, mean_absolute_error: 19.167330, mean_q: 39.356560\n",
      "  6960/100000: episode: 95, duration: 0.455s, episode steps: 138, steps per second: 303, episode reward: 138.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.428 [0.000, 1.000], mean observation: -0.427 [-3.716, 1.868], loss: 0.381492, mean_absolute_error: 19.466076, mean_q: 39.969288\n",
      "  7078/100000: episode: 96, duration: 0.260s, episode steps: 118, steps per second: 455, episode reward: 118.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.449 [0.000, 1.000], mean observation: -0.402 [-2.204, 0.304], loss: 0.581034, mean_absolute_error: 19.443996, mean_q: 39.937756\n",
      "  7207/100000: episode: 97, duration: 0.278s, episode steps: 129, steps per second: 465, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.361 [-1.976, 0.322], loss: 0.714044, mean_absolute_error: 19.595516, mean_q: 40.205532\n",
      "  7329/100000: episode: 98, duration: 0.384s, episode steps: 122, steps per second: 317, episode reward: 122.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.360 [-1.839, 0.297], loss: 0.336319, mean_absolute_error: 19.943079, mean_q: 40.933655\n",
      "  7460/100000: episode: 99, duration: 0.310s, episode steps: 131, steps per second: 422, episode reward: 131.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.348 [-2.052, 0.320], loss: 0.446023, mean_absolute_error: 20.062281, mean_q: 41.171040\n",
      "  7627/100000: episode: 100, duration: 0.361s, episode steps: 167, steps per second: 462, episode reward: 167.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.283 [-1.999, 0.318], loss: 0.415494, mean_absolute_error: 20.146017, mean_q: 41.383427\n",
      "  7793/100000: episode: 101, duration: 0.428s, episode steps: 166, steps per second: 388, episode reward: 166.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.314 [-2.169, 0.368], loss: 0.597349, mean_absolute_error: 20.542902, mean_q: 42.221470\n",
      "  7923/100000: episode: 102, duration: 0.282s, episode steps: 130, steps per second: 461, episode reward: 130.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.361 [-1.850, 0.305], loss: 0.581397, mean_absolute_error: 20.692938, mean_q: 42.524628\n",
      "  8086/100000: episode: 103, duration: 0.362s, episode steps: 163, steps per second: 451, episode reward: 163.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.295 [-2.047, 0.459], loss: 0.835914, mean_absolute_error: 20.986904, mean_q: 43.108685\n",
      "  8224/100000: episode: 104, duration: 0.331s, episode steps: 138, steps per second: 417, episode reward: 138.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.330 [-1.840, 0.331], loss: 0.774881, mean_absolute_error: 21.352585, mean_q: 43.731258\n",
      "  8357/100000: episode: 105, duration: 0.295s, episode steps: 133, steps per second: 450, episode reward: 133.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.362 [-2.020, 0.321], loss: 0.586155, mean_absolute_error: 21.551439, mean_q: 44.231651\n",
      "  8480/100000: episode: 106, duration: 0.270s, episode steps: 123, steps per second: 455, episode reward: 123.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.387 [-1.984, 0.339], loss: 0.669173, mean_absolute_error: 21.306255, mean_q: 43.815197\n",
      "  8627/100000: episode: 107, duration: 0.538s, episode steps: 147, steps per second: 273, episode reward: 147.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.336 [-1.996, 0.322], loss: 0.568933, mean_absolute_error: 21.522354, mean_q: 44.151459\n",
      "  8785/100000: episode: 108, duration: 0.415s, episode steps: 158, steps per second: 380, episode reward: 158.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.311 [-2.143, 0.319], loss: 0.680653, mean_absolute_error: 21.436733, mean_q: 44.003784\n",
      "  8913/100000: episode: 109, duration: 0.439s, episode steps: 128, steps per second: 292, episode reward: 128.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.372 [-2.189, 0.305], loss: 0.529524, mean_absolute_error: 22.039211, mean_q: 45.220406\n",
      "  9090/100000: episode: 110, duration: 0.400s, episode steps: 177, steps per second: 442, episode reward: 177.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.446 [0.000, 1.000], mean observation: -0.339 [-3.539, 1.439], loss: 0.446637, mean_absolute_error: 22.108585, mean_q: 45.365818\n",
      "  9213/100000: episode: 111, duration: 0.283s, episode steps: 123, steps per second: 434, episode reward: 123.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.408 [-2.330, 0.412], loss: 0.437538, mean_absolute_error: 22.168425, mean_q: 45.453476\n",
      "  9371/100000: episode: 112, duration: 0.443s, episode steps: 158, steps per second: 357, episode reward: 158.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.328 [-2.210, 0.292], loss: 0.517818, mean_absolute_error: 22.467712, mean_q: 45.996407\n",
      "  9521/100000: episode: 113, duration: 0.327s, episode steps: 150, steps per second: 458, episode reward: 150.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.349 [-2.183, 0.392], loss: 0.560872, mean_absolute_error: 22.453381, mean_q: 45.898983\n",
      "  9663/100000: episode: 114, duration: 0.402s, episode steps: 142, steps per second: 353, episode reward: 142.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.430 [0.000, 1.000], mean observation: -0.419 [-3.672, 1.692], loss: 0.548334, mean_absolute_error: 22.589859, mean_q: 46.160923\n",
      "  9805/100000: episode: 115, duration: 0.628s, episode steps: 142, steps per second: 226, episode reward: 142.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.344 [-2.193, 0.380], loss: 0.573767, mean_absolute_error: 22.650549, mean_q: 46.339329\n",
      "  9927/100000: episode: 116, duration: 0.471s, episode steps: 122, steps per second: 259, episode reward: 122.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.451 [0.000, 1.000], mean observation: -0.426 [-2.199, 0.373], loss: 0.656651, mean_absolute_error: 22.950054, mean_q: 46.962955\n",
      " 10054/100000: episode: 117, duration: 0.331s, episode steps: 127, steps per second: 384, episode reward: 127.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.404 [-2.048, 0.341], loss: 0.660455, mean_absolute_error: 22.816370, mean_q: 46.788738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10190/100000: episode: 118, duration: 0.362s, episode steps: 136, steps per second: 375, episode reward: 136.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.388 [-2.158, 0.294], loss: 0.668896, mean_absolute_error: 22.953077, mean_q: 47.008106\n",
      " 10318/100000: episode: 119, duration: 0.343s, episode steps: 128, steps per second: 373, episode reward: 128.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.387 [-2.193, 0.343], loss: 0.530298, mean_absolute_error: 22.970858, mean_q: 47.062469\n",
      " 10476/100000: episode: 120, duration: 0.363s, episode steps: 158, steps per second: 435, episode reward: 158.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.329 [-2.165, 0.325], loss: 0.646032, mean_absolute_error: 23.374964, mean_q: 47.897354\n",
      " 10639/100000: episode: 121, duration: 0.435s, episode steps: 163, steps per second: 374, episode reward: 163.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.313 [-2.017, 0.288], loss: 0.645347, mean_absolute_error: 23.460737, mean_q: 48.072006\n",
      " 10772/100000: episode: 122, duration: 0.382s, episode steps: 133, steps per second: 349, episode reward: 133.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.386 [-2.043, 0.366], loss: 0.635215, mean_absolute_error: 23.734779, mean_q: 48.539181\n",
      " 10929/100000: episode: 123, duration: 0.338s, episode steps: 157, steps per second: 465, episode reward: 157.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.333 [-2.037, 0.354], loss: 0.895703, mean_absolute_error: 23.470823, mean_q: 47.926365\n",
      " 11083/100000: episode: 124, duration: 0.382s, episode steps: 154, steps per second: 403, episode reward: 154.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.461 [0.000, 1.000], mean observation: -0.353 [-2.154, 0.299], loss: 0.508156, mean_absolute_error: 23.583923, mean_q: 48.272461\n",
      " 11220/100000: episode: 125, duration: 0.296s, episode steps: 137, steps per second: 463, episode reward: 137.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.352 [-1.993, 0.347], loss: 0.885629, mean_absolute_error: 23.848545, mean_q: 48.774910\n",
      " 11385/100000: episode: 126, duration: 0.353s, episode steps: 165, steps per second: 467, episode reward: 165.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.295 [-1.952, 0.435], loss: 0.442086, mean_absolute_error: 23.935349, mean_q: 48.875149\n",
      " 11555/100000: episode: 127, duration: 0.444s, episode steps: 170, steps per second: 383, episode reward: 170.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.283 [-2.195, 0.378], loss: 0.509558, mean_absolute_error: 24.144480, mean_q: 49.297031\n",
      " 11688/100000: episode: 128, duration: 0.293s, episode steps: 133, steps per second: 453, episode reward: 133.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.355 [-1.965, 0.307], loss: 0.719851, mean_absolute_error: 24.144018, mean_q: 49.298176\n",
      " 11853/100000: episode: 129, duration: 0.363s, episode steps: 165, steps per second: 454, episode reward: 165.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.295 [-1.964, 0.340], loss: 0.810048, mean_absolute_error: 24.232517, mean_q: 49.410267\n",
      " 12032/100000: episode: 130, duration: 0.423s, episode steps: 179, steps per second: 423, episode reward: 179.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.275 [-1.994, 0.391], loss: 1.038259, mean_absolute_error: 24.391165, mean_q: 49.745472\n",
      " 12185/100000: episode: 131, duration: 0.328s, episode steps: 153, steps per second: 467, episode reward: 153.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.321 [-2.004, 0.312], loss: 0.610873, mean_absolute_error: 24.238247, mean_q: 49.479351\n",
      " 12352/100000: episode: 132, duration: 0.412s, episode steps: 167, steps per second: 406, episode reward: 167.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.298 [-2.004, 0.317], loss: 0.515381, mean_absolute_error: 24.524763, mean_q: 50.128651\n",
      " 12519/100000: episode: 133, duration: 0.368s, episode steps: 167, steps per second: 454, episode reward: 167.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.304 [-2.014, 0.326], loss: 0.617523, mean_absolute_error: 24.645918, mean_q: 50.371750\n",
      " 12656/100000: episode: 134, duration: 0.359s, episode steps: 137, steps per second: 382, episode reward: 137.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.362 [-2.025, 0.380], loss: 0.768337, mean_absolute_error: 24.349064, mean_q: 49.777653\n",
      " 12809/100000: episode: 135, duration: 0.362s, episode steps: 153, steps per second: 422, episode reward: 153.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.309 [-2.023, 0.363], loss: 0.691185, mean_absolute_error: 24.826654, mean_q: 50.794464\n",
      " 12954/100000: episode: 136, duration: 0.311s, episode steps: 145, steps per second: 466, episode reward: 145.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.317 [-1.957, 0.303], loss: 0.414553, mean_absolute_error: 24.713516, mean_q: 50.588799\n",
      " 13100/100000: episode: 137, duration: 0.348s, episode steps: 146, steps per second: 420, episode reward: 146.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.305 [-1.821, 0.303], loss: 0.627083, mean_absolute_error: 24.719830, mean_q: 50.598499\n",
      " 13300/100000: episode: 138, duration: 0.452s, episode steps: 200, steps per second: 443, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.080 [-1.096, 0.394], loss: 0.530847, mean_absolute_error: 25.212784, mean_q: 51.603939\n",
      " 13426/100000: episode: 139, duration: 0.272s, episode steps: 126, steps per second: 464, episode reward: 126.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.353 [-1.866, 0.380], loss: 0.644122, mean_absolute_error: 24.897558, mean_q: 50.979675\n",
      " 13566/100000: episode: 140, duration: 0.304s, episode steps: 140, steps per second: 460, episode reward: 140.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.327 [-1.869, 0.280], loss: 0.631973, mean_absolute_error: 25.060759, mean_q: 51.310265\n",
      " 13766/100000: episode: 141, duration: 0.461s, episode steps: 200, steps per second: 433, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.022 [-0.384, 0.294], loss: 0.553153, mean_absolute_error: 24.939156, mean_q: 51.023643\n",
      " 13910/100000: episode: 142, duration: 0.311s, episode steps: 144, steps per second: 463, episode reward: 144.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.300 [-1.785, 0.367], loss: 0.551925, mean_absolute_error: 25.627096, mean_q: 52.395313\n",
      " 14089/100000: episode: 143, duration: 0.408s, episode steps: 179, steps per second: 439, episode reward: 179.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.247 [-1.849, 0.357], loss: 0.437281, mean_absolute_error: 25.641563, mean_q: 52.379322\n",
      " 14275/100000: episode: 144, duration: 0.405s, episode steps: 186, steps per second: 460, episode reward: 186.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.233 [-1.795, 0.353], loss: 0.471117, mean_absolute_error: 25.494129, mean_q: 52.193531\n",
      " 14424/100000: episode: 145, duration: 0.325s, episode steps: 149, steps per second: 458, episode reward: 149.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.316 [-1.999, 0.278], loss: 1.223191, mean_absolute_error: 25.739906, mean_q: 52.491234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 14555/100000: episode: 146, duration: 0.308s, episode steps: 131, steps per second: 426, episode reward: 131.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.327 [-1.999, 0.289], loss: 0.927416, mean_absolute_error: 25.894117, mean_q: 52.936279\n",
      " 14700/100000: episode: 147, duration: 0.313s, episode steps: 145, steps per second: 464, episode reward: 145.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.316 [-1.973, 0.287], loss: 0.385359, mean_absolute_error: 25.539463, mean_q: 52.252735\n",
      " 14838/100000: episode: 148, duration: 0.297s, episode steps: 138, steps per second: 465, episode reward: 138.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.323 [-1.792, 0.324], loss: 0.366012, mean_absolute_error: 26.013453, mean_q: 53.160610\n",
      " 14965/100000: episode: 149, duration: 0.356s, episode steps: 127, steps per second: 357, episode reward: 127.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.357 [-1.983, 0.309], loss: 0.659520, mean_absolute_error: 25.666428, mean_q: 52.376461\n",
      " 15132/100000: episode: 150, duration: 0.408s, episode steps: 167, steps per second: 409, episode reward: 167.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.267 [-1.986, 0.317], loss: 0.529412, mean_absolute_error: 25.866287, mean_q: 52.753006\n",
      " 15323/100000: episode: 151, duration: 0.408s, episode steps: 191, steps per second: 468, episode reward: 191.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.236 [-2.004, 0.381], loss: 0.632236, mean_absolute_error: 25.877762, mean_q: 52.796543\n",
      " 15462/100000: episode: 152, duration: 0.333s, episode steps: 139, steps per second: 418, episode reward: 139.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.339 [-1.991, 0.299], loss: 0.601971, mean_absolute_error: 25.971933, mean_q: 52.929379\n",
      " 15596/100000: episode: 153, duration: 0.287s, episode steps: 134, steps per second: 467, episode reward: 134.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.326 [-1.847, 0.273], loss: 0.452138, mean_absolute_error: 26.246956, mean_q: 53.505348\n",
      " 15759/100000: episode: 154, duration: 0.347s, episode steps: 163, steps per second: 470, episode reward: 163.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.277 [-1.839, 0.370], loss: 0.346676, mean_absolute_error: 26.216055, mean_q: 53.539555\n",
      " 15919/100000: episode: 155, duration: 0.375s, episode steps: 160, steps per second: 426, episode reward: 160.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.292 [-2.021, 0.403], loss: 1.128078, mean_absolute_error: 25.957336, mean_q: 52.961060\n",
      " 16072/100000: episode: 156, duration: 0.326s, episode steps: 153, steps per second: 470, episode reward: 153.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.302 [-1.989, 0.264], loss: 0.501078, mean_absolute_error: 25.892685, mean_q: 52.892765\n",
      " 16205/100000: episode: 157, duration: 0.285s, episode steps: 133, steps per second: 467, episode reward: 133.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.331 [-2.003, 0.290], loss: 0.289871, mean_absolute_error: 26.050228, mean_q: 53.214928\n",
      " 16387/100000: episode: 158, duration: 0.511s, episode steps: 182, steps per second: 356, episode reward: 182.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.249 [-1.847, 0.362], loss: 0.877126, mean_absolute_error: 26.288107, mean_q: 53.670017\n",
      " 16521/100000: episode: 159, duration: 0.317s, episode steps: 134, steps per second: 422, episode reward: 134.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.326 [-1.837, 0.295], loss: 0.721143, mean_absolute_error: 25.921803, mean_q: 52.850250\n",
      " 16714/100000: episode: 160, duration: 0.502s, episode steps: 193, steps per second: 385, episode reward: 193.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.226 [-1.799, 0.409], loss: 0.304548, mean_absolute_error: 26.066669, mean_q: 53.221069\n",
      " 16856/100000: episode: 161, duration: 0.330s, episode steps: 142, steps per second: 431, episode reward: 142.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.319 [-1.970, 0.334], loss: 0.354744, mean_absolute_error: 25.925005, mean_q: 52.908394\n",
      " 16998/100000: episode: 162, duration: 0.303s, episode steps: 142, steps per second: 468, episode reward: 142.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.332 [-2.025, 0.293], loss: 0.468075, mean_absolute_error: 26.416012, mean_q: 53.846653\n",
      " 17189/100000: episode: 163, duration: 0.463s, episode steps: 191, steps per second: 413, episode reward: 191.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.238 [-1.983, 0.342], loss: 1.097912, mean_absolute_error: 26.256466, mean_q: 53.477634\n",
      " 17341/100000: episode: 164, duration: 0.323s, episode steps: 152, steps per second: 471, episode reward: 152.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.296 [-1.835, 0.279], loss: 0.626804, mean_absolute_error: 26.106272, mean_q: 53.276478\n",
      " 17512/100000: episode: 165, duration: 0.374s, episode steps: 171, steps per second: 458, episode reward: 171.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.265 [-1.964, 0.303], loss: 0.418773, mean_absolute_error: 26.377569, mean_q: 53.780952\n",
      " 17712/100000: episode: 166, duration: 0.468s, episode steps: 200, steps per second: 427, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.257 [-2.942, 1.138], loss: 1.210324, mean_absolute_error: 26.335508, mean_q: 53.648647\n",
      " 17885/100000: episode: 167, duration: 0.369s, episode steps: 173, steps per second: 468, episode reward: 173.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.434 [0.000, 1.000], mean observation: -0.325 [-4.295, 2.938], loss: 0.318787, mean_absolute_error: 26.296785, mean_q: 53.645294\n",
      " 18039/100000: episode: 168, duration: 0.388s, episode steps: 154, steps per second: 397, episode reward: 154.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: -0.360 [-4.124, 2.766], loss: 1.492413, mean_absolute_error: 26.134523, mean_q: 53.298161\n",
      " 18204/100000: episode: 169, duration: 0.383s, episode steps: 165, steps per second: 431, episode reward: 165.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.430 [0.000, 1.000], mean observation: -0.334 [-4.334, 3.104], loss: 1.242567, mean_absolute_error: 26.035290, mean_q: 52.927532\n",
      " 18383/100000: episode: 170, duration: 0.469s, episode steps: 179, steps per second: 381, episode reward: 179.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.436 [0.000, 1.000], mean observation: -0.307 [-4.298, 3.073], loss: 0.461449, mean_absolute_error: 26.159857, mean_q: 53.330078\n",
      " 18536/100000: episode: 171, duration: 0.562s, episode steps: 153, steps per second: 272, episode reward: 153.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.425 [0.000, 1.000], mean observation: -0.353 [-4.377, 3.348], loss: 0.273042, mean_absolute_error: 26.225229, mean_q: 53.516560\n",
      " 18712/100000: episode: 172, duration: 0.606s, episode steps: 176, steps per second: 290, episode reward: 176.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.318 [-4.099, 2.633], loss: 0.886678, mean_absolute_error: 26.291248, mean_q: 53.571018\n",
      " 18882/100000: episode: 173, duration: 0.623s, episode steps: 170, steps per second: 273, episode reward: 170.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: -0.321 [-4.477, 3.450], loss: 1.110922, mean_absolute_error: 26.222502, mean_q: 53.339245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 19073/100000: episode: 174, duration: 0.438s, episode steps: 191, steps per second: 436, episode reward: 191.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: -0.292 [-4.315, 3.102], loss: 0.322283, mean_absolute_error: 26.429720, mean_q: 53.775871\n",
      " 19273/100000: episode: 175, duration: 0.436s, episode steps: 200, steps per second: 458, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.076 [-0.408, 0.421], loss: 0.501130, mean_absolute_error: 26.493887, mean_q: 53.803467\n",
      " 19420/100000: episode: 176, duration: 0.345s, episode steps: 147, steps per second: 426, episode reward: 147.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.422 [0.000, 1.000], mean observation: -0.365 [-4.283, 3.130], loss: 1.263686, mean_absolute_error: 26.689672, mean_q: 54.162323\n",
      " 19620/100000: episode: 177, duration: 0.430s, episode steps: 200, steps per second: 465, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.074 [-0.892, 0.411], loss: 1.005968, mean_absolute_error: 26.725046, mean_q: 54.238018\n",
      " 19820/100000: episode: 178, duration: 0.425s, episode steps: 200, steps per second: 471, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.097 [-1.064, 0.301], loss: 0.433877, mean_absolute_error: 26.925566, mean_q: 54.687096\n",
      " 20020/100000: episode: 179, duration: 0.516s, episode steps: 200, steps per second: 387, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.236 [-2.570, 0.784], loss: 0.837064, mean_absolute_error: 27.002802, mean_q: 54.862972\n",
      " 20176/100000: episode: 180, duration: 0.345s, episode steps: 156, steps per second: 452, episode reward: 156.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: -0.357 [-4.145, 2.915], loss: 0.748170, mean_absolute_error: 27.013597, mean_q: 54.876152\n",
      " 20376/100000: episode: 181, duration: 0.455s, episode steps: 200, steps per second: 440, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.169 [-1.485, 0.303], loss: 0.236268, mean_absolute_error: 27.133858, mean_q: 55.156525\n",
      " 20576/100000: episode: 182, duration: 0.432s, episode steps: 200, steps per second: 463, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.175 [-1.483, 0.266], loss: 0.350829, mean_absolute_error: 27.141655, mean_q: 55.132919\n",
      " 20776/100000: episode: 183, duration: 0.475s, episode steps: 200, steps per second: 421, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.101 [-1.144, 0.459], loss: 1.050869, mean_absolute_error: 27.073851, mean_q: 54.900429\n",
      " 20976/100000: episode: 184, duration: 0.427s, episode steps: 200, steps per second: 468, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.044 [-0.551, 0.284], loss: 0.349954, mean_absolute_error: 27.409731, mean_q: 55.615147\n",
      " 21173/100000: episode: 185, duration: 0.446s, episode steps: 197, steps per second: 442, episode reward: 197.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: -0.272 [-4.379, 3.619], loss: 1.269836, mean_absolute_error: 27.793310, mean_q: 56.341049\n",
      " 21322/100000: episode: 186, duration: 0.343s, episode steps: 149, steps per second: 435, episode reward: 149.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.423 [0.000, 1.000], mean observation: -0.336 [-4.364, 3.668], loss: 0.827027, mean_absolute_error: 27.448977, mean_q: 55.519333\n",
      " 21495/100000: episode: 187, duration: 0.375s, episode steps: 173, steps per second: 461, episode reward: 173.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.434 [0.000, 1.000], mean observation: -0.289 [-4.389, 3.735], loss: 0.619932, mean_absolute_error: 27.386597, mean_q: 55.481583\n",
      " 21695/100000: episode: 188, duration: 0.467s, episode steps: 200, steps per second: 429, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.130 [-1.232, 0.395], loss: 1.416221, mean_absolute_error: 27.349859, mean_q: 55.478916\n",
      " 21836/100000: episode: 189, duration: 0.309s, episode steps: 141, steps per second: 457, episode reward: 141.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.418 [0.000, 1.000], mean observation: -0.354 [-4.383, 3.741], loss: 0.341308, mean_absolute_error: 27.481615, mean_q: 55.851269\n",
      " 22036/100000: episode: 190, duration: 0.443s, episode steps: 200, steps per second: 452, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.071 [-0.420, 0.425], loss: 0.755303, mean_absolute_error: 27.466507, mean_q: 55.723801\n",
      " 22224/100000: episode: 191, duration: 0.413s, episode steps: 188, steps per second: 456, episode reward: 188.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.441 [0.000, 1.000], mean observation: -0.273 [-4.189, 3.449], loss: 0.411749, mean_absolute_error: 27.735756, mean_q: 56.308159\n",
      " 22408/100000: episode: 192, duration: 0.397s, episode steps: 184, steps per second: 463, episode reward: 184.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: -0.276 [-4.200, 3.470], loss: 1.453213, mean_absolute_error: 27.599058, mean_q: 55.996231\n",
      " 22559/100000: episode: 193, duration: 0.387s, episode steps: 151, steps per second: 390, episode reward: 151.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.424 [0.000, 1.000], mean observation: -0.335 [-4.373, 3.726], loss: 0.854504, mean_absolute_error: 27.677771, mean_q: 56.034767\n",
      " 22759/100000: episode: 194, duration: 0.436s, episode steps: 200, steps per second: 459, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.235 [-2.960, 1.533], loss: 0.449425, mean_absolute_error: 27.460665, mean_q: 55.681419\n",
      " 22959/100000: episode: 195, duration: 0.452s, episode steps: 200, steps per second: 442, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.107 [-0.359, 0.654], loss: 1.333455, mean_absolute_error: 27.807833, mean_q: 56.336758\n",
      " 23159/100000: episode: 196, duration: 0.433s, episode steps: 200, steps per second: 462, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.034 [-0.395, 0.281], loss: 1.148169, mean_absolute_error: 27.858252, mean_q: 56.460728\n",
      " 23359/100000: episode: 197, duration: 0.431s, episode steps: 200, steps per second: 465, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.035 [-0.331, 0.343], loss: 1.363251, mean_absolute_error: 27.886114, mean_q: 56.478867\n",
      " 23503/100000: episode: 198, duration: 0.347s, episode steps: 144, steps per second: 415, episode reward: 144.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.424 [0.000, 1.000], mean observation: -0.344 [-4.196, 3.424], loss: 0.661091, mean_absolute_error: 28.065224, mean_q: 56.831264\n",
      " 23703/100000: episode: 199, duration: 0.431s, episode steps: 200, steps per second: 465, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.176 [-1.634, 0.343], loss: 1.837591, mean_absolute_error: 28.309301, mean_q: 57.324982\n",
      " 23902/100000: episode: 200, duration: 0.445s, episode steps: 199, steps per second: 448, episode reward: 199.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: -0.259 [-4.320, 3.561], loss: 0.804306, mean_absolute_error: 28.260208, mean_q: 57.307846\n",
      " 24076/100000: episode: 201, duration: 0.500s, episode steps: 174, steps per second: 348, episode reward: 174.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.437 [0.000, 1.000], mean observation: -0.321 [-4.112, 2.862], loss: 1.120226, mean_absolute_error: 28.026991, mean_q: 56.772945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 24269/100000: episode: 202, duration: 0.456s, episode steps: 193, steps per second: 423, episode reward: 193.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: -0.286 [-4.312, 3.365], loss: 0.465955, mean_absolute_error: 28.215153, mean_q: 57.171856\n",
      " 24469/100000: episode: 203, duration: 0.454s, episode steps: 200, steps per second: 440, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.094 [-0.373, 0.557], loss: 1.297290, mean_absolute_error: 28.407400, mean_q: 57.516602\n",
      " 24669/100000: episode: 204, duration: 0.462s, episode steps: 200, steps per second: 433, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.160 [-1.417, 0.291], loss: 1.055286, mean_absolute_error: 28.201460, mean_q: 57.109650\n",
      " 24859/100000: episode: 205, duration: 0.434s, episode steps: 190, steps per second: 438, episode reward: 190.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.275 [-1.969, 0.370], loss: 2.692237, mean_absolute_error: 28.504519, mean_q: 57.596191\n",
      " 25059/100000: episode: 206, duration: 0.498s, episode steps: 200, steps per second: 402, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.212 [-1.870, 0.321], loss: 1.411787, mean_absolute_error: 28.391592, mean_q: 57.416298\n",
      " 25259/100000: episode: 207, duration: 0.527s, episode steps: 200, steps per second: 379, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.128 [-1.133, 0.312], loss: 0.314423, mean_absolute_error: 28.004541, mean_q: 56.767391\n",
      " 25423/100000: episode: 208, duration: 0.353s, episode steps: 164, steps per second: 465, episode reward: 164.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.309 [-2.018, 0.373], loss: 1.213992, mean_absolute_error: 28.293541, mean_q: 57.295357\n",
      " 25623/100000: episode: 209, duration: 0.528s, episode steps: 200, steps per second: 379, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.089 [-0.412, 0.520], loss: 1.348620, mean_absolute_error: 28.311821, mean_q: 57.318974\n",
      " 25774/100000: episode: 210, duration: 0.328s, episode steps: 151, steps per second: 460, episode reward: 151.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.328 [-2.029, 0.340], loss: 0.485251, mean_absolute_error: 28.565552, mean_q: 57.830814\n",
      " 25974/100000: episode: 211, duration: 0.438s, episode steps: 200, steps per second: 457, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.159 [-1.411, 0.480], loss: 1.465930, mean_absolute_error: 28.595581, mean_q: 57.954777\n",
      " 26174/100000: episode: 212, duration: 0.480s, episode steps: 200, steps per second: 416, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.146 [-1.244, 0.308], loss: 0.524822, mean_absolute_error: 28.422764, mean_q: 57.648361\n",
      " 26374/100000: episode: 213, duration: 0.465s, episode steps: 200, steps per second: 430, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.210 [-1.827, 0.311], loss: 2.402900, mean_absolute_error: 28.507671, mean_q: 57.658756\n",
      " 26574/100000: episode: 214, duration: 0.430s, episode steps: 200, steps per second: 465, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.188 [-1.497, 0.339], loss: 2.055143, mean_absolute_error: 28.255243, mean_q: 57.223057\n",
      " 26774/100000: episode: 215, duration: 0.428s, episode steps: 200, steps per second: 468, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.070 [-0.330, 0.379], loss: 1.628852, mean_absolute_error: 28.340899, mean_q: 57.428410\n",
      " 26961/100000: episode: 216, duration: 0.433s, episode steps: 187, steps per second: 431, episode reward: 187.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.439 [0.000, 1.000], mean observation: -0.270 [-4.305, 3.684], loss: 2.536741, mean_absolute_error: 28.523970, mean_q: 57.729164\n",
      " 27161/100000: episode: 217, duration: 0.428s, episode steps: 200, steps per second: 467, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.033 [-0.350, 0.362], loss: 0.433047, mean_absolute_error: 28.568642, mean_q: 57.922192\n",
      " 27307/100000: episode: 218, duration: 0.354s, episode steps: 146, steps per second: 413, episode reward: 146.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.332 [-1.869, 0.505], loss: 0.571114, mean_absolute_error: 28.991184, mean_q: 58.814400\n",
      " 27507/100000: episode: 219, duration: 0.432s, episode steps: 200, steps per second: 463, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.059 [-0.367, 0.436], loss: 2.170753, mean_absolute_error: 28.749844, mean_q: 58.275234\n",
      " 27707/100000: episode: 220, duration: 0.457s, episode steps: 200, steps per second: 438, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.069 [-0.432, 0.429], loss: 2.908009, mean_absolute_error: 28.635666, mean_q: 58.023369\n",
      " 27907/100000: episode: 221, duration: 0.441s, episode steps: 200, steps per second: 454, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.079 [-0.410, 0.493], loss: 1.041469, mean_absolute_error: 28.849817, mean_q: 58.565155\n",
      " 28107/100000: episode: 222, duration: 0.427s, episode steps: 200, steps per second: 469, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.022 [-0.556, 0.359], loss: 1.690786, mean_absolute_error: 29.275503, mean_q: 59.381557\n",
      " 28273/100000: episode: 223, duration: 0.385s, episode steps: 166, steps per second: 431, episode reward: 166.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.434 [0.000, 1.000], mean observation: -0.328 [-4.131, 3.182], loss: 2.742016, mean_absolute_error: 28.990814, mean_q: 58.717667\n",
      " 28473/100000: episode: 224, duration: 0.431s, episode steps: 200, steps per second: 464, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.044 [-0.301, 0.328], loss: 2.243770, mean_absolute_error: 29.225014, mean_q: 59.237915\n",
      " 28622/100000: episode: 225, duration: 0.383s, episode steps: 149, steps per second: 389, episode reward: 149.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.423 [0.000, 1.000], mean observation: -0.353 [-4.365, 3.703], loss: 2.434706, mean_absolute_error: 29.192051, mean_q: 59.202904\n",
      " 28822/100000: episode: 226, duration: 0.430s, episode steps: 200, steps per second: 465, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.140 [-0.378, 0.732], loss: 2.662539, mean_absolute_error: 29.160316, mean_q: 59.049316\n",
      " 29022/100000: episode: 227, duration: 0.436s, episode steps: 200, steps per second: 458, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.090 [-0.367, 0.540], loss: 1.340635, mean_absolute_error: 29.474319, mean_q: 59.759453\n",
      " 29222/100000: episode: 228, duration: 0.457s, episode steps: 200, steps per second: 438, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.231 [-2.175, 0.328], loss: 1.473152, mean_absolute_error: 29.223862, mean_q: 59.269894\n",
      " 29380/100000: episode: 229, duration: 0.343s, episode steps: 158, steps per second: 460, episode reward: 158.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.430 [0.000, 1.000], mean observation: -0.347 [-4.170, 3.100], loss: 2.456513, mean_absolute_error: 29.171745, mean_q: 59.121193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 29580/100000: episode: 230, duration: 0.511s, episode steps: 200, steps per second: 391, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.238 [-2.607, 0.913], loss: 2.719050, mean_absolute_error: 29.330494, mean_q: 59.390614\n",
      " 29780/100000: episode: 231, duration: 0.430s, episode steps: 200, steps per second: 465, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.075 [-0.452, 0.401], loss: 1.825972, mean_absolute_error: 29.126961, mean_q: 59.000126\n",
      " 29963/100000: episode: 232, duration: 0.414s, episode steps: 183, steps per second: 442, episode reward: 183.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.437 [0.000, 1.000], mean observation: -0.291 [-4.371, 3.651], loss: 2.225020, mean_absolute_error: 29.765070, mean_q: 60.260166\n",
      " 30139/100000: episode: 233, duration: 0.376s, episode steps: 176, steps per second: 468, episode reward: 176.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.312 [-4.120, 3.131], loss: 2.476946, mean_absolute_error: 29.663630, mean_q: 59.970646\n",
      " 30300/100000: episode: 234, duration: 0.349s, episode steps: 161, steps per second: 461, episode reward: 161.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: -0.350 [-3.949, 2.759], loss: 2.527156, mean_absolute_error: 29.636337, mean_q: 59.852085\n",
      " 30500/100000: episode: 235, duration: 0.454s, episode steps: 200, steps per second: 441, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.013 [-0.324, 0.349], loss: 2.637644, mean_absolute_error: 29.644459, mean_q: 59.929169\n",
      " 30679/100000: episode: 236, duration: 0.388s, episode steps: 179, steps per second: 462, episode reward: 179.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.441 [0.000, 1.000], mean observation: -0.267 [-4.006, 3.395], loss: 4.521955, mean_absolute_error: 29.716742, mean_q: 60.024185\n",
      " 30879/100000: episode: 237, duration: 0.454s, episode steps: 200, steps per second: 441, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.267 [-3.773, 2.559], loss: 2.584681, mean_absolute_error: 29.572128, mean_q: 59.741646\n",
      " 31037/100000: episode: 238, duration: 0.341s, episode steps: 158, steps per second: 464, episode reward: 158.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.430 [0.000, 1.000], mean observation: -0.327 [-4.204, 3.548], loss: 1.221857, mean_absolute_error: 29.167150, mean_q: 59.027561\n",
      " 31237/100000: episode: 239, duration: 0.434s, episode steps: 200, steps per second: 461, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.008 [-0.367, 0.340], loss: 1.066589, mean_absolute_error: 29.878962, mean_q: 60.520683\n",
      " 31437/100000: episode: 240, duration: 0.453s, episode steps: 200, steps per second: 442, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.080 [-0.376, 0.492], loss: 2.266212, mean_absolute_error: 29.827377, mean_q: 60.350204\n",
      " 31591/100000: episode: 241, duration: 0.341s, episode steps: 154, steps per second: 452, episode reward: 154.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: -0.336 [-4.206, 3.511], loss: 3.428013, mean_absolute_error: 29.733480, mean_q: 60.079216\n",
      " 31791/100000: episode: 242, duration: 0.482s, episode steps: 200, steps per second: 415, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.101 [-0.382, 0.513], loss: 3.893682, mean_absolute_error: 29.955595, mean_q: 60.487427\n",
      " 31991/100000: episode: 243, duration: 0.430s, episode steps: 200, steps per second: 465, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.081 [-0.786, 0.326], loss: 3.666611, mean_absolute_error: 30.001812, mean_q: 60.653984\n",
      " 32191/100000: episode: 244, duration: 0.476s, episode steps: 200, steps per second: 420, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.076 [-0.467, 0.528], loss: 4.266869, mean_absolute_error: 29.906147, mean_q: 60.279572\n",
      " 32391/100000: episode: 245, duration: 0.442s, episode steps: 200, steps per second: 453, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.238 [-2.249, 0.532], loss: 3.816749, mean_absolute_error: 30.146425, mean_q: 60.820568\n",
      " 32591/100000: episode: 246, duration: 0.467s, episode steps: 200, steps per second: 428, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.105 [-0.361, 0.604], loss: 4.289182, mean_absolute_error: 30.080072, mean_q: 60.651424\n",
      " 32791/100000: episode: 247, duration: 0.454s, episode steps: 200, steps per second: 440, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.073 [-0.470, 0.454], loss: 2.844605, mean_absolute_error: 30.149076, mean_q: 60.867332\n",
      " 32991/100000: episode: 248, duration: 0.428s, episode steps: 200, steps per second: 467, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.082 [-0.363, 0.512], loss: 1.964001, mean_absolute_error: 30.453569, mean_q: 61.449451\n",
      " 33144/100000: episode: 249, duration: 0.356s, episode steps: 153, steps per second: 430, episode reward: 153.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.425 [0.000, 1.000], mean observation: -0.354 [-4.330, 3.540], loss: 1.984255, mean_absolute_error: 30.490656, mean_q: 61.521164\n",
      " 33344/100000: episode: 250, duration: 0.431s, episode steps: 200, steps per second: 464, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.222 [-1.858, 0.365], loss: 3.593188, mean_absolute_error: 30.318838, mean_q: 61.113701\n",
      " 33544/100000: episode: 251, duration: 0.455s, episode steps: 200, steps per second: 440, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.263 [-3.401, 1.896], loss: 1.363172, mean_absolute_error: 30.373178, mean_q: 61.353085\n",
      " 33744/100000: episode: 252, duration: 0.425s, episode steps: 200, steps per second: 471, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.128 [-0.428, 0.746], loss: 3.874717, mean_absolute_error: 30.224297, mean_q: 60.992889\n",
      " 33944/100000: episode: 253, duration: 0.451s, episode steps: 200, steps per second: 444, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.053 [-0.398, 0.392], loss: 4.108890, mean_absolute_error: 30.504753, mean_q: 61.539394\n",
      " 34144/100000: episode: 254, duration: 0.441s, episode steps: 200, steps per second: 453, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.084 [-0.333, 0.536], loss: 1.763568, mean_absolute_error: 30.412600, mean_q: 61.404327\n",
      " 34344/100000: episode: 255, duration: 0.431s, episode steps: 200, steps per second: 464, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.093 [-0.379, 0.565], loss: 3.594086, mean_absolute_error: 30.736069, mean_q: 61.993618\n",
      " 34544/100000: episode: 256, duration: 0.525s, episode steps: 200, steps per second: 381, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.037 [-0.328, 0.403], loss: 2.961138, mean_absolute_error: 30.660025, mean_q: 61.901085\n",
      " 34744/100000: episode: 257, duration: 0.431s, episode steps: 200, steps per second: 464, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.127 [-0.353, 0.662], loss: 4.337667, mean_absolute_error: 30.959272, mean_q: 62.455555\n",
      " 34944/100000: episode: 258, duration: 0.825s, episode steps: 200, steps per second: 242, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.005 [-0.354, 0.316], loss: 3.985868, mean_absolute_error: 30.753834, mean_q: 61.963097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 35099/100000: episode: 259, duration: 0.749s, episode steps: 155, steps per second: 207, episode reward: 155.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.432 [0.000, 1.000], mean observation: -0.363 [-3.901, 2.514], loss: 2.044124, mean_absolute_error: 30.678667, mean_q: 61.904945\n",
      " 35299/100000: episode: 260, duration: 1.120s, episode steps: 200, steps per second: 179, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.255 [-2.567, 0.956], loss: 3.210951, mean_absolute_error: 30.754047, mean_q: 62.061905\n",
      " 35499/100000: episode: 261, duration: 0.682s, episode steps: 200, steps per second: 293, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.185 [-1.663, 0.346], loss: 2.789771, mean_absolute_error: 30.829256, mean_q: 62.171211\n",
      " 35699/100000: episode: 262, duration: 0.631s, episode steps: 200, steps per second: 317, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.249 [-3.004, 1.547], loss: 5.984732, mean_absolute_error: 31.009628, mean_q: 62.430817\n",
      " 35880/100000: episode: 263, duration: 0.663s, episode steps: 181, steps per second: 273, episode reward: 181.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: -0.317 [-3.985, 2.999], loss: 1.633359, mean_absolute_error: 30.524218, mean_q: 61.685925\n",
      " 36046/100000: episode: 264, duration: 0.373s, episode steps: 166, steps per second: 445, episode reward: 166.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: -0.348 [-3.711, 2.480], loss: 2.551549, mean_absolute_error: 30.696325, mean_q: 61.957832\n",
      " 36246/100000: episode: 265, duration: 0.457s, episode steps: 200, steps per second: 437, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.097 [-0.522, 0.675], loss: 1.662510, mean_absolute_error: 30.989660, mean_q: 62.559559\n",
      " 36389/100000: episode: 266, duration: 0.347s, episode steps: 143, steps per second: 412, episode reward: 143.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.427 [0.000, 1.000], mean observation: -0.307 [-4.020, 3.514], loss: 4.674699, mean_absolute_error: 31.032909, mean_q: 62.434052\n",
      " 36576/100000: episode: 267, duration: 0.473s, episode steps: 187, steps per second: 396, episode reward: 187.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.314 [-3.934, 2.688], loss: 2.120183, mean_absolute_error: 31.138975, mean_q: 62.743874\n",
      " 36772/100000: episode: 268, duration: 0.607s, episode steps: 196, steps per second: 323, episode reward: 196.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.280 [-4.172, 3.544], loss: 2.156663, mean_absolute_error: 30.824339, mean_q: 62.153763\n",
      " 36900/100000: episode: 269, duration: 0.831s, episode steps: 128, steps per second: 154, episode reward: 128.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.422 [0.000, 1.000], mean observation: -0.307 [-3.831, 3.414], loss: 6.004045, mean_absolute_error: 31.185322, mean_q: 62.632782\n",
      " 37100/100000: episode: 270, duration: 0.633s, episode steps: 200, steps per second: 316, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.049 [-0.726, 0.558], loss: 4.180959, mean_absolute_error: 30.908476, mean_q: 62.144218\n",
      " 37300/100000: episode: 271, duration: 0.459s, episode steps: 200, steps per second: 436, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.078 [-0.920, 0.416], loss: 2.779000, mean_absolute_error: 31.131092, mean_q: 62.633427\n",
      " 37500/100000: episode: 272, duration: 0.543s, episode steps: 200, steps per second: 368, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.126 [-0.343, 0.698], loss: 2.222253, mean_absolute_error: 31.270605, mean_q: 62.978535\n",
      " 37700/100000: episode: 273, duration: 0.431s, episode steps: 200, steps per second: 464, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.124 [-1.060, 0.385], loss: 2.685666, mean_absolute_error: 31.400816, mean_q: 63.171051\n",
      " 37900/100000: episode: 274, duration: 0.527s, episode steps: 200, steps per second: 379, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.067 [-0.321, 0.473], loss: 1.795007, mean_absolute_error: 31.342764, mean_q: 63.142330\n",
      " 38084/100000: episode: 275, duration: 0.400s, episode steps: 184, steps per second: 460, episode reward: 184.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.446 [0.000, 1.000], mean observation: -0.323 [-3.783, 2.324], loss: 3.358614, mean_absolute_error: 31.602200, mean_q: 63.523426\n",
      " 38284/100000: episode: 276, duration: 0.535s, episode steps: 200, steps per second: 374, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.118 [-0.336, 0.667], loss: 1.376467, mean_absolute_error: 31.619089, mean_q: 63.565353\n",
      " 38425/100000: episode: 277, duration: 0.376s, episode steps: 141, steps per second: 375, episode reward: 141.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.426 [0.000, 1.000], mean observation: -0.402 [-3.948, 2.676], loss: 2.378472, mean_absolute_error: 31.805712, mean_q: 63.963417\n",
      " 38625/100000: episode: 278, duration: 0.453s, episode steps: 200, steps per second: 441, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.162 [-0.330, 0.877], loss: 3.708780, mean_absolute_error: 31.272638, mean_q: 62.777557\n",
      " 38825/100000: episode: 279, duration: 0.439s, episode steps: 200, steps per second: 456, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.089 [-0.534, 0.586], loss: 2.741523, mean_absolute_error: 31.364220, mean_q: 63.003780\n",
      " 38976/100000: episode: 280, duration: 0.327s, episode steps: 151, steps per second: 462, episode reward: 151.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.430 [0.000, 1.000], mean observation: -0.370 [-3.952, 2.842], loss: 1.872216, mean_absolute_error: 31.106060, mean_q: 62.645851\n",
      " 39176/100000: episode: 281, duration: 0.537s, episode steps: 200, steps per second: 373, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.284 [-3.382, 1.767], loss: 1.576745, mean_absolute_error: 31.331812, mean_q: 63.100113\n",
      " 39376/100000: episode: 282, duration: 0.459s, episode steps: 200, steps per second: 435, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.108 [-0.394, 0.724], loss: 2.242342, mean_absolute_error: 31.275749, mean_q: 62.946392\n",
      " 39538/100000: episode: 283, duration: 0.623s, episode steps: 162, steps per second: 260, episode reward: 162.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.432 [0.000, 1.000], mean observation: -0.340 [-4.174, 3.181], loss: 2.030055, mean_absolute_error: 31.279043, mean_q: 63.021618\n",
      " 39708/100000: episode: 284, duration: 0.380s, episode steps: 170, steps per second: 447, episode reward: 170.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: -0.320 [-4.189, 3.136], loss: 4.378994, mean_absolute_error: 31.116121, mean_q: 62.549362\n",
      " 39908/100000: episode: 285, duration: 0.501s, episode steps: 200, steps per second: 399, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.189 [-1.696, 0.398], loss: 2.106160, mean_absolute_error: 30.856743, mean_q: 62.164394\n",
      " 40089/100000: episode: 286, duration: 0.419s, episode steps: 181, steps per second: 432, episode reward: 181.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: -0.309 [-3.888, 2.592], loss: 2.096883, mean_absolute_error: 31.246372, mean_q: 62.949997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 40264/100000: episode: 287, duration: 0.543s, episode steps: 175, steps per second: 322, episode reward: 175.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.446 [0.000, 1.000], mean observation: -0.336 [-3.589, 2.001], loss: 3.474854, mean_absolute_error: 30.847940, mean_q: 62.141022\n",
      " 40464/100000: episode: 288, duration: 0.692s, episode steps: 200, steps per second: 289, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.076 [-0.882, 0.372], loss: 3.844881, mean_absolute_error: 31.203463, mean_q: 62.836109\n",
      " 40664/100000: episode: 289, duration: 0.550s, episode steps: 200, steps per second: 364, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.079 [-0.448, 0.478], loss: 3.427130, mean_absolute_error: 30.906178, mean_q: 62.189354\n",
      " 40864/100000: episode: 290, duration: 0.489s, episode steps: 200, steps per second: 409, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.141 [-0.385, 0.762], loss: 3.455933, mean_absolute_error: 30.747236, mean_q: 61.851738\n",
      " 41064/100000: episode: 291, duration: 0.447s, episode steps: 200, steps per second: 448, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.072 [-0.353, 0.478], loss: 1.602272, mean_absolute_error: 30.877632, mean_q: 62.240490\n",
      " 41255/100000: episode: 292, duration: 0.418s, episode steps: 191, steps per second: 457, episode reward: 191.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.445 [0.000, 1.000], mean observation: -0.305 [-3.977, 2.602], loss: 2.617589, mean_absolute_error: 30.568117, mean_q: 61.553234\n",
      " 41455/100000: episode: 293, duration: 0.469s, episode steps: 200, steps per second: 427, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.131 [-0.357, 0.746], loss: 2.342079, mean_absolute_error: 30.923452, mean_q: 62.292847\n",
      " 41655/100000: episode: 294, duration: 0.432s, episode steps: 200, steps per second: 463, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.135 [-0.339, 0.771], loss: 3.001464, mean_absolute_error: 30.850430, mean_q: 62.131012\n",
      " 41855/100000: episode: 295, duration: 0.478s, episode steps: 200, steps per second: 418, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.133 [-0.357, 0.736], loss: 5.479844, mean_absolute_error: 30.917706, mean_q: 62.192715\n",
      " 42055/100000: episode: 296, duration: 0.536s, episode steps: 200, steps per second: 373, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.078 [-0.539, 0.498], loss: 2.515515, mean_absolute_error: 30.819118, mean_q: 62.045677\n",
      " 42255/100000: episode: 297, duration: 0.499s, episode steps: 200, steps per second: 401, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.144 [-0.365, 0.767], loss: 1.339794, mean_absolute_error: 30.752592, mean_q: 61.952560\n",
      " 42419/100000: episode: 298, duration: 0.556s, episode steps: 164, steps per second: 295, episode reward: 164.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.439 [0.000, 1.000], mean observation: -0.348 [-3.784, 2.636], loss: 1.860018, mean_absolute_error: 30.770472, mean_q: 61.914436\n",
      " 42586/100000: episode: 299, duration: 0.435s, episode steps: 167, steps per second: 383, episode reward: 167.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.437 [0.000, 1.000], mean observation: -0.349 [-3.920, 2.691], loss: 1.653552, mean_absolute_error: 30.759630, mean_q: 61.990620\n",
      " 42775/100000: episode: 300, duration: 0.405s, episode steps: 189, steps per second: 467, episode reward: 189.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.304 [-3.889, 2.487], loss: 1.519757, mean_absolute_error: 30.627676, mean_q: 61.728790\n",
      " 42975/100000: episode: 301, duration: 0.590s, episode steps: 200, steps per second: 339, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.079 [-0.750, 0.294], loss: 1.549155, mean_absolute_error: 30.458681, mean_q: 61.334335\n",
      " 43175/100000: episode: 302, duration: 0.436s, episode steps: 200, steps per second: 459, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.029 [-0.605, 0.336], loss: 3.349181, mean_absolute_error: 30.583532, mean_q: 61.514175\n",
      " 43375/100000: episode: 303, duration: 0.506s, episode steps: 200, steps per second: 396, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.124 [-0.320, 0.695], loss: 3.981126, mean_absolute_error: 30.589199, mean_q: 61.529831\n",
      " 43575/100000: episode: 304, duration: 0.507s, episode steps: 200, steps per second: 394, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.210 [-1.846, 0.376], loss: 3.707710, mean_absolute_error: 30.353804, mean_q: 61.053810\n",
      " 43742/100000: episode: 305, duration: 0.665s, episode steps: 167, steps per second: 251, episode reward: 167.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.437 [0.000, 1.000], mean observation: -0.342 [-3.965, 2.592], loss: 2.314914, mean_absolute_error: 30.324202, mean_q: 61.085609\n",
      " 43942/100000: episode: 306, duration: 0.509s, episode steps: 200, steps per second: 393, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.249 [-2.594, 0.612], loss: 2.201190, mean_absolute_error: 29.973516, mean_q: 60.363262\n",
      " 44106/100000: episode: 307, duration: 0.441s, episode steps: 164, steps per second: 372, episode reward: 164.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.433 [0.000, 1.000], mean observation: -0.340 [-4.188, 3.108], loss: 1.571222, mean_absolute_error: 30.242420, mean_q: 60.951153\n",
      " 44306/100000: episode: 308, duration: 0.491s, episode steps: 200, steps per second: 408, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.038 [-0.522, 0.412], loss: 2.505920, mean_absolute_error: 29.992487, mean_q: 60.490170\n",
      " 44506/100000: episode: 309, duration: 0.549s, episode steps: 200, steps per second: 364, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.276 [-3.366, 1.782], loss: 2.710612, mean_absolute_error: 30.071514, mean_q: 60.730701\n",
      " 44706/100000: episode: 310, duration: 0.501s, episode steps: 200, steps per second: 399, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.275 [-2.949, 1.184], loss: 2.231964, mean_absolute_error: 29.909616, mean_q: 60.443039\n",
      " 44906/100000: episode: 311, duration: 0.495s, episode steps: 200, steps per second: 404, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.126 [-1.098, 0.335], loss: 1.395036, mean_absolute_error: 30.152998, mean_q: 60.970478\n",
      " 45075/100000: episode: 312, duration: 0.395s, episode steps: 169, steps per second: 428, episode reward: 169.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.336 [-3.941, 2.811], loss: 2.616173, mean_absolute_error: 30.197845, mean_q: 61.019024\n",
      " 45275/100000: episode: 313, duration: 0.465s, episode steps: 200, steps per second: 430, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.081 [-0.325, 0.535], loss: 2.295873, mean_absolute_error: 30.273863, mean_q: 61.102188\n",
      " 45475/100000: episode: 314, duration: 0.450s, episode steps: 200, steps per second: 444, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.116 [-0.425, 0.705], loss: 2.790191, mean_absolute_error: 30.187891, mean_q: 60.929489\n",
      " 45675/100000: episode: 315, duration: 0.459s, episode steps: 200, steps per second: 436, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.179 [-1.607, 0.478], loss: 3.713587, mean_absolute_error: 30.064863, mean_q: 60.614437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 45875/100000: episode: 316, duration: 0.434s, episode steps: 200, steps per second: 461, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.132 [-1.138, 0.358], loss: 0.829987, mean_absolute_error: 30.151812, mean_q: 60.853085\n",
      " 46057/100000: episode: 317, duration: 0.408s, episode steps: 182, steps per second: 446, episode reward: 182.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.445 [0.000, 1.000], mean observation: -0.333 [-3.781, 2.213], loss: 1.059143, mean_absolute_error: 30.112080, mean_q: 60.883331\n",
      " 46256/100000: episode: 318, duration: 0.444s, episode steps: 199, steps per second: 448, episode reward: 199.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.294 [-3.929, 2.749], loss: 1.084597, mean_absolute_error: 30.411547, mean_q: 61.348942\n",
      " 46435/100000: episode: 319, duration: 0.389s, episode steps: 179, steps per second: 460, episode reward: 179.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.441 [0.000, 1.000], mean observation: -0.323 [-3.975, 2.745], loss: 2.404487, mean_absolute_error: 30.404324, mean_q: 61.235760\n",
      " 46635/100000: episode: 320, duration: 0.507s, episode steps: 200, steps per second: 395, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.080 [-0.333, 0.496], loss: 2.029662, mean_absolute_error: 30.142649, mean_q: 60.751022\n",
      " 46835/100000: episode: 321, duration: 0.458s, episode steps: 200, steps per second: 436, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.006 [-0.370, 0.297], loss: 1.846948, mean_absolute_error: 30.216948, mean_q: 60.823387\n",
      " 46985/100000: episode: 322, duration: 0.417s, episode steps: 150, steps per second: 360, episode reward: 150.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.433 [0.000, 1.000], mean observation: -0.385 [-3.786, 2.339], loss: 1.541010, mean_absolute_error: 30.122618, mean_q: 60.761509\n",
      " 47175/100000: episode: 323, duration: 0.499s, episode steps: 190, steps per second: 381, episode reward: 190.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: -0.296 [-4.186, 3.215], loss: 1.313022, mean_absolute_error: 30.182772, mean_q: 60.890575\n",
      " 47375/100000: episode: 324, duration: 0.619s, episode steps: 200, steps per second: 323, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.114 [-0.351, 0.606], loss: 2.767136, mean_absolute_error: 30.085981, mean_q: 60.642208\n",
      " 47575/100000: episode: 325, duration: 0.489s, episode steps: 200, steps per second: 409, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.179 [-1.645, 0.297], loss: 1.800432, mean_absolute_error: 30.190411, mean_q: 60.816776\n",
      " 47775/100000: episode: 326, duration: 0.486s, episode steps: 200, steps per second: 412, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.215 [-2.014, 0.385], loss: 2.862226, mean_absolute_error: 29.722837, mean_q: 59.915596\n",
      " 47975/100000: episode: 327, duration: 0.452s, episode steps: 200, steps per second: 443, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.086 [-0.501, 0.509], loss: 1.951853, mean_absolute_error: 29.899382, mean_q: 60.363319\n",
      " 48165/100000: episode: 328, duration: 0.479s, episode steps: 190, steps per second: 397, episode reward: 190.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.308 [-3.781, 2.219], loss: 2.489364, mean_absolute_error: 29.629759, mean_q: 59.803638\n",
      " 48343/100000: episode: 329, duration: 0.468s, episode steps: 178, steps per second: 380, episode reward: 178.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.309 [-4.122, 3.005], loss: 1.727614, mean_absolute_error: 29.885117, mean_q: 60.332485\n",
      " 48543/100000: episode: 330, duration: 0.498s, episode steps: 200, steps per second: 401, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.138 [-0.340, 0.749], loss: 3.140035, mean_absolute_error: 29.918524, mean_q: 60.245689\n",
      " 48722/100000: episode: 331, duration: 0.391s, episode steps: 179, steps per second: 458, episode reward: 179.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.441 [0.000, 1.000], mean observation: -0.315 [-3.968, 2.642], loss: 1.501697, mean_absolute_error: 29.537476, mean_q: 59.655499\n",
      " 48922/100000: episode: 332, duration: 0.535s, episode steps: 200, steps per second: 374, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.076 [-0.322, 0.457], loss: 1.189481, mean_absolute_error: 29.958559, mean_q: 60.503681\n",
      " 49122/100000: episode: 333, duration: 0.452s, episode steps: 200, steps per second: 442, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.140 [-1.299, 0.318], loss: 1.350346, mean_absolute_error: 29.722071, mean_q: 59.952461\n",
      " 49322/100000: episode: 334, duration: 0.485s, episode steps: 200, steps per second: 412, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.230 [-1.971, 0.283], loss: 2.117013, mean_absolute_error: 29.707495, mean_q: 59.968468\n",
      " 49478/100000: episode: 335, duration: 0.339s, episode steps: 156, steps per second: 460, episode reward: 156.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: -0.360 [-4.098, 2.844], loss: 2.218694, mean_absolute_error: 29.711767, mean_q: 59.953682\n",
      " 49678/100000: episode: 336, duration: 0.425s, episode steps: 200, steps per second: 471, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.071 [-0.707, 0.300], loss: 2.495300, mean_absolute_error: 29.687893, mean_q: 59.951759\n",
      " 49878/100000: episode: 337, duration: 0.551s, episode steps: 200, steps per second: 363, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.126 [-0.315, 0.732], loss: 1.415763, mean_absolute_error: 29.480711, mean_q: 59.613857\n",
      " 50078/100000: episode: 338, duration: 0.451s, episode steps: 200, steps per second: 444, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.138 [-0.409, 0.766], loss: 2.040937, mean_absolute_error: 29.606030, mean_q: 59.806984\n",
      " 50278/100000: episode: 339, duration: 0.447s, episode steps: 200, steps per second: 447, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.137 [-0.538, 0.756], loss: 2.524217, mean_absolute_error: 29.608782, mean_q: 59.757534\n",
      " 50478/100000: episode: 340, duration: 0.429s, episode steps: 200, steps per second: 466, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.083 [-0.321, 0.539], loss: 3.011233, mean_absolute_error: 29.343645, mean_q: 59.147129\n",
      " 50678/100000: episode: 341, duration: 0.495s, episode steps: 200, steps per second: 404, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.039 [-0.315, 0.409], loss: 1.341816, mean_absolute_error: 29.779087, mean_q: 60.177261\n",
      " 50878/100000: episode: 342, duration: 0.429s, episode steps: 200, steps per second: 466, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.135 [-0.351, 0.743], loss: 1.388205, mean_absolute_error: 29.371731, mean_q: 59.316212\n",
      " 51078/100000: episode: 343, duration: 0.472s, episode steps: 200, steps per second: 423, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.117 [-0.328, 0.750], loss: 1.448912, mean_absolute_error: 29.587025, mean_q: 59.745201\n",
      " 51278/100000: episode: 344, duration: 0.457s, episode steps: 200, steps per second: 438, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.010 [-0.345, 0.377], loss: 2.870598, mean_absolute_error: 29.770533, mean_q: 60.038311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 51451/100000: episode: 345, duration: 0.419s, episode steps: 173, steps per second: 413, episode reward: 173.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.439 [0.000, 1.000], mean observation: -0.337 [-3.922, 2.703], loss: 1.839450, mean_absolute_error: 29.743807, mean_q: 60.095406\n",
      " 51632/100000: episode: 346, duration: 0.462s, episode steps: 181, steps per second: 392, episode reward: 181.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: -0.330 [-3.910, 2.631], loss: 0.800623, mean_absolute_error: 29.392847, mean_q: 59.380707\n",
      " 51832/100000: episode: 347, duration: 0.432s, episode steps: 200, steps per second: 463, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.137 [-0.352, 0.795], loss: 2.067365, mean_absolute_error: 29.400602, mean_q: 59.287354\n",
      " 52032/100000: episode: 348, duration: 0.477s, episode steps: 200, steps per second: 419, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.154 [-0.306, 0.794], loss: 1.949394, mean_absolute_error: 29.552299, mean_q: 59.609249\n",
      " 52232/100000: episode: 349, duration: 0.432s, episode steps: 200, steps per second: 463, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.198 [-1.494, 0.429], loss: 1.889543, mean_absolute_error: 29.673851, mean_q: 59.821651\n",
      " 52432/100000: episode: 350, duration: 0.505s, episode steps: 200, steps per second: 396, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.105 [-0.415, 0.671], loss: 2.150678, mean_absolute_error: 29.773464, mean_q: 60.015518\n",
      " 52625/100000: episode: 351, duration: 0.633s, episode steps: 193, steps per second: 305, episode reward: 193.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.446 [0.000, 1.000], mean observation: -0.302 [-3.984, 2.715], loss: 1.854554, mean_absolute_error: 29.793108, mean_q: 59.937908\n",
      " 52825/100000: episode: 352, duration: 0.751s, episode steps: 200, steps per second: 266, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.113 [-0.411, 0.692], loss: 1.706644, mean_absolute_error: 29.754389, mean_q: 59.959572\n",
      " 53025/100000: episode: 353, duration: 0.459s, episode steps: 200, steps per second: 436, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.152 [-0.319, 0.808], loss: 2.622488, mean_absolute_error: 29.666258, mean_q: 59.607735\n",
      " 53225/100000: episode: 354, duration: 0.705s, episode steps: 200, steps per second: 284, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.236 [-2.203, 0.516], loss: 2.496492, mean_absolute_error: 29.778975, mean_q: 59.898613\n",
      " 53425/100000: episode: 355, duration: 0.471s, episode steps: 200, steps per second: 425, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.118 [-0.414, 0.699], loss: 2.274623, mean_absolute_error: 29.616344, mean_q: 59.578602\n",
      " 53625/100000: episode: 356, duration: 0.458s, episode steps: 200, steps per second: 437, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.209 [-1.646, 0.440], loss: 2.863237, mean_absolute_error: 29.778549, mean_q: 59.856846\n",
      " 53825/100000: episode: 357, duration: 0.666s, episode steps: 200, steps per second: 300, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.139 [-0.411, 0.841], loss: 3.025640, mean_absolute_error: 29.515537, mean_q: 59.216358\n",
      " 54025/100000: episode: 358, duration: 0.920s, episode steps: 200, steps per second: 217, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.258 [-2.579, 0.891], loss: 1.754285, mean_absolute_error: 29.813608, mean_q: 59.928104\n",
      " 54225/100000: episode: 359, duration: 0.717s, episode steps: 200, steps per second: 279, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.110 [-0.340, 0.652], loss: 1.496200, mean_absolute_error: 29.789892, mean_q: 59.873856\n",
      " 54425/100000: episode: 360, duration: 0.499s, episode steps: 200, steps per second: 401, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.049 [-0.535, 0.326], loss: 1.227839, mean_absolute_error: 29.880859, mean_q: 60.093887\n",
      " 54625/100000: episode: 361, duration: 0.538s, episode steps: 200, steps per second: 372, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.086 [-0.322, 0.564], loss: 1.412502, mean_absolute_error: 29.866436, mean_q: 60.013847\n",
      " 54811/100000: episode: 362, duration: 0.434s, episode steps: 186, steps per second: 428, episode reward: 186.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.446 [0.000, 1.000], mean observation: -0.320 [-3.805, 2.656], loss: 2.295286, mean_absolute_error: 29.720293, mean_q: 59.696106\n",
      " 54963/100000: episode: 363, duration: 0.334s, episode steps: 152, steps per second: 455, episode reward: 152.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.434 [0.000, 1.000], mean observation: -0.379 [-3.763, 2.504], loss: 1.426753, mean_absolute_error: 29.939604, mean_q: 60.188019\n",
      " 55163/100000: episode: 364, duration: 0.585s, episode steps: 200, steps per second: 342, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.080 [-0.331, 0.512], loss: 1.548753, mean_absolute_error: 29.818306, mean_q: 59.899796\n",
      " 55363/100000: episode: 365, duration: 0.444s, episode steps: 200, steps per second: 450, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.122 [-0.503, 0.697], loss: 1.353397, mean_absolute_error: 29.807262, mean_q: 59.886024\n",
      " 55518/100000: episode: 366, duration: 0.398s, episode steps: 155, steps per second: 390, episode reward: 155.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.432 [0.000, 1.000], mean observation: -0.363 [-3.944, 2.854], loss: 1.401300, mean_absolute_error: 29.613861, mean_q: 59.509487\n",
      " 55718/100000: episode: 367, duration: 0.424s, episode steps: 200, steps per second: 471, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.079 [-0.378, 0.489], loss: 1.594584, mean_absolute_error: 29.667553, mean_q: 59.471466\n",
      " 55918/100000: episode: 368, duration: 0.539s, episode steps: 200, steps per second: 371, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.084 [-0.408, 0.503], loss: 1.790588, mean_absolute_error: 29.787926, mean_q: 59.719025\n",
      " 56118/100000: episode: 369, duration: 0.439s, episode steps: 200, steps per second: 455, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.144 [-0.403, 0.795], loss: 1.554707, mean_absolute_error: 29.679653, mean_q: 59.572857\n",
      " 56318/100000: episode: 370, duration: 0.475s, episode steps: 200, steps per second: 421, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.105 [-0.424, 0.632], loss: 1.276191, mean_absolute_error: 29.838181, mean_q: 59.946930\n",
      " 56518/100000: episode: 371, duration: 0.439s, episode steps: 200, steps per second: 455, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.048 [-0.426, 0.541], loss: 2.425550, mean_absolute_error: 30.054176, mean_q: 60.238968\n",
      " 56718/100000: episode: 372, duration: 0.516s, episode steps: 200, steps per second: 388, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.159 [-1.240, 0.445], loss: 2.106633, mean_absolute_error: 30.047997, mean_q: 60.191029\n",
      " 56918/100000: episode: 373, duration: 0.461s, episode steps: 200, steps per second: 434, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.080 [-0.428, 0.507], loss: 0.895410, mean_absolute_error: 30.076233, mean_q: 60.410538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 57118/100000: episode: 374, duration: 0.612s, episode steps: 200, steps per second: 327, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.056 [-0.429, 0.576], loss: 1.329797, mean_absolute_error: 30.204292, mean_q: 60.637054\n",
      " 57318/100000: episode: 375, duration: 0.433s, episode steps: 200, steps per second: 462, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.118 [-0.520, 0.633], loss: 1.915955, mean_absolute_error: 29.786377, mean_q: 59.740360\n",
      " 57518/100000: episode: 376, duration: 0.623s, episode steps: 200, steps per second: 321, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.040 [-0.474, 0.350], loss: 1.341245, mean_absolute_error: 30.298733, mean_q: 60.793140\n",
      " 57718/100000: episode: 377, duration: 0.502s, episode steps: 200, steps per second: 398, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.063 [-0.446, 0.405], loss: 1.722964, mean_absolute_error: 30.223598, mean_q: 60.645653\n",
      " 57900/100000: episode: 378, duration: 0.429s, episode steps: 182, steps per second: 424, episode reward: 182.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.445 [0.000, 1.000], mean observation: -0.329 [-3.736, 2.148], loss: 2.376105, mean_absolute_error: 30.343321, mean_q: 60.762554\n",
      " 58060/100000: episode: 379, duration: 0.359s, episode steps: 160, steps per second: 445, episode reward: 160.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.366 [-3.800, 2.555], loss: 0.441652, mean_absolute_error: 29.958241, mean_q: 60.180511\n",
      " 58260/100000: episode: 380, duration: 0.455s, episode steps: 200, steps per second: 439, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.115 [-0.516, 0.610], loss: 2.205274, mean_absolute_error: 30.386076, mean_q: 60.915821\n",
      " 58460/100000: episode: 381, duration: 0.468s, episode steps: 200, steps per second: 427, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.112 [-0.396, 0.589], loss: 1.761518, mean_absolute_error: 30.000166, mean_q: 60.126045\n",
      " 58660/100000: episode: 382, duration: 0.459s, episode steps: 200, steps per second: 436, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.091 [-0.535, 0.512], loss: 2.525134, mean_absolute_error: 30.301689, mean_q: 60.722080\n",
      " 58860/100000: episode: 383, duration: 0.638s, episode steps: 200, steps per second: 313, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.108 [-0.441, 0.610], loss: 2.404290, mean_absolute_error: 30.143049, mean_q: 60.316620\n",
      " 59060/100000: episode: 384, duration: 0.584s, episode steps: 200, steps per second: 342, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.033 [-0.500, 0.574], loss: 2.426045, mean_absolute_error: 29.979809, mean_q: 59.978127\n",
      " 59260/100000: episode: 385, duration: 0.448s, episode steps: 200, steps per second: 446, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.009 [-0.374, 0.526], loss: 1.651909, mean_absolute_error: 30.110962, mean_q: 60.288399\n",
      " 59460/100000: episode: 386, duration: 0.511s, episode steps: 200, steps per second: 391, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.038 [-0.396, 0.401], loss: 1.450518, mean_absolute_error: 30.630884, mean_q: 61.365002\n",
      " 59660/100000: episode: 387, duration: 0.427s, episode steps: 200, steps per second: 468, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.029 [-0.432, 0.399], loss: 1.789682, mean_absolute_error: 30.652895, mean_q: 61.384453\n",
      " 59860/100000: episode: 388, duration: 0.469s, episode steps: 200, steps per second: 426, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.061 [-0.389, 0.442], loss: 1.224771, mean_absolute_error: 30.492569, mean_q: 61.155743\n",
      " 60060/100000: episode: 389, duration: 0.482s, episode steps: 200, steps per second: 415, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.022 [-0.364, 0.562], loss: 1.688453, mean_absolute_error: 31.044510, mean_q: 62.182148\n",
      " 60260/100000: episode: 390, duration: 0.515s, episode steps: 200, steps per second: 388, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.068 [-0.451, 0.424], loss: 2.613112, mean_absolute_error: 30.888243, mean_q: 61.834862\n",
      " 60460/100000: episode: 391, duration: 0.503s, episode steps: 200, steps per second: 398, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.105 [-0.414, 0.586], loss: 2.265014, mean_absolute_error: 31.139267, mean_q: 62.318935\n",
      " 60660/100000: episode: 392, duration: 0.756s, episode steps: 200, steps per second: 265, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.054 [-0.451, 0.419], loss: 3.101827, mean_absolute_error: 30.971430, mean_q: 61.880283\n",
      " 60860/100000: episode: 393, duration: 0.428s, episode steps: 200, steps per second: 468, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.005 [-0.470, 0.599], loss: 3.828821, mean_absolute_error: 30.660728, mean_q: 61.233761\n",
      " 61032/100000: episode: 394, duration: 0.408s, episode steps: 172, steps per second: 422, episode reward: 172.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: -0.339 [-3.761, 2.579], loss: 2.178724, mean_absolute_error: 31.062778, mean_q: 62.156818\n",
      " 61232/100000: episode: 395, duration: 0.477s, episode steps: 200, steps per second: 419, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.139 [-1.125, 0.417], loss: 2.694693, mean_absolute_error: 31.122688, mean_q: 62.235199\n",
      " 61432/100000: episode: 396, duration: 0.456s, episode steps: 200, steps per second: 438, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.076 [-0.560, 0.423], loss: 2.162496, mean_absolute_error: 30.928091, mean_q: 61.878841\n",
      " 61632/100000: episode: 397, duration: 0.451s, episode steps: 200, steps per second: 443, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.043 [-0.450, 0.380], loss: 2.154351, mean_absolute_error: 31.026369, mean_q: 62.075218\n",
      " 61832/100000: episode: 398, duration: 0.450s, episode steps: 200, steps per second: 444, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.179 [-1.482, 0.570], loss: 0.937821, mean_absolute_error: 31.456240, mean_q: 62.996368\n",
      " 62032/100000: episode: 399, duration: 0.474s, episode steps: 200, steps per second: 422, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.034 [-0.486, 0.659], loss: 3.304215, mean_absolute_error: 31.335527, mean_q: 62.603828\n",
      " 62232/100000: episode: 400, duration: 0.423s, episode steps: 200, steps per second: 473, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.034 [-0.440, 0.405], loss: 2.057712, mean_absolute_error: 31.256411, mean_q: 62.546474\n",
      " 62432/100000: episode: 401, duration: 0.490s, episode steps: 200, steps per second: 408, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.051 [-0.431, 0.408], loss: 1.993094, mean_absolute_error: 31.254570, mean_q: 62.573475\n",
      " 62632/100000: episode: 402, duration: 0.615s, episode steps: 200, steps per second: 325, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.063 [-0.476, 0.432], loss: 1.934028, mean_absolute_error: 31.695938, mean_q: 63.430508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 62832/100000: episode: 403, duration: 0.534s, episode steps: 200, steps per second: 375, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.070 [-0.487, 0.411], loss: 3.609052, mean_absolute_error: 31.598661, mean_q: 63.099804\n",
      " 63032/100000: episode: 404, duration: 0.437s, episode steps: 200, steps per second: 458, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.059 [-0.451, 0.388], loss: 1.728357, mean_absolute_error: 31.573149, mean_q: 63.135899\n",
      " 63232/100000: episode: 405, duration: 0.486s, episode steps: 200, steps per second: 412, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.026 [-0.686, 0.578], loss: 3.344896, mean_absolute_error: 31.772930, mean_q: 63.455109\n",
      " 63432/100000: episode: 406, duration: 0.607s, episode steps: 200, steps per second: 330, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.126 [-0.938, 0.452], loss: 2.774755, mean_absolute_error: 32.009575, mean_q: 64.002724\n",
      " 63632/100000: episode: 407, duration: 0.688s, episode steps: 200, steps per second: 291, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.027 [-0.470, 0.405], loss: 2.339430, mean_absolute_error: 31.879536, mean_q: 63.741959\n",
      " 63832/100000: episode: 408, duration: 0.604s, episode steps: 200, steps per second: 331, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.077 [-0.439, 0.743], loss: 4.481303, mean_absolute_error: 31.864885, mean_q: 63.555401\n",
      " 64032/100000: episode: 409, duration: 0.650s, episode steps: 200, steps per second: 308, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.051 [-0.429, 0.566], loss: 1.805133, mean_absolute_error: 31.844593, mean_q: 63.720093\n",
      " 64232/100000: episode: 410, duration: 0.869s, episode steps: 200, steps per second: 230, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.085 [-0.434, 0.445], loss: 4.082189, mean_absolute_error: 31.807261, mean_q: 63.472870\n",
      " 64432/100000: episode: 411, duration: 0.680s, episode steps: 200, steps per second: 294, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.101 [-0.431, 0.593], loss: 3.940919, mean_absolute_error: 31.979511, mean_q: 63.823223\n",
      " 64632/100000: episode: 412, duration: 0.428s, episode steps: 200, steps per second: 468, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.063 [-0.514, 0.414], loss: 2.597192, mean_absolute_error: 31.837656, mean_q: 63.671337\n",
      " 64832/100000: episode: 413, duration: 0.476s, episode steps: 200, steps per second: 420, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.065 [-0.597, 0.834], loss: 3.172799, mean_absolute_error: 32.179092, mean_q: 64.221497\n",
      " 65032/100000: episode: 414, duration: 0.421s, episode steps: 200, steps per second: 475, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.026 [-0.473, 0.603], loss: 2.207913, mean_absolute_error: 32.142899, mean_q: 64.260651\n",
      " 65232/100000: episode: 415, duration: 0.471s, episode steps: 200, steps per second: 425, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.007 [-0.476, 0.630], loss: 3.449959, mean_absolute_error: 32.362118, mean_q: 64.616898\n",
      " 65432/100000: episode: 416, duration: 0.424s, episode steps: 200, steps per second: 471, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.021 [-0.588, 0.577], loss: 4.292022, mean_absolute_error: 32.072891, mean_q: 63.946671\n",
      " 65632/100000: episode: 417, duration: 0.572s, episode steps: 200, steps per second: 350, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.257 [-1.806, 0.553], loss: 3.562897, mean_absolute_error: 32.214779, mean_q: 64.288368\n",
      " 65832/100000: episode: 418, duration: 0.448s, episode steps: 200, steps per second: 446, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.078 [-0.369, 0.388], loss: 1.557142, mean_absolute_error: 32.259644, mean_q: 64.513771\n",
      " 66032/100000: episode: 419, duration: 0.442s, episode steps: 200, steps per second: 452, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.056 [-0.483, 0.618], loss: 2.616068, mean_absolute_error: 32.094181, mean_q: 64.117577\n",
      " 66232/100000: episode: 420, duration: 0.426s, episode steps: 200, steps per second: 469, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.052 [-0.581, 0.771], loss: 3.354627, mean_absolute_error: 32.328739, mean_q: 64.560257\n",
      " 66432/100000: episode: 421, duration: 0.429s, episode steps: 200, steps per second: 466, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.082 [-0.542, 0.548], loss: 1.804491, mean_absolute_error: 32.432873, mean_q: 64.807335\n",
      " 66632/100000: episode: 422, duration: 0.430s, episode steps: 200, steps per second: 465, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.027 [-0.412, 0.552], loss: 4.143505, mean_absolute_error: 32.622372, mean_q: 65.003410\n",
      " 66832/100000: episode: 423, duration: 0.428s, episode steps: 200, steps per second: 468, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.038 [-0.567, 0.602], loss: 4.162627, mean_absolute_error: 32.462673, mean_q: 64.716766\n",
      " 67032/100000: episode: 424, duration: 0.424s, episode steps: 200, steps per second: 472, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.055 [-0.586, 0.714], loss: 4.555667, mean_absolute_error: 32.343819, mean_q: 64.415253\n",
      " 67232/100000: episode: 425, duration: 0.435s, episode steps: 200, steps per second: 459, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.020 [-0.539, 0.533], loss: 3.956392, mean_absolute_error: 32.250580, mean_q: 64.284935\n",
      " 67432/100000: episode: 426, duration: 0.430s, episode steps: 200, steps per second: 465, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.033 [-0.444, 0.538], loss: 4.575165, mean_absolute_error: 32.472599, mean_q: 64.690697\n",
      " 67632/100000: episode: 427, duration: 0.428s, episode steps: 200, steps per second: 468, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.072 [-0.497, 0.532], loss: 5.208431, mean_absolute_error: 32.421528, mean_q: 64.547752\n",
      " 67832/100000: episode: 428, duration: 0.431s, episode steps: 200, steps per second: 465, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.076 [-0.587, 0.869], loss: 3.253882, mean_absolute_error: 32.460777, mean_q: 64.747452\n",
      " 68032/100000: episode: 429, duration: 0.418s, episode steps: 200, steps per second: 478, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.083 [-0.436, 0.448], loss: 3.184849, mean_absolute_error: 32.558266, mean_q: 64.935974\n",
      " 68232/100000: episode: 430, duration: 0.427s, episode steps: 200, steps per second: 469, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.107 [-0.793, 0.828], loss: 2.793613, mean_absolute_error: 32.505909, mean_q: 64.857552\n",
      " 68432/100000: episode: 431, duration: 0.426s, episode steps: 200, steps per second: 470, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.061 [-0.520, 0.573], loss: 2.906992, mean_absolute_error: 32.487560, mean_q: 64.736038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 68632/100000: episode: 432, duration: 0.412s, episode steps: 200, steps per second: 485, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.055 [-0.579, 0.890], loss: 4.660538, mean_absolute_error: 32.895172, mean_q: 65.466476\n",
      " 68832/100000: episode: 433, duration: 0.435s, episode steps: 200, steps per second: 460, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.056 [-0.598, 0.854], loss: 2.000577, mean_absolute_error: 32.723499, mean_q: 65.325897\n",
      " 69032/100000: episode: 434, duration: 0.418s, episode steps: 200, steps per second: 478, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.128 [-0.941, 1.068], loss: 5.611868, mean_absolute_error: 32.852501, mean_q: 65.266502\n",
      " 69232/100000: episode: 435, duration: 0.427s, episode steps: 200, steps per second: 469, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.016 [-0.560, 0.827], loss: 2.986620, mean_absolute_error: 32.646202, mean_q: 65.045570\n",
      " 69432/100000: episode: 436, duration: 0.428s, episode steps: 200, steps per second: 467, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.026 [-0.478, 0.634], loss: 2.648117, mean_absolute_error: 32.675827, mean_q: 65.109406\n",
      " 69632/100000: episode: 437, duration: 0.426s, episode steps: 200, steps per second: 470, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.019 [-0.776, 1.087], loss: 3.822880, mean_absolute_error: 32.447983, mean_q: 64.597679\n",
      " 69832/100000: episode: 438, duration: 0.432s, episode steps: 200, steps per second: 463, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.047 [-0.635, 0.865], loss: 3.324671, mean_absolute_error: 32.721779, mean_q: 65.169601\n",
      " 70032/100000: episode: 439, duration: 0.421s, episode steps: 200, steps per second: 475, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.021 [-0.621, 0.697], loss: 4.144203, mean_absolute_error: 32.670712, mean_q: 65.023743\n",
      " 70232/100000: episode: 440, duration: 0.432s, episode steps: 200, steps per second: 463, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.026 [-0.807, 1.121], loss: 3.801756, mean_absolute_error: 32.820297, mean_q: 65.349518\n",
      " 70432/100000: episode: 441, duration: 0.428s, episode steps: 200, steps per second: 467, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.001 [-0.717, 0.672], loss: 3.974968, mean_absolute_error: 32.797272, mean_q: 65.304565\n",
      " 70632/100000: episode: 442, duration: 0.427s, episode steps: 200, steps per second: 469, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.249 [-1.701, 1.068], loss: 4.317919, mean_absolute_error: 32.854759, mean_q: 65.479218\n",
      " 70832/100000: episode: 443, duration: 0.417s, episode steps: 200, steps per second: 479, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.069 [-0.588, 0.558], loss: 4.554355, mean_absolute_error: 32.647633, mean_q: 64.982140\n",
      " 71032/100000: episode: 444, duration: 0.422s, episode steps: 200, steps per second: 474, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.025 [-0.629, 0.616], loss: 3.836876, mean_absolute_error: 32.521812, mean_q: 64.788513\n",
      " 71232/100000: episode: 445, duration: 0.431s, episode steps: 200, steps per second: 464, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.106 [-0.517, 0.701], loss: 4.509244, mean_absolute_error: 33.305286, mean_q: 66.298195\n",
      " 71432/100000: episode: 446, duration: 0.421s, episode steps: 200, steps per second: 475, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.033 [-0.666, 0.649], loss: 2.314105, mean_absolute_error: 32.960758, mean_q: 65.758987\n",
      " 71632/100000: episode: 447, duration: 0.427s, episode steps: 200, steps per second: 468, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.098 [-0.646, 0.891], loss: 2.934431, mean_absolute_error: 32.658607, mean_q: 65.108459\n",
      " 71832/100000: episode: 448, duration: 0.418s, episode steps: 200, steps per second: 479, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.162 [-0.913, 1.466], loss: 5.139720, mean_absolute_error: 33.005550, mean_q: 65.795769\n",
      " 72032/100000: episode: 449, duration: 0.435s, episode steps: 200, steps per second: 460, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.115 [-0.913, 1.336], loss: 4.470705, mean_absolute_error: 33.007313, mean_q: 65.767220\n",
      " 72232/100000: episode: 450, duration: 0.517s, episode steps: 200, steps per second: 387, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.040 [-0.484, 0.523], loss: 2.001989, mean_absolute_error: 33.045372, mean_q: 65.948624\n",
      " 72432/100000: episode: 451, duration: 0.451s, episode steps: 200, steps per second: 444, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.030 [-0.798, 1.001], loss: 4.830801, mean_absolute_error: 33.207577, mean_q: 66.100372\n",
      " 72632/100000: episode: 452, duration: 0.428s, episode steps: 200, steps per second: 467, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.017 [-0.920, 1.146], loss: 6.279196, mean_absolute_error: 33.251793, mean_q: 66.019524\n",
      " 72832/100000: episode: 453, duration: 0.425s, episode steps: 200, steps per second: 471, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.044 [-0.594, 0.923], loss: 3.578145, mean_absolute_error: 33.400723, mean_q: 66.541451\n",
      " 73032/100000: episode: 454, duration: 0.432s, episode steps: 200, steps per second: 462, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.115 [-0.807, 1.160], loss: 3.602455, mean_absolute_error: 33.344398, mean_q: 66.450401\n",
      " 73232/100000: episode: 455, duration: 0.511s, episode steps: 200, steps per second: 391, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.068 [-0.688, 0.663], loss: 4.221511, mean_absolute_error: 33.409657, mean_q: 66.480194\n",
      " 73432/100000: episode: 456, duration: 0.435s, episode steps: 200, steps per second: 460, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.100 [-0.794, 1.103], loss: 4.240767, mean_absolute_error: 33.394680, mean_q: 66.442963\n",
      " 73632/100000: episode: 457, duration: 0.421s, episode steps: 200, steps per second: 475, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.040 [-0.847, 1.130], loss: 3.153487, mean_absolute_error: 33.238277, mean_q: 66.303879\n",
      " 73832/100000: episode: 458, duration: 0.428s, episode steps: 200, steps per second: 467, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.000 [-0.699, 0.668], loss: 4.946745, mean_absolute_error: 33.516399, mean_q: 66.693542\n",
      " 74032/100000: episode: 459, duration: 0.420s, episode steps: 200, steps per second: 476, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.024 [-0.572, 0.834], loss: 2.828384, mean_absolute_error: 33.793167, mean_q: 67.362000\n",
      " 74232/100000: episode: 460, duration: 0.428s, episode steps: 200, steps per second: 467, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.070 [-0.568, 0.899], loss: 4.197329, mean_absolute_error: 33.826424, mean_q: 67.357948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 74432/100000: episode: 461, duration: 0.415s, episode steps: 200, steps per second: 482, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.117 [-1.128, 1.428], loss: 5.095970, mean_absolute_error: 34.004288, mean_q: 67.668106\n",
      " 74632/100000: episode: 462, duration: 0.418s, episode steps: 200, steps per second: 479, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.060 [-0.587, 0.658], loss: 4.332879, mean_absolute_error: 34.057281, mean_q: 67.793114\n",
      " 74832/100000: episode: 463, duration: 0.429s, episode steps: 200, steps per second: 467, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.060 [-0.634, 0.779], loss: 5.076116, mean_absolute_error: 33.984879, mean_q: 67.569954\n",
      " 75032/100000: episode: 464, duration: 0.417s, episode steps: 200, steps per second: 479, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.092 [-1.034, 1.377], loss: 5.494535, mean_absolute_error: 33.849777, mean_q: 67.334976\n",
      " 75232/100000: episode: 465, duration: 0.460s, episode steps: 200, steps per second: 435, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.005 [-0.572, 0.891], loss: 3.749773, mean_absolute_error: 33.904778, mean_q: 67.503304\n",
      " 75432/100000: episode: 466, duration: 0.686s, episode steps: 200, steps per second: 292, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.046 [-1.039, 1.089], loss: 4.024478, mean_absolute_error: 33.870331, mean_q: 67.431740\n",
      " 75629/100000: episode: 467, duration: 0.492s, episode steps: 197, steps per second: 400, episode reward: 197.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.208 [-1.606, 0.872], loss: 3.900972, mean_absolute_error: 34.209850, mean_q: 68.164505\n",
      " 75805/100000: episode: 468, duration: 0.445s, episode steps: 176, steps per second: 395, episode reward: 176.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.243 [-1.841, 0.748], loss: 4.629963, mean_absolute_error: 34.195553, mean_q: 68.087296\n",
      " 76005/100000: episode: 469, duration: 0.734s, episode steps: 200, steps per second: 272, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.184 [-1.640, 0.899], loss: 6.748682, mean_absolute_error: 34.287895, mean_q: 68.130341\n",
      " 76205/100000: episode: 470, duration: 0.452s, episode steps: 200, steps per second: 443, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.140 [-1.259, 0.598], loss: 3.578815, mean_absolute_error: 34.446026, mean_q: 68.652069\n",
      " 76405/100000: episode: 471, duration: 0.434s, episode steps: 200, steps per second: 461, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.035 [-0.736, 0.854], loss: 5.986819, mean_absolute_error: 34.376976, mean_q: 68.305458\n",
      " 76605/100000: episode: 472, duration: 0.422s, episode steps: 200, steps per second: 474, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.056 [-0.760, 0.917], loss: 6.686644, mean_absolute_error: 34.368427, mean_q: 68.292526\n",
      " 76805/100000: episode: 473, duration: 0.432s, episode steps: 200, steps per second: 463, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.101 [-0.851, 1.030], loss: 6.565845, mean_absolute_error: 34.295200, mean_q: 68.157341\n",
      " 77005/100000: episode: 474, duration: 0.422s, episode steps: 200, steps per second: 473, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.088 [-0.939, 1.402], loss: 6.665625, mean_absolute_error: 34.265678, mean_q: 68.055595\n",
      " 77205/100000: episode: 475, duration: 0.428s, episode steps: 200, steps per second: 468, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.110 [-0.934, 1.376], loss: 4.607881, mean_absolute_error: 34.352413, mean_q: 68.450409\n",
      " 77405/100000: episode: 476, duration: 0.423s, episode steps: 200, steps per second: 473, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.114 [-0.730, 1.162], loss: 5.668775, mean_absolute_error: 34.483845, mean_q: 68.620285\n",
      " 77605/100000: episode: 477, duration: 0.423s, episode steps: 200, steps per second: 473, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.109 [-0.657, 1.095], loss: 5.729845, mean_absolute_error: 34.590664, mean_q: 68.831566\n",
      " 77805/100000: episode: 478, duration: 0.430s, episode steps: 200, steps per second: 465, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.066 [-0.883, 0.799], loss: 4.084168, mean_absolute_error: 34.672752, mean_q: 69.029060\n",
      " 78005/100000: episode: 479, duration: 0.426s, episode steps: 200, steps per second: 470, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.102 [-0.873, 1.323], loss: 5.508066, mean_absolute_error: 34.645004, mean_q: 68.975372\n",
      " 78205/100000: episode: 480, duration: 0.427s, episode steps: 200, steps per second: 468, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.095 [-0.832, 1.288], loss: 5.124016, mean_absolute_error: 34.564816, mean_q: 68.814133\n",
      " 78405/100000: episode: 481, duration: 0.419s, episode steps: 200, steps per second: 477, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.037 [-0.729, 1.127], loss: 5.966032, mean_absolute_error: 34.708233, mean_q: 69.067513\n",
      " 78605/100000: episode: 482, duration: 0.432s, episode steps: 200, steps per second: 463, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.004 [-0.577, 0.595], loss: 3.928067, mean_absolute_error: 34.760738, mean_q: 69.245003\n",
      " 78805/100000: episode: 483, duration: 0.425s, episode steps: 200, steps per second: 470, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.096 [-0.771, 1.254], loss: 5.502003, mean_absolute_error: 34.814220, mean_q: 69.167831\n",
      " 79005/100000: episode: 484, duration: 0.422s, episode steps: 200, steps per second: 474, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.089 [-0.699, 0.983], loss: 5.315300, mean_absolute_error: 34.906105, mean_q: 69.440765\n",
      " 79205/100000: episode: 485, duration: 0.524s, episode steps: 200, steps per second: 382, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.128 [-0.954, 1.527], loss: 5.878390, mean_absolute_error: 34.897892, mean_q: 69.435028\n",
      " 79405/100000: episode: 486, duration: 0.416s, episode steps: 200, steps per second: 481, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.085 [-0.775, 1.103], loss: 4.842483, mean_absolute_error: 34.942314, mean_q: 69.573868\n",
      " 79605/100000: episode: 487, duration: 0.425s, episode steps: 200, steps per second: 471, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.139 [-0.797, 1.307], loss: 5.866205, mean_absolute_error: 35.108047, mean_q: 69.833176\n",
      " 79805/100000: episode: 488, duration: 0.423s, episode steps: 200, steps per second: 473, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.015 [-0.663, 0.801], loss: 5.229671, mean_absolute_error: 34.913208, mean_q: 69.523842\n",
      " 80005/100000: episode: 489, duration: 0.439s, episode steps: 200, steps per second: 456, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.019 [-0.713, 0.819], loss: 5.080872, mean_absolute_error: 35.134518, mean_q: 69.884163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 80205/100000: episode: 490, duration: 0.423s, episode steps: 200, steps per second: 473, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.019 [-0.752, 1.103], loss: 5.626646, mean_absolute_error: 35.291195, mean_q: 70.169830\n",
      " 80405/100000: episode: 491, duration: 0.430s, episode steps: 200, steps per second: 465, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.003 [-0.619, 0.899], loss: 5.931578, mean_absolute_error: 35.346169, mean_q: 70.322266\n",
      " 80605/100000: episode: 492, duration: 0.420s, episode steps: 200, steps per second: 476, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.022 [-0.821, 1.156], loss: 4.322699, mean_absolute_error: 35.148884, mean_q: 70.005562\n",
      " 80805/100000: episode: 493, duration: 0.419s, episode steps: 200, steps per second: 478, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.054 [-0.988, 1.728], loss: 4.978518, mean_absolute_error: 35.402119, mean_q: 70.519920\n",
      " 81005/100000: episode: 494, duration: 0.427s, episode steps: 200, steps per second: 468, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.154 [-1.226, 1.870], loss: 6.533534, mean_absolute_error: 35.637619, mean_q: 70.948616\n",
      " 81205/100000: episode: 495, duration: 0.427s, episode steps: 200, steps per second: 469, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.156 [-0.775, 1.337], loss: 7.120337, mean_absolute_error: 35.586979, mean_q: 70.760712\n",
      " 81405/100000: episode: 496, duration: 0.427s, episode steps: 200, steps per second: 468, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.008 [-0.748, 1.064], loss: 5.259208, mean_absolute_error: 35.648991, mean_q: 70.979851\n",
      " 81605/100000: episode: 497, duration: 0.418s, episode steps: 200, steps per second: 479, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.111 [-0.824, 1.267], loss: 6.932210, mean_absolute_error: 35.700253, mean_q: 70.937592\n",
      " 81805/100000: episode: 498, duration: 0.421s, episode steps: 200, steps per second: 475, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.031 [-0.969, 1.287], loss: 8.510939, mean_absolute_error: 35.462936, mean_q: 70.390144\n",
      " 82005/100000: episode: 499, duration: 0.428s, episode steps: 200, steps per second: 467, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.015 [-0.906, 1.055], loss: 6.854858, mean_absolute_error: 35.764614, mean_q: 71.132713\n",
      " 82205/100000: episode: 500, duration: 0.420s, episode steps: 200, steps per second: 476, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.221 [-1.865, 0.612], loss: 5.067578, mean_absolute_error: 35.923496, mean_q: 71.480125\n",
      " 82405/100000: episode: 501, duration: 0.429s, episode steps: 200, steps per second: 467, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.028 [-0.879, 0.795], loss: 8.491366, mean_absolute_error: 35.976547, mean_q: 71.394958\n",
      " 82605/100000: episode: 502, duration: 0.421s, episode steps: 200, steps per second: 475, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.013 [-0.780, 1.358], loss: 7.527025, mean_absolute_error: 36.103504, mean_q: 71.745338\n",
      " 82805/100000: episode: 503, duration: 0.430s, episode steps: 200, steps per second: 465, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.137 [-0.876, 1.468], loss: 4.425165, mean_absolute_error: 36.089291, mean_q: 72.026535\n",
      " 83005/100000: episode: 504, duration: 0.420s, episode steps: 200, steps per second: 476, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.187 [-0.939, 1.548], loss: 7.605469, mean_absolute_error: 36.221119, mean_q: 72.137238\n",
      " 83205/100000: episode: 505, duration: 0.415s, episode steps: 200, steps per second: 482, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.184 [-0.972, 1.546], loss: 3.936999, mean_absolute_error: 36.240551, mean_q: 72.303680\n",
      " 83405/100000: episode: 506, duration: 0.430s, episode steps: 200, steps per second: 465, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.151 [-0.841, 1.246], loss: 8.358772, mean_absolute_error: 36.718151, mean_q: 72.985474\n",
      " 83605/100000: episode: 507, duration: 0.424s, episode steps: 200, steps per second: 472, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.125 [-0.827, 1.267], loss: 9.954272, mean_absolute_error: 36.565449, mean_q: 72.556923\n",
      " 83805/100000: episode: 508, duration: 0.428s, episode steps: 200, steps per second: 468, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.168 [-1.024, 1.484], loss: 8.517047, mean_absolute_error: 36.648636, mean_q: 72.779831\n",
      " 84005/100000: episode: 509, duration: 0.423s, episode steps: 200, steps per second: 473, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.021 [-0.953, 1.324], loss: 5.799165, mean_absolute_error: 36.697662, mean_q: 72.989525\n",
      " 84205/100000: episode: 510, duration: 0.430s, episode steps: 200, steps per second: 466, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.146 [-1.195, 1.732], loss: 9.060987, mean_absolute_error: 36.931778, mean_q: 73.281448\n",
      " 84405/100000: episode: 511, duration: 0.420s, episode steps: 200, steps per second: 476, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.160 [-0.959, 1.344], loss: 9.903648, mean_absolute_error: 36.850719, mean_q: 73.173988\n",
      " 84605/100000: episode: 512, duration: 0.430s, episode steps: 200, steps per second: 465, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.148 [-0.973, 1.398], loss: 7.857998, mean_absolute_error: 36.669750, mean_q: 72.854378\n",
      " 84805/100000: episode: 513, duration: 0.422s, episode steps: 200, steps per second: 473, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.140 [-0.861, 1.320], loss: 10.269370, mean_absolute_error: 36.450115, mean_q: 72.359261\n",
      " 85005/100000: episode: 514, duration: 0.418s, episode steps: 200, steps per second: 479, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.107 [-0.841, 1.294], loss: 7.826455, mean_absolute_error: 36.655762, mean_q: 72.868317\n",
      " 85205/100000: episode: 515, duration: 0.431s, episode steps: 200, steps per second: 464, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.145 [-1.045, 1.526], loss: 8.657163, mean_absolute_error: 36.706749, mean_q: 72.849442\n",
      " 85405/100000: episode: 516, duration: 0.423s, episode steps: 200, steps per second: 473, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.265 [-2.591, 1.291], loss: 5.468319, mean_absolute_error: 36.687981, mean_q: 72.971199\n",
      " 85605/100000: episode: 517, duration: 0.430s, episode steps: 200, steps per second: 465, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.246 [-2.400, 1.073], loss: 6.969180, mean_absolute_error: 36.800781, mean_q: 73.068092\n",
      " 85805/100000: episode: 518, duration: 0.422s, episode steps: 200, steps per second: 474, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.013 [-0.830, 0.875], loss: 7.897260, mean_absolute_error: 36.937084, mean_q: 73.334671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 86005/100000: episode: 519, duration: 0.420s, episode steps: 200, steps per second: 476, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.141 [-0.915, 0.989], loss: 5.558784, mean_absolute_error: 36.757427, mean_q: 73.104958\n",
      " 86205/100000: episode: 520, duration: 0.431s, episode steps: 200, steps per second: 464, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.006 [-1.017, 1.175], loss: 7.486008, mean_absolute_error: 36.930069, mean_q: 73.335670\n",
      " 86405/100000: episode: 521, duration: 0.428s, episode steps: 200, steps per second: 467, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.133 [-0.915, 1.246], loss: 7.789072, mean_absolute_error: 36.966461, mean_q: 73.471115\n",
      " 86605/100000: episode: 522, duration: 0.429s, episode steps: 200, steps per second: 466, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.113 [-0.812, 1.305], loss: 6.278482, mean_absolute_error: 36.842377, mean_q: 73.314232\n",
      " 86805/100000: episode: 523, duration: 0.418s, episode steps: 200, steps per second: 479, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.054 [-0.882, 1.005], loss: 7.522642, mean_absolute_error: 36.928993, mean_q: 73.391563\n",
      " 87005/100000: episode: 524, duration: 0.422s, episode steps: 200, steps per second: 473, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.076 [-1.008, 1.418], loss: 7.590219, mean_absolute_error: 36.992477, mean_q: 73.494423\n",
      " 87205/100000: episode: 525, duration: 0.422s, episode steps: 200, steps per second: 474, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.007 [-0.607, 0.836], loss: 5.429133, mean_absolute_error: 37.057751, mean_q: 73.724648\n",
      " 87405/100000: episode: 526, duration: 0.425s, episode steps: 200, steps per second: 471, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.008 [-0.585, 0.593], loss: 6.414374, mean_absolute_error: 37.228573, mean_q: 74.046539\n",
      " 87605/100000: episode: 527, duration: 0.528s, episode steps: 200, steps per second: 378, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.003 [-0.827, 1.082], loss: 6.738222, mean_absolute_error: 36.858788, mean_q: 73.283318\n",
      " 87805/100000: episode: 528, duration: 0.604s, episode steps: 200, steps per second: 331, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.000 [-0.709, 0.582], loss: 8.038660, mean_absolute_error: 37.148041, mean_q: 73.804649\n",
      " 88005/100000: episode: 529, duration: 0.747s, episode steps: 200, steps per second: 268, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.001 [-0.558, 0.558], loss: 6.526086, mean_absolute_error: 37.170242, mean_q: 73.962288\n",
      " 88205/100000: episode: 530, duration: 0.537s, episode steps: 200, steps per second: 372, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.097 [-0.896, 1.168], loss: 7.982771, mean_absolute_error: 37.243389, mean_q: 74.052193\n",
      " 88405/100000: episode: 531, duration: 0.480s, episode steps: 200, steps per second: 417, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.021 [-0.531, 0.594], loss: 5.160999, mean_absolute_error: 37.341881, mean_q: 74.391212\n",
      " 88605/100000: episode: 532, duration: 0.523s, episode steps: 200, steps per second: 383, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.005 [-0.659, 0.823], loss: 9.197258, mean_absolute_error: 37.592575, mean_q: 74.713722\n",
      " 88805/100000: episode: 533, duration: 0.423s, episode steps: 200, steps per second: 473, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.001 [-0.589, 0.552], loss: 9.095172, mean_absolute_error: 37.381180, mean_q: 74.289436\n",
      " 89005/100000: episode: 534, duration: 0.564s, episode steps: 200, steps per second: 354, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.011 [-0.586, 0.705], loss: 8.432925, mean_absolute_error: 37.499069, mean_q: 74.525345\n",
      " 89205/100000: episode: 535, duration: 0.475s, episode steps: 200, steps per second: 421, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.011 [-0.629, 0.625], loss: 12.410392, mean_absolute_error: 37.110912, mean_q: 73.651260\n",
      " 89405/100000: episode: 536, duration: 0.631s, episode steps: 200, steps per second: 317, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.104 [-0.905, 0.782], loss: 8.950746, mean_absolute_error: 37.185860, mean_q: 73.897484\n",
      " 89605/100000: episode: 537, duration: 0.482s, episode steps: 200, steps per second: 415, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.031 [-0.961, 1.000], loss: 9.530464, mean_absolute_error: 37.148144, mean_q: 73.839432\n",
      " 89805/100000: episode: 538, duration: 0.620s, episode steps: 200, steps per second: 322, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.006 [-0.545, 0.630], loss: 10.270530, mean_absolute_error: 37.311268, mean_q: 74.010788\n",
      " 90005/100000: episode: 539, duration: 0.448s, episode steps: 200, steps per second: 447, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.012 [-0.880, 0.570], loss: 5.326883, mean_absolute_error: 37.154152, mean_q: 73.987152\n",
      " 90205/100000: episode: 540, duration: 0.461s, episode steps: 200, steps per second: 434, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.011 [-0.512, 0.603], loss: 8.207541, mean_absolute_error: 37.265591, mean_q: 74.082947\n",
      " 90405/100000: episode: 541, duration: 0.426s, episode steps: 200, steps per second: 470, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.005 [-0.867, 1.103], loss: 8.798786, mean_absolute_error: 37.325375, mean_q: 74.148643\n",
      " 90605/100000: episode: 542, duration: 0.470s, episode steps: 200, steps per second: 425, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.080 [-0.678, 0.881], loss: 7.476504, mean_absolute_error: 37.093727, mean_q: 73.795006\n",
      " 90805/100000: episode: 543, duration: 0.422s, episode steps: 200, steps per second: 474, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.009 [-0.559, 0.734], loss: 8.802411, mean_absolute_error: 37.229404, mean_q: 73.989647\n",
      " 91005/100000: episode: 544, duration: 0.447s, episode steps: 200, steps per second: 447, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.013 [-0.580, 0.550], loss: 7.891558, mean_absolute_error: 37.060726, mean_q: 73.714867\n",
      " 91205/100000: episode: 545, duration: 0.422s, episode steps: 200, steps per second: 474, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.009 [-0.573, 0.524], loss: 6.366250, mean_absolute_error: 37.287457, mean_q: 74.217606\n",
      " 91405/100000: episode: 546, duration: 0.457s, episode steps: 200, steps per second: 438, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.008 [-0.897, 0.589], loss: 8.228452, mean_absolute_error: 37.441940, mean_q: 74.469208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 91605/100000: episode: 547, duration: 0.436s, episode steps: 200, steps per second: 458, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.002 [-0.654, 0.810], loss: 11.890256, mean_absolute_error: 37.362675, mean_q: 74.119553\n",
      " 91805/100000: episode: 548, duration: 0.417s, episode steps: 200, steps per second: 479, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.019 [-0.579, 0.580], loss: 4.218125, mean_absolute_error: 37.203476, mean_q: 74.154480\n",
      " 92005/100000: episode: 549, duration: 0.631s, episode steps: 200, steps per second: 317, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.126 [-0.800, 0.950], loss: 9.351850, mean_absolute_error: 37.700108, mean_q: 74.895935\n",
      " 92205/100000: episode: 550, duration: 0.836s, episode steps: 200, steps per second: 239, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.117 [-0.933, 0.949], loss: 7.586250, mean_absolute_error: 37.415730, mean_q: 74.460381\n",
      " 92405/100000: episode: 551, duration: 0.677s, episode steps: 200, steps per second: 295, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.106 [-1.014, 1.288], loss: 6.874822, mean_absolute_error: 37.570797, mean_q: 74.795898\n",
      " 92605/100000: episode: 552, duration: 0.617s, episode steps: 200, steps per second: 324, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.035 [-0.753, 1.080], loss: 8.602063, mean_absolute_error: 37.733276, mean_q: 75.036545\n",
      " 92805/100000: episode: 553, duration: 0.816s, episode steps: 200, steps per second: 245, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.136 [-0.805, 1.116], loss: 8.551715, mean_absolute_error: 37.861156, mean_q: 75.242096\n",
      " 93005/100000: episode: 554, duration: 0.657s, episode steps: 200, steps per second: 304, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.004 [-0.885, 0.870], loss: 7.838566, mean_absolute_error: 37.492420, mean_q: 74.595612\n",
      " 93205/100000: episode: 555, duration: 0.472s, episode steps: 200, steps per second: 424, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.004 [-0.837, 0.776], loss: 10.125632, mean_absolute_error: 37.357357, mean_q: 74.124931\n",
      " 93405/100000: episode: 556, duration: 0.494s, episode steps: 200, steps per second: 405, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.081 [-0.764, 1.135], loss: 9.017309, mean_absolute_error: 37.366390, mean_q: 74.273270\n",
      " 93605/100000: episode: 557, duration: 0.767s, episode steps: 200, steps per second: 261, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.003 [-0.599, 0.795], loss: 6.512450, mean_absolute_error: 37.254696, mean_q: 74.120598\n",
      " 93805/100000: episode: 558, duration: 0.557s, episode steps: 200, steps per second: 359, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.006 [-0.763, 0.661], loss: 5.175177, mean_absolute_error: 37.416046, mean_q: 74.534645\n",
      " 94005/100000: episode: 559, duration: 0.438s, episode steps: 200, steps per second: 457, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.081 [-0.816, 1.298], loss: 11.005604, mean_absolute_error: 37.576351, mean_q: 74.619362\n",
      " 94205/100000: episode: 560, duration: 0.514s, episode steps: 200, steps per second: 389, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.015 [-0.602, 0.861], loss: 6.807955, mean_absolute_error: 37.574867, mean_q: 74.793182\n",
      " 94383/100000: episode: 561, duration: 0.411s, episode steps: 178, steps per second: 434, episode reward: 178.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.343 [-2.440, 0.677], loss: 10.776778, mean_absolute_error: 37.351086, mean_q: 74.190849\n",
      " 94583/100000: episode: 562, duration: 0.522s, episode steps: 200, steps per second: 383, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.006 [-0.600, 0.533], loss: 9.250936, mean_absolute_error: 37.567543, mean_q: 74.627480\n",
      " 94783/100000: episode: 563, duration: 0.673s, episode steps: 200, steps per second: 297, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.029 [-0.897, 0.927], loss: 9.566219, mean_absolute_error: 37.451050, mean_q: 74.381477\n",
      " 94983/100000: episode: 564, duration: 0.697s, episode steps: 200, steps per second: 287, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.070 [-0.676, 0.905], loss: 8.435925, mean_absolute_error: 37.382374, mean_q: 74.279610\n",
      " 95183/100000: episode: 565, duration: 0.473s, episode steps: 200, steps per second: 423, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.080 [-0.954, 1.132], loss: 11.749638, mean_absolute_error: 37.215004, mean_q: 73.780853\n",
      " 95383/100000: episode: 566, duration: 0.434s, episode steps: 200, steps per second: 461, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.009 [-0.704, 0.818], loss: 9.805726, mean_absolute_error: 37.142593, mean_q: 73.772552\n",
      " 95583/100000: episode: 567, duration: 0.482s, episode steps: 200, steps per second: 415, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.038 [-0.600, 0.953], loss: 8.871050, mean_absolute_error: 37.110661, mean_q: 73.686783\n",
      " 95783/100000: episode: 568, duration: 0.459s, episode steps: 200, steps per second: 435, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.018 [-0.631, 0.611], loss: 5.014942, mean_absolute_error: 37.095360, mean_q: 73.915306\n",
      " 95983/100000: episode: 569, duration: 0.453s, episode steps: 200, steps per second: 442, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.284 [-2.825, 1.460], loss: 13.760523, mean_absolute_error: 37.185459, mean_q: 73.620552\n",
      " 96183/100000: episode: 570, duration: 0.441s, episode steps: 200, steps per second: 454, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.062 [-0.974, 1.063], loss: 9.988089, mean_absolute_error: 36.997459, mean_q: 73.461571\n",
      " 96383/100000: episode: 571, duration: 0.438s, episode steps: 200, steps per second: 456, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.164 [-1.494, 0.840], loss: 8.741131, mean_absolute_error: 36.992348, mean_q: 73.566223\n",
      " 96583/100000: episode: 572, duration: 0.486s, episode steps: 200, steps per second: 411, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.096 [-0.617, 0.923], loss: 7.386858, mean_absolute_error: 37.046009, mean_q: 73.678017\n",
      " 96783/100000: episode: 573, duration: 0.437s, episode steps: 200, steps per second: 458, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.029 [-0.602, 0.477], loss: 8.028628, mean_absolute_error: 36.699688, mean_q: 72.872910\n",
      " 96983/100000: episode: 574, duration: 0.484s, episode steps: 200, steps per second: 413, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.067 [-1.022, 1.001], loss: 10.065941, mean_absolute_error: 36.830936, mean_q: 73.091705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 97183/100000: episode: 575, duration: 0.436s, episode steps: 200, steps per second: 458, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.071 [-0.622, 0.942], loss: 7.779481, mean_absolute_error: 37.139889, mean_q: 73.827019\n",
      " 97383/100000: episode: 576, duration: 0.456s, episode steps: 200, steps per second: 439, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.102 [-1.175, 1.263], loss: 11.627105, mean_absolute_error: 36.990524, mean_q: 73.347878\n",
      " 97583/100000: episode: 577, duration: 0.432s, episode steps: 200, steps per second: 463, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.094 [-0.849, 1.096], loss: 7.355681, mean_absolute_error: 37.070419, mean_q: 73.672409\n",
      " 97783/100000: episode: 578, duration: 0.458s, episode steps: 200, steps per second: 437, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.108 [-0.893, 1.356], loss: 10.478771, mean_absolute_error: 37.065983, mean_q: 73.531677\n",
      " 97983/100000: episode: 579, duration: 0.443s, episode steps: 200, steps per second: 451, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.121 [-0.825, 1.305], loss: 6.123943, mean_absolute_error: 36.772648, mean_q: 73.208504\n",
      " 98183/100000: episode: 580, duration: 0.432s, episode steps: 200, steps per second: 463, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.134 [-1.292, 1.203], loss: 13.228145, mean_absolute_error: 37.105473, mean_q: 73.581726\n",
      " 98383/100000: episode: 581, duration: 0.502s, episode steps: 200, steps per second: 399, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: 0.135 [-0.906, 1.357], loss: 5.789438, mean_absolute_error: 37.127037, mean_q: 74.008858\n",
      " 98583/100000: episode: 582, duration: 0.429s, episode steps: 200, steps per second: 466, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.117 [-1.168, 1.453], loss: 8.767932, mean_absolute_error: 37.226021, mean_q: 74.013901\n",
      " 98783/100000: episode: 583, duration: 0.488s, episode steps: 200, steps per second: 409, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.045 [-0.736, 0.797], loss: 9.251051, mean_absolute_error: 37.379192, mean_q: 74.244476\n",
      " 98983/100000: episode: 584, duration: 0.431s, episode steps: 200, steps per second: 464, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.191 [-1.035, 1.530], loss: 8.717652, mean_absolute_error: 37.476574, mean_q: 74.579697\n",
      " 99183/100000: episode: 585, duration: 0.489s, episode steps: 200, steps per second: 409, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.181 [-1.507, 1.530], loss: 8.912790, mean_absolute_error: 37.610615, mean_q: 74.902870\n",
      " 99383/100000: episode: 586, duration: 0.435s, episode steps: 200, steps per second: 460, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: 0.133 [-0.788, 1.104], loss: 13.512409, mean_absolute_error: 37.866776, mean_q: 74.987190\n",
      " 99583/100000: episode: 587, duration: 0.498s, episode steps: 200, steps per second: 401, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.159 [-0.922, 0.996], loss: 8.904076, mean_absolute_error: 37.545940, mean_q: 74.649315\n",
      " 99783/100000: episode: 588, duration: 0.430s, episode steps: 200, steps per second: 465, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.105 [-0.619, 0.975], loss: 7.166502, mean_absolute_error: 37.474445, mean_q: 74.513229\n",
      " 99983/100000: episode: 589, duration: 0.470s, episode steps: 200, steps per second: 426, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: 0.105 [-0.902, 1.172], loss: 10.168702, mean_absolute_error: 37.539307, mean_q: 74.431274\n",
      "done, took 247.231 seconds\n"
     ]
    }
   ],
   "source": [
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "history = dqn.fit(env, nb_steps=100000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] cartpole_dqn.hdf5 already exists - overwrite? [y/n]y\n",
      "[TIP] Next time specify overwrite=True!\n"
     ]
    }
   ],
   "source": [
    "with open('cartpole_history.json', 'w') as f:\n",
    "    json.dump(history.history, f)\n",
    "\n",
    "dqn.save_weights('cartpole_dqn.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 200.000, steps: 200\n",
      "Episode 2: reward: 200.000, steps: 200\n",
      "Episode 3: reward: 200.000, steps: 200\n",
      "Episode 4: reward: 200.000, steps: 200\n",
      "Episode 5: reward: 200.000, steps: 200\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "dqn.load_weights('cartpole_dqn.hdf5')\n",
    "dqn.test(env, nb_episodes=5, visualize=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = None\n",
    "with open('cartpole_history.json', 'r') as f:\n",
    "    y = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title('DQN_rewards')\n",
    "ax.set_xlabel('episodes')\n",
    "ax.set_ylabel('rewards')\n",
    "episode_reward = np.array(y['episode_reward'])\n",
    "ax.plot(episode_reward)\n",
    "plt.savefig('CartPole_DQN_rewards')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "589"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(episode_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. reward:  188.867075665\n"
     ]
    }
   ],
   "source": [
    "# 1) Avg. reward over the last 100 consecutive episodes\n",
    "print('Avg. reward: ', np.mean(episode_reward[100:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) How many episodes before you encounter 100 consecutive episodes\n",
    "b4 = False\n",
    "count = 0\n",
    "ans = None\n",
    "for idx, reward in enumerate(episode_reward > 150):\n",
    "    count = count + 1 if b4 and reward else 0\n",
    "    if count == 100:\n",
    "        ans = idx\n",
    "        break\n",
    "    b4 = reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "422"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) any other informatoins\n",
    "* The reward for every episodes\n",
    "* DQN network summery\n",
    "\n",
    "Are shown abeve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
