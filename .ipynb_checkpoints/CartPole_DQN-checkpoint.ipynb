{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "import gym\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent and Environment Description\n",
    "\n",
    "* observation:  \n",
    "    (Cart Position ,  Cart Velocity ,  Pole Angle ,  Pole Velocity At Tip)\n",
    " \n",
    "* Actions:  \n",
    "    0 Push cart to the left; 1 Push cart to the right\n",
    "\n",
    "* Reward:  \n",
    "    Reward is for every step taken ,  including the termination step\n",
    "\n",
    "* Starting State:  \n",
    "    All observations are assigned a uniform random value between +-0.05\n",
    "\n",
    "* Episode Termination:  \n",
    "    Pole Angle is more than ±12°\n",
    "    Cart Position is more than ± 2.4 \n",
    "    (center of the cart reaches the edge of the display)\n",
    "    Episode length is greater than 200\n",
    "\n",
    "* reference:  \n",
    "    http:  //neuro-educator.com/rl1/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# input: (n x 4) ( Cart Position, Cart Vel, Pole Angle, Pole Vel)\n",
    "model.add(Flatten(input_shape=(1, env.observation_space.shape[0])))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# output: (n x 3) (left, no, right)\n",
    "model.add(Dense(env.action_space.n))\n",
    "model.add(Activation('linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_3 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2)                 34        \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 658\n",
      "Trainable params: 658\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory replay\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "\n",
    "# epsilon greedy algorithm\n",
    "policy = EpsGreedyQPolicy(eps=0.001)\n",
    "\n",
    "dqn = DQNAgent(model=model, nb_actions=env.action_space.n, gamma=0.99, memory=memory,\n",
    "              nb_steps_warmup=10, target_model_update=1e-2, policy=policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n",
      "    10/50000: episode: 1, duration: 0.081s, episode steps: 10, steps per second: 123, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.121 [-1.988, 3.069], loss: --, mean_absolute_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/syuntoku14/.pyenv/versions/anaconda3-5.1.0/envs/coursera/lib/python3.6/site-packages/rl/memory.py:29: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    20/50000: episode: 2, duration: 0.659s, episode steps: 10, steps per second: 15, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.148 [-1.945, 3.031], loss: 0.451599, mean_absolute_error: 0.542596, mean_q: 0.313780\n",
      "    29/50000: episode: 3, duration: 0.019s, episode steps: 9, steps per second: 464, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.137 [-1.766, 2.750], loss: 0.313576, mean_absolute_error: 0.472863, mean_q: 0.496396\n",
      "    38/50000: episode: 4, duration: 0.021s, episode steps: 9, steps per second: 432, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.130 [-1.728, 2.733], loss: 0.210792, mean_absolute_error: 0.419664, mean_q: 0.723855\n",
      "    46/50000: episode: 5, duration: 0.017s, episode steps: 8, steps per second: 466, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.175 [-1.556, 2.597], loss: 0.165984, mean_absolute_error: 0.384656, mean_q: 0.938905\n",
      "    55/50000: episode: 6, duration: 0.019s, episode steps: 9, steps per second: 477, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.138 [-1.765, 2.801], loss: 0.152438, mean_absolute_error: 0.362245, mean_q: 1.063103\n",
      "    66/50000: episode: 7, duration: 0.021s, episode steps: 11, steps per second: 514, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.130 [-2.144, 3.293], loss: 0.141641, mean_absolute_error: 0.354636, mean_q: 1.215127\n",
      "    76/50000: episode: 8, duration: 0.020s, episode steps: 10, steps per second: 495, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.172 [-1.908, 3.120], loss: 0.131065, mean_absolute_error: 0.361537, mean_q: 1.315707\n",
      "    85/50000: episode: 9, duration: 0.018s, episode steps: 9, steps per second: 499, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.144 [-1.776, 2.841], loss: 0.130898, mean_absolute_error: 0.375362, mean_q: 1.400355\n",
      "    96/50000: episode: 10, duration: 0.023s, episode steps: 11, steps per second: 476, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.130 [-2.146, 3.264], loss: 0.103595, mean_absolute_error: 0.351856, mean_q: 1.531490\n",
      "   104/50000: episode: 11, duration: 0.017s, episode steps: 8, steps per second: 483, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.170 [-1.526, 2.603], loss: 0.090289, mean_absolute_error: 0.349834, mean_q: 1.592656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/syuntoku14/.pyenv/versions/anaconda3-5.1.0/envs/coursera/lib/python3.6/site-packages/rl/memory.py:29: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
      "/home/syuntoku14/.pyenv/versions/anaconda3-5.1.0/envs/coursera/lib/python3.6/site-packages/rl/memory.py:29: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   114/50000: episode: 12, duration: 0.034s, episode steps: 10, steps per second: 296, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.141 [-1.931, 3.071], loss: 0.088240, mean_absolute_error: 0.326159, mean_q: 1.658745\n",
      "   124/50000: episode: 13, duration: 0.032s, episode steps: 10, steps per second: 310, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.128 [-2.000, 3.029], loss: 0.103528, mean_absolute_error: 0.351240, mean_q: 1.737501\n",
      "   132/50000: episode: 14, duration: 0.017s, episode steps: 8, steps per second: 462, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.145 [-1.611, 2.610], loss: 0.068482, mean_absolute_error: 0.294748, mean_q: 1.820973\n",
      "   142/50000: episode: 15, duration: 0.021s, episode steps: 10, steps per second: 473, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.134 [-1.959, 3.073], loss: 0.090653, mean_absolute_error: 0.316089, mean_q: 1.857231\n",
      "   151/50000: episode: 16, duration: 0.019s, episode steps: 9, steps per second: 482, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.173 [-1.716, 2.825], loss: 0.082657, mean_absolute_error: 0.305010, mean_q: 1.873062\n",
      "   160/50000: episode: 17, duration: 0.018s, episode steps: 9, steps per second: 496, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.147 [-1.731, 2.802], loss: 0.078261, mean_absolute_error: 0.307121, mean_q: 1.869807\n",
      "   169/50000: episode: 18, duration: 0.021s, episode steps: 9, steps per second: 439, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.162 [-1.721, 2.810], loss: 0.063161, mean_absolute_error: 0.287902, mean_q: 1.943930\n",
      "   179/50000: episode: 19, duration: 0.022s, episode steps: 10, steps per second: 455, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.120 [-1.926, 2.976], loss: 0.078664, mean_absolute_error: 0.290542, mean_q: 2.069181\n",
      "   189/50000: episode: 20, duration: 0.021s, episode steps: 10, steps per second: 477, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.144 [-2.001, 3.101], loss: 0.089225, mean_absolute_error: 0.328663, mean_q: 1.916497\n",
      "   198/50000: episode: 21, duration: 0.019s, episode steps: 9, steps per second: 485, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.125 [-1.795, 2.805], loss: 0.071388, mean_absolute_error: 0.292465, mean_q: 2.052672\n",
      "   206/50000: episode: 22, duration: 0.029s, episode steps: 8, steps per second: 272, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.140 [-1.611, 2.598], loss: 0.074642, mean_absolute_error: 0.301746, mean_q: 2.018437\n",
      "   215/50000: episode: 23, duration: 0.036s, episode steps: 9, steps per second: 251, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.124 [-1.804, 2.752], loss: 0.063339, mean_absolute_error: 0.282146, mean_q: 2.141074\n",
      "   225/50000: episode: 24, duration: 0.022s, episode steps: 10, steps per second: 462, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.125 [-1.977, 3.052], loss: 0.058617, mean_absolute_error: 0.274045, mean_q: 2.183264\n",
      "   235/50000: episode: 25, duration: 0.021s, episode steps: 10, steps per second: 480, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.139 [-1.934, 3.079], loss: 0.056104, mean_absolute_error: 0.264927, mean_q: 2.262199\n",
      "   245/50000: episode: 26, duration: 0.021s, episode steps: 10, steps per second: 473, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.105 [-1.963, 2.954], loss: 0.067908, mean_absolute_error: 0.292899, mean_q: 2.241287\n",
      "   254/50000: episode: 27, duration: 0.018s, episode steps: 9, steps per second: 487, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.149 [-1.811, 2.832], loss: 0.064753, mean_absolute_error: 0.278978, mean_q: 2.330439\n",
      "   262/50000: episode: 28, duration: 0.018s, episode steps: 8, steps per second: 438, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.139 [-1.609, 2.514], loss: 0.055295, mean_absolute_error: 0.285953, mean_q: 2.329798\n",
      "   270/50000: episode: 29, duration: 0.018s, episode steps: 8, steps per second: 454, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.179 [-1.537, 2.599], loss: 0.052687, mean_absolute_error: 0.275167, mean_q: 2.400292\n",
      "   279/50000: episode: 30, duration: 0.019s, episode steps: 9, steps per second: 486, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.157 [-1.750, 2.814], loss: 0.057191, mean_absolute_error: 0.275315, mean_q: 2.450940\n",
      "   289/50000: episode: 31, duration: 0.021s, episode steps: 10, steps per second: 483, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.142 [-1.957, 3.070], loss: 0.058277, mean_absolute_error: 0.285247, mean_q: 2.451945\n",
      "   299/50000: episode: 32, duration: 0.034s, episode steps: 10, steps per second: 298, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.131 [-1.988, 3.049], loss: 0.057718, mean_absolute_error: 0.279641, mean_q: 2.530225\n",
      "   307/50000: episode: 33, duration: 0.018s, episode steps: 8, steps per second: 454, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.159 [-1.542, 2.572], loss: 0.048486, mean_absolute_error: 0.261770, mean_q: 2.623284\n",
      "   317/50000: episode: 34, duration: 0.021s, episode steps: 10, steps per second: 467, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.139 [-1.931, 3.061], loss: 0.059370, mean_absolute_error: 0.278866, mean_q: 2.627665\n",
      "   326/50000: episode: 35, duration: 0.023s, episode steps: 9, steps per second: 395, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.155 [-1.717, 2.770], loss: 0.050805, mean_absolute_error: 0.266571, mean_q: 2.769749\n",
      "   337/50000: episode: 36, duration: 0.023s, episode steps: 11, steps per second: 475, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.146 [-2.138, 3.306], loss: 0.053725, mean_absolute_error: 0.283311, mean_q: 2.745502\n",
      "   346/50000: episode: 37, duration: 0.019s, episode steps: 9, steps per second: 481, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.148 [-1.776, 2.805], loss: 0.042921, mean_absolute_error: 0.260753, mean_q: 2.851419\n",
      "   354/50000: episode: 38, duration: 0.017s, episode steps: 8, steps per second: 469, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.161 [-1.550, 2.573], loss: 0.052690, mean_absolute_error: 0.284311, mean_q: 2.857697\n",
      "   363/50000: episode: 39, duration: 0.019s, episode steps: 9, steps per second: 485, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.175 [-1.770, 2.906], loss: 0.051450, mean_absolute_error: 0.286046, mean_q: 2.908161\n",
      "   373/50000: episode: 40, duration: 0.021s, episode steps: 10, steps per second: 487, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.148 [-1.904, 3.047], loss: 0.041937, mean_absolute_error: 0.267835, mean_q: 3.018487\n",
      "   381/50000: episode: 41, duration: 0.017s, episode steps: 8, steps per second: 471, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.167 [-1.545, 2.555], loss: 0.043967, mean_absolute_error: 0.266508, mean_q: 3.041450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   391/50000: episode: 42, duration: 0.021s, episode steps: 10, steps per second: 479, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.127 [-1.943, 2.955], loss: 0.044421, mean_absolute_error: 0.281560, mean_q: 3.075160\n",
      "   401/50000: episode: 43, duration: 0.031s, episode steps: 10, steps per second: 318, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.139 [-1.995, 3.069], loss: 0.041997, mean_absolute_error: 0.279064, mean_q: 3.123060\n",
      "   410/50000: episode: 44, duration: 0.020s, episode steps: 9, steps per second: 457, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.172 [-1.725, 2.898], loss: 0.058647, mean_absolute_error: 0.307885, mean_q: 3.048997\n",
      "   419/50000: episode: 45, duration: 0.019s, episode steps: 9, steps per second: 469, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.155 [-1.750, 2.808], loss: 0.047444, mean_absolute_error: 0.281512, mean_q: 3.249151\n",
      "   429/50000: episode: 46, duration: 0.021s, episode steps: 10, steps per second: 475, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.138 [-1.960, 3.083], loss: 0.044565, mean_absolute_error: 0.285647, mean_q: 3.212176\n",
      "   439/50000: episode: 47, duration: 0.021s, episode steps: 10, steps per second: 482, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.144 [-1.953, 3.074], loss: 0.054428, mean_absolute_error: 0.310757, mean_q: 3.193870\n",
      "   448/50000: episode: 48, duration: 0.019s, episode steps: 9, steps per second: 483, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.164 [-1.783, 2.871], loss: 0.054456, mean_absolute_error: 0.317781, mean_q: 3.274342\n",
      "   458/50000: episode: 49, duration: 0.020s, episode steps: 10, steps per second: 501, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.145 [-1.949, 3.038], loss: 0.031502, mean_absolute_error: 0.263933, mean_q: 3.505156\n",
      "   467/50000: episode: 50, duration: 0.018s, episode steps: 9, steps per second: 487, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.138 [-1.735, 2.749], loss: 0.032253, mean_absolute_error: 0.281089, mean_q: 3.425452\n",
      "   477/50000: episode: 51, duration: 0.021s, episode steps: 10, steps per second: 485, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.119 [-1.971, 2.984], loss: 0.022334, mean_absolute_error: 0.261335, mean_q: 3.568001\n",
      "   486/50000: episode: 52, duration: 0.029s, episode steps: 9, steps per second: 306, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.148 [-1.764, 2.780], loss: 0.038248, mean_absolute_error: 0.289725, mean_q: 3.513841\n",
      "   496/50000: episode: 53, duration: 0.023s, episode steps: 10, steps per second: 432, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.160 [-1.909, 3.097], loss: 0.028705, mean_absolute_error: 0.278329, mean_q: 3.684200\n",
      "   506/50000: episode: 54, duration: 0.022s, episode steps: 10, steps per second: 463, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.116 [-1.976, 3.020], loss: 0.028346, mean_absolute_error: 0.284804, mean_q: 3.566573\n",
      "   515/50000: episode: 55, duration: 0.019s, episode steps: 9, steps per second: 465, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.139 [-1.790, 2.841], loss: 0.027570, mean_absolute_error: 0.278878, mean_q: 3.716182\n",
      "   525/50000: episode: 56, duration: 0.021s, episode steps: 10, steps per second: 475, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.129 [-1.962, 2.984], loss: 0.022476, mean_absolute_error: 0.279990, mean_q: 3.758776\n",
      "   535/50000: episode: 57, duration: 0.022s, episode steps: 10, steps per second: 459, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.156 [-1.923, 3.061], loss: 0.027036, mean_absolute_error: 0.291257, mean_q: 3.760555\n",
      "   545/50000: episode: 58, duration: 0.021s, episode steps: 10, steps per second: 483, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.126 [-1.991, 3.056], loss: 0.016045, mean_absolute_error: 0.267892, mean_q: 3.919760\n",
      "   555/50000: episode: 59, duration: 0.021s, episode steps: 10, steps per second: 486, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.137 [-1.964, 3.045], loss: 0.022478, mean_absolute_error: 0.289106, mean_q: 3.763952\n",
      "   564/50000: episode: 60, duration: 0.019s, episode steps: 9, steps per second: 474, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.137 [-1.810, 2.779], loss: 0.022661, mean_absolute_error: 0.298483, mean_q: 3.761034\n",
      "   572/50000: episode: 61, duration: 0.017s, episode steps: 8, steps per second: 470, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.157 [-1.550, 2.590], loss: 0.018843, mean_absolute_error: 0.274832, mean_q: 4.041612\n",
      "   582/50000: episode: 62, duration: 0.023s, episode steps: 10, steps per second: 437, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.125 [-1.971, 2.964], loss: 0.020562, mean_absolute_error: 0.279508, mean_q: 4.019588\n",
      "   591/50000: episode: 63, duration: 0.026s, episode steps: 9, steps per second: 343, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.153 [-1.722, 2.818], loss: 0.012533, mean_absolute_error: 0.286521, mean_q: 4.100191\n",
      "   600/50000: episode: 64, duration: 0.019s, episode steps: 9, steps per second: 464, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.140 [-1.779, 2.809], loss: 0.025071, mean_absolute_error: 0.304841, mean_q: 3.943578\n",
      "   610/50000: episode: 65, duration: 0.021s, episode steps: 10, steps per second: 472, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.152 [-1.933, 3.058], loss: 0.025299, mean_absolute_error: 0.314375, mean_q: 4.025841\n",
      "   619/50000: episode: 66, duration: 0.019s, episode steps: 9, steps per second: 467, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.162 [-1.778, 2.873], loss: 0.019105, mean_absolute_error: 0.303006, mean_q: 4.232028\n",
      "   629/50000: episode: 67, duration: 0.021s, episode steps: 10, steps per second: 480, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.148 [-1.963, 3.070], loss: 0.014641, mean_absolute_error: 0.290447, mean_q: 4.226832\n",
      "   639/50000: episode: 68, duration: 0.021s, episode steps: 10, steps per second: 484, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.140 [-1.921, 3.025], loss: 0.015504, mean_absolute_error: 0.312792, mean_q: 4.058478\n",
      "   649/50000: episode: 69, duration: 0.023s, episode steps: 10, steps per second: 433, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.116 [-1.971, 2.975], loss: 0.020340, mean_absolute_error: 0.311673, mean_q: 4.087650\n",
      "   658/50000: episode: 70, duration: 0.019s, episode steps: 9, steps per second: 472, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.142 [-1.782, 2.838], loss: 0.012798, mean_absolute_error: 0.293869, mean_q: 4.202336\n",
      "   667/50000: episode: 71, duration: 0.019s, episode steps: 9, steps per second: 479, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.149 [-1.722, 2.794], loss: 0.018636, mean_absolute_error: 0.310940, mean_q: 4.248205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   675/50000: episode: 72, duration: 0.022s, episode steps: 8, steps per second: 359, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.167 [-1.531, 2.562], loss: 0.010635, mean_absolute_error: 0.305985, mean_q: 4.188293\n",
      "   684/50000: episode: 73, duration: 0.028s, episode steps: 9, steps per second: 323, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.125 [-1.806, 2.792], loss: 0.014711, mean_absolute_error: 0.317323, mean_q: 4.245582\n",
      "   692/50000: episode: 74, duration: 0.018s, episode steps: 8, steps per second: 456, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.135 [-1.593, 2.546], loss: 0.010721, mean_absolute_error: 0.312454, mean_q: 4.346805\n",
      "   702/50000: episode: 75, duration: 0.022s, episode steps: 10, steps per second: 465, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.130 [-1.930, 3.000], loss: 0.011473, mean_absolute_error: 0.312949, mean_q: 4.343718\n",
      "   712/50000: episode: 76, duration: 0.021s, episode steps: 10, steps per second: 473, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.147 [-1.923, 3.064], loss: 0.014148, mean_absolute_error: 0.331896, mean_q: 4.444453\n",
      "   722/50000: episode: 77, duration: 0.022s, episode steps: 10, steps per second: 462, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.132 [-1.964, 3.063], loss: 0.012617, mean_absolute_error: 0.329562, mean_q: 4.459006\n",
      "   731/50000: episode: 78, duration: 0.019s, episode steps: 9, steps per second: 469, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.132 [-1.759, 2.763], loss: 0.020516, mean_absolute_error: 0.348115, mean_q: 4.418475\n",
      "   739/50000: episode: 79, duration: 0.017s, episode steps: 8, steps per second: 471, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.142 [-1.571, 2.566], loss: 0.007518, mean_absolute_error: 0.339621, mean_q: 4.604988\n",
      "   749/50000: episode: 80, duration: 0.021s, episode steps: 10, steps per second: 479, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.113 [-1.786, 2.683], loss: 0.007959, mean_absolute_error: 0.334097, mean_q: 4.661080\n",
      "   759/50000: episode: 81, duration: 0.020s, episode steps: 10, steps per second: 496, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.138 [-1.998, 3.069], loss: 0.013895, mean_absolute_error: 0.353725, mean_q: 4.498478\n",
      "   769/50000: episode: 82, duration: 0.035s, episode steps: 10, steps per second: 287, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.126 [-2.001, 3.053], loss: 0.013873, mean_absolute_error: 0.357340, mean_q: 4.600580\n",
      "   778/50000: episode: 83, duration: 0.020s, episode steps: 9, steps per second: 439, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.149 [-1.799, 2.850], loss: 0.013458, mean_absolute_error: 0.350836, mean_q: 4.836378\n",
      "   788/50000: episode: 84, duration: 0.021s, episode steps: 10, steps per second: 466, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.127 [-1.970, 3.047], loss: 0.012028, mean_absolute_error: 0.367971, mean_q: 4.552666\n",
      "   798/50000: episode: 85, duration: 0.022s, episode steps: 10, steps per second: 463, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.119 [-1.958, 3.021], loss: 0.017156, mean_absolute_error: 0.370288, mean_q: 4.552379\n",
      "   807/50000: episode: 86, duration: 0.019s, episode steps: 9, steps per second: 469, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.176 [-1.726, 2.868], loss: 0.011088, mean_absolute_error: 0.292392, mean_q: 4.673003\n",
      "   815/50000: episode: 87, duration: 0.017s, episode steps: 8, steps per second: 479, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.139 [-1.573, 2.558], loss: 0.009749, mean_absolute_error: 0.234880, mean_q: 4.674636\n",
      "   824/50000: episode: 88, duration: 0.019s, episode steps: 9, steps per second: 475, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.130 [-1.781, 2.798], loss: 0.023654, mean_absolute_error: 0.207634, mean_q: 4.809114\n",
      "   834/50000: episode: 89, duration: 0.021s, episode steps: 10, steps per second: 482, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.160 [-1.916, 3.047], loss: 0.012505, mean_absolute_error: 0.141043, mean_q: 4.744426\n",
      "   844/50000: episode: 90, duration: 0.020s, episode steps: 10, steps per second: 498, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.165 [-1.907, 3.078], loss: 0.008792, mean_absolute_error: 0.150425, mean_q: 4.775872\n",
      "   853/50000: episode: 91, duration: 0.018s, episode steps: 9, steps per second: 488, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.159 [-1.767, 2.862], loss: 0.009585, mean_absolute_error: 0.152990, mean_q: 4.698744\n",
      "   862/50000: episode: 92, duration: 0.018s, episode steps: 9, steps per second: 498, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.178 [-1.739, 2.890], loss: 0.009297, mean_absolute_error: 0.150961, mean_q: 4.899953\n",
      "   871/50000: episode: 93, duration: 0.028s, episode steps: 9, steps per second: 318, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.141 [-1.778, 2.787], loss: 0.012687, mean_absolute_error: 0.167256, mean_q: 4.853846\n",
      "   881/50000: episode: 94, duration: 0.022s, episode steps: 10, steps per second: 447, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.155 [-1.933, 3.049], loss: 0.011980, mean_absolute_error: 0.173533, mean_q: 4.546218\n",
      "   891/50000: episode: 95, duration: 0.021s, episode steps: 10, steps per second: 467, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.131 [-1.945, 3.054], loss: 0.010957, mean_absolute_error: 0.158282, mean_q: 4.944333\n",
      "   899/50000: episode: 96, duration: 0.017s, episode steps: 8, steps per second: 467, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.168 [-1.529, 2.592], loss: 0.014185, mean_absolute_error: 0.179283, mean_q: 4.675346\n",
      "   909/50000: episode: 97, duration: 0.020s, episode steps: 10, steps per second: 496, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.133 [-1.982, 3.021], loss: 0.017593, mean_absolute_error: 0.178621, mean_q: 4.593632\n",
      "   918/50000: episode: 98, duration: 0.019s, episode steps: 9, steps per second: 483, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.162 [-1.747, 2.888], loss: 0.013693, mean_absolute_error: 0.196986, mean_q: 4.834306\n",
      "   927/50000: episode: 99, duration: 0.019s, episode steps: 9, steps per second: 484, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.161 [-1.714, 2.868], loss: 0.017702, mean_absolute_error: 0.229825, mean_q: 4.851988\n",
      "   937/50000: episode: 100, duration: 0.020s, episode steps: 10, steps per second: 490, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.128 [-1.956, 3.059], loss: 0.013190, mean_absolute_error: 0.263245, mean_q: 4.971153\n",
      "   946/50000: episode: 101, duration: 0.018s, episode steps: 9, steps per second: 491, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.149 [-1.756, 2.786], loss: 0.016354, mean_absolute_error: 0.277468, mean_q: 4.814588\n",
      "   956/50000: episode: 102, duration: 0.021s, episode steps: 10, steps per second: 474, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.154 [-1.936, 3.057], loss: 0.011393, mean_absolute_error: 0.267808, mean_q: 4.826890\n",
      "   965/50000: episode: 103, duration: 0.019s, episode steps: 9, steps per second: 466, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.135 [-1.763, 2.799], loss: 0.007206, mean_absolute_error: 0.258587, mean_q: 4.958172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   974/50000: episode: 104, duration: 0.033s, episode steps: 9, steps per second: 276, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.142 [-1.785, 2.796], loss: 0.014298, mean_absolute_error: 0.266739, mean_q: 4.926844\n",
      "   983/50000: episode: 105, duration: 0.020s, episode steps: 9, steps per second: 454, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.157 [-1.759, 2.817], loss: 0.013124, mean_absolute_error: 0.267757, mean_q: 4.754552\n",
      "   993/50000: episode: 106, duration: 0.021s, episode steps: 10, steps per second: 475, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.156 [-1.926, 3.068], loss: 0.012637, mean_absolute_error: 0.304962, mean_q: 4.794399\n",
      "  1003/50000: episode: 107, duration: 0.021s, episode steps: 10, steps per second: 469, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.123 [-1.959, 3.043], loss: 0.010183, mean_absolute_error: 0.345234, mean_q: 5.086766\n",
      "  1013/50000: episode: 108, duration: 0.020s, episode steps: 10, steps per second: 489, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.121 [-1.998, 3.037], loss: 0.010888, mean_absolute_error: 0.344659, mean_q: 4.891641\n",
      "  1021/50000: episode: 109, duration: 0.017s, episode steps: 8, steps per second: 484, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.162 [-1.570, 2.557], loss: 0.012041, mean_absolute_error: 0.367781, mean_q: 5.029630\n",
      "  1030/50000: episode: 110, duration: 0.019s, episode steps: 9, steps per second: 482, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.171 [-1.750, 2.908], loss: 0.009333, mean_absolute_error: 0.376240, mean_q: 4.979075\n",
      "  1040/50000: episode: 111, duration: 0.020s, episode steps: 10, steps per second: 489, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.139 [-1.947, 3.051], loss: 0.010171, mean_absolute_error: 0.405635, mean_q: 5.117625\n",
      "  1050/50000: episode: 112, duration: 0.022s, episode steps: 10, steps per second: 455, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.139 [-1.985, 3.054], loss: 0.011363, mean_absolute_error: 0.394989, mean_q: 5.043862\n",
      "  1060/50000: episode: 113, duration: 0.021s, episode steps: 10, steps per second: 485, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.149 [-1.964, 3.063], loss: 0.010224, mean_absolute_error: 0.417135, mean_q: 5.085757\n",
      "  1070/50000: episode: 114, duration: 0.022s, episode steps: 10, steps per second: 446, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.116 [-1.975, 2.991], loss: 0.010959, mean_absolute_error: 0.456494, mean_q: 5.068242\n",
      "  1080/50000: episode: 115, duration: 0.030s, episode steps: 10, steps per second: 332, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.129 [-1.969, 3.053], loss: 0.011244, mean_absolute_error: 0.474604, mean_q: 4.956618\n",
      "  1090/50000: episode: 116, duration: 0.021s, episode steps: 10, steps per second: 465, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.116 [-1.955, 3.004], loss: 0.009807, mean_absolute_error: 0.499198, mean_q: 5.002588\n",
      "  1100/50000: episode: 117, duration: 0.021s, episode steps: 10, steps per second: 465, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.148 [-1.958, 3.082], loss: 0.011831, mean_absolute_error: 0.538422, mean_q: 5.114496\n",
      "  1109/50000: episode: 118, duration: 0.019s, episode steps: 9, steps per second: 476, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.145 [-1.754, 2.825], loss: 0.011996, mean_absolute_error: 0.517341, mean_q: 4.621935\n",
      "  1118/50000: episode: 119, duration: 0.018s, episode steps: 9, steps per second: 488, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.140 [-1.739, 2.785], loss: 0.008670, mean_absolute_error: 0.562141, mean_q: 4.897090\n",
      "  1128/50000: episode: 120, duration: 0.021s, episode steps: 10, steps per second: 480, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.141 [-1.971, 3.101], loss: 0.007546, mean_absolute_error: 0.585786, mean_q: 5.051776\n",
      "  1138/50000: episode: 121, duration: 0.021s, episode steps: 10, steps per second: 487, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.154 [-1.938, 3.049], loss: 0.008369, mean_absolute_error: 0.588768, mean_q: 5.097014\n",
      "  1148/50000: episode: 122, duration: 0.020s, episode steps: 10, steps per second: 495, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.141 [-1.905, 2.980], loss: 0.015908, mean_absolute_error: 0.578752, mean_q: 4.817271\n",
      "  1157/50000: episode: 123, duration: 0.019s, episode steps: 9, steps per second: 481, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.122 [-1.811, 2.802], loss: 0.005969, mean_absolute_error: 0.585031, mean_q: 5.133287\n",
      "  1166/50000: episode: 124, duration: 0.029s, episode steps: 9, steps per second: 307, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.134 [-1.769, 2.742], loss: 0.009278, mean_absolute_error: 0.573841, mean_q: 4.998435\n",
      "  1175/50000: episode: 125, duration: 0.020s, episode steps: 9, steps per second: 442, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.160 [-1.729, 2.810], loss: 0.007808, mean_absolute_error: 0.596370, mean_q: 5.240469\n",
      "  1183/50000: episode: 126, duration: 0.017s, episode steps: 8, steps per second: 468, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.147 [-1.617, 2.573], loss: 0.007173, mean_absolute_error: 0.558988, mean_q: 4.749929\n",
      "  1193/50000: episode: 127, duration: 0.023s, episode steps: 10, steps per second: 443, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.150 [-1.919, 3.106], loss: 0.007844, mean_absolute_error: 0.597135, mean_q: 5.062045\n",
      "  1203/50000: episode: 128, duration: 0.025s, episode steps: 10, steps per second: 401, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.160 [-1.922, 3.083], loss: 0.007345, mean_absolute_error: 0.586968, mean_q: 4.900983\n",
      "  1213/50000: episode: 129, duration: 0.021s, episode steps: 10, steps per second: 468, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.150 [-1.921, 3.054], loss: 0.007127, mean_absolute_error: 0.592701, mean_q: 4.989150\n",
      "  1223/50000: episode: 130, duration: 0.021s, episode steps: 10, steps per second: 476, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.144 [-1.995, 3.085], loss: 0.008531, mean_absolute_error: 0.617469, mean_q: 5.126351\n",
      "  1231/50000: episode: 131, duration: 0.017s, episode steps: 8, steps per second: 475, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.123 [-1.601, 2.526], loss: 0.010526, mean_absolute_error: 0.625898, mean_q: 5.003819\n",
      "  1240/50000: episode: 132, duration: 0.019s, episode steps: 9, steps per second: 476, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.153 [-1.748, 2.848], loss: 0.009806, mean_absolute_error: 0.645797, mean_q: 5.137282\n",
      "  1249/50000: episode: 133, duration: 0.019s, episode steps: 9, steps per second: 479, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.156 [-1.756, 2.866], loss: 0.009117, mean_absolute_error: 0.640265, mean_q: 4.893244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1258/50000: episode: 134, duration: 0.020s, episode steps: 9, steps per second: 451, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.136 [-1.744, 2.784], loss: 0.005104, mean_absolute_error: 0.634910, mean_q: 4.899576\n",
      "  1268/50000: episode: 135, duration: 0.029s, episode steps: 10, steps per second: 344, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.127 [-1.962, 2.982], loss: 0.006002, mean_absolute_error: 0.647007, mean_q: 4.917839\n",
      "  1276/50000: episode: 136, duration: 0.018s, episode steps: 8, steps per second: 449, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.153 [-1.556, 2.600], loss: 0.008106, mean_absolute_error: 0.643854, mean_q: 4.798599\n",
      "  1285/50000: episode: 137, duration: 0.019s, episode steps: 9, steps per second: 466, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.152 [-1.784, 2.875], loss: 0.009403, mean_absolute_error: 0.679228, mean_q: 5.034095\n",
      "  1295/50000: episode: 138, duration: 0.021s, episode steps: 10, steps per second: 477, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.138 [-1.943, 3.026], loss: 0.007674, mean_absolute_error: 0.696745, mean_q: 5.189102\n",
      "  1304/50000: episode: 139, duration: 0.018s, episode steps: 9, steps per second: 489, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.174 [-1.750, 2.869], loss: 0.010750, mean_absolute_error: 0.697695, mean_q: 5.164476\n",
      "  1313/50000: episode: 140, duration: 0.019s, episode steps: 9, steps per second: 478, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.168 [-1.749, 2.864], loss: 0.012710, mean_absolute_error: 0.718129, mean_q: 5.249822\n",
      "  1322/50000: episode: 141, duration: 0.019s, episode steps: 9, steps per second: 479, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.174 [-1.764, 2.876], loss: 0.007853, mean_absolute_error: 0.692576, mean_q: 5.126312\n",
      "  1331/50000: episode: 142, duration: 0.018s, episode steps: 9, steps per second: 487, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.138 [-1.747, 2.810], loss: 0.007872, mean_absolute_error: 0.691965, mean_q: 5.128019\n",
      "  1340/50000: episode: 143, duration: 0.019s, episode steps: 9, steps per second: 468, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.148 [-1.712, 2.769], loss: 0.010901, mean_absolute_error: 0.684145, mean_q: 5.050313\n",
      "  1350/50000: episode: 144, duration: 0.025s, episode steps: 10, steps per second: 408, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.119 [-1.940, 2.986], loss: 0.005767, mean_absolute_error: 0.670751, mean_q: 4.995562\n",
      "  1359/50000: episode: 145, duration: 0.026s, episode steps: 9, steps per second: 352, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.133 [-1.800, 2.808], loss: 0.009304, mean_absolute_error: 0.706472, mean_q: 5.287493\n",
      "  1368/50000: episode: 146, duration: 0.020s, episode steps: 9, steps per second: 461, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [-1.720, 2.908], loss: 0.012429, mean_absolute_error: 0.673357, mean_q: 4.867677\n",
      "  1377/50000: episode: 147, duration: 0.020s, episode steps: 9, steps per second: 441, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.163 [-1.732, 2.807], loss: 0.007919, mean_absolute_error: 0.705694, mean_q: 5.177662\n",
      "  1387/50000: episode: 148, duration: 0.022s, episode steps: 10, steps per second: 461, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.132 [-1.931, 3.013], loss: 0.005664, mean_absolute_error: 0.702685, mean_q: 5.151515\n",
      "  1396/50000: episode: 149, duration: 0.019s, episode steps: 9, steps per second: 469, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.154 [-1.723, 2.799], loss: 0.004536, mean_absolute_error: 0.692126, mean_q: 5.038409\n",
      "  1405/50000: episode: 150, duration: 0.019s, episode steps: 9, steps per second: 486, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.146 [-1.749, 2.778], loss: 0.006508, mean_absolute_error: 0.707784, mean_q: 5.143643\n",
      "  1414/50000: episode: 151, duration: 0.019s, episode steps: 9, steps per second: 471, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.156 [-1.734, 2.851], loss: 0.006301, mean_absolute_error: 0.698140, mean_q: 5.067144\n",
      "  1424/50000: episode: 152, duration: 0.022s, episode steps: 10, steps per second: 464, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.121 [-1.923, 2.986], loss: 0.007285, mean_absolute_error: 0.702804, mean_q: 4.983244\n",
      "  1433/50000: episode: 153, duration: 0.019s, episode steps: 9, steps per second: 476, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.167 [-1.754, 2.876], loss: 0.007528, mean_absolute_error: 0.698748, mean_q: 4.860711\n",
      "  1442/50000: episode: 154, duration: 0.021s, episode steps: 9, steps per second: 436, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.171 [-1.749, 2.889], loss: 0.007817, mean_absolute_error: 0.725537, mean_q: 5.040873\n",
      "  1453/50000: episode: 155, duration: 0.032s, episode steps: 11, steps per second: 346, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.110 [-1.772, 2.794], loss: 0.006567, mean_absolute_error: 0.719197, mean_q: 4.998193\n",
      "  1462/50000: episode: 156, duration: 0.034s, episode steps: 9, steps per second: 263, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.151 [-1.789, 2.818], loss: 0.008604, mean_absolute_error: 0.737562, mean_q: 5.126212\n",
      "  1472/50000: episode: 157, duration: 0.086s, episode steps: 10, steps per second: 116, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.144 [-1.987, 3.126], loss: 0.006063, mean_absolute_error: 0.707787, mean_q: 4.888113\n",
      "  1481/50000: episode: 158, duration: 0.072s, episode steps: 9, steps per second: 126, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.156 [-1.754, 2.876], loss: 0.009883, mean_absolute_error: 0.726897, mean_q: 4.975147\n",
      "  1490/50000: episode: 159, duration: 0.057s, episode steps: 9, steps per second: 157, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.141 [-1.791, 2.796], loss: 0.013892, mean_absolute_error: 0.747792, mean_q: 5.101751\n",
      "  1501/50000: episode: 160, duration: 0.024s, episode steps: 11, steps per second: 454, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.129 [-2.181, 3.318], loss: 0.009304, mean_absolute_error: 0.710760, mean_q: 4.847521\n",
      "  1510/50000: episode: 161, duration: 0.020s, episode steps: 9, steps per second: 456, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.136 [-1.794, 2.796], loss: 0.009237, mean_absolute_error: 0.731841, mean_q: 5.085474\n",
      "  1520/50000: episode: 162, duration: 0.021s, episode steps: 10, steps per second: 472, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.146 [-1.980, 3.091], loss: 0.005785, mean_absolute_error: 0.733449, mean_q: 5.176503\n",
      "  1529/50000: episode: 163, duration: 0.020s, episode steps: 9, steps per second: 440, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.159 [-1.736, 2.811], loss: 0.129771, mean_absolute_error: 0.746101, mean_q: 4.989386\n",
      "  1539/50000: episode: 164, duration: 0.022s, episode steps: 10, steps per second: 448, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.135 [-1.976, 3.076], loss: 0.006526, mean_absolute_error: 0.860103, mean_q: 5.159449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1548/50000: episode: 165, duration: 0.054s, episode steps: 9, steps per second: 167, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.165 [-1.720, 2.829], loss: 0.007424, mean_absolute_error: 0.940336, mean_q: 5.185304\n",
      "  1558/50000: episode: 166, duration: 0.095s, episode steps: 10, steps per second: 105, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.168 [-1.958, 3.128], loss: 0.006557, mean_absolute_error: 0.941164, mean_q: 4.967233\n",
      "  1567/50000: episode: 167, duration: 0.075s, episode steps: 9, steps per second: 120, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.155 [-1.726, 2.825], loss: 0.005956, mean_absolute_error: 0.952745, mean_q: 5.023810\n",
      "  1576/50000: episode: 168, duration: 0.025s, episode steps: 9, steps per second: 367, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.161 [-1.720, 2.827], loss: 0.003478, mean_absolute_error: 0.958419, mean_q: 5.105180\n",
      "  1585/50000: episode: 169, duration: 0.035s, episode steps: 9, steps per second: 260, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.133 [-1.807, 2.814], loss: 0.006166, mean_absolute_error: 0.947652, mean_q: 4.962197\n",
      "  1595/50000: episode: 170, duration: 0.022s, episode steps: 10, steps per second: 459, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.156 [-1.995, 3.129], loss: 0.006759, mean_absolute_error: 0.953160, mean_q: 4.982382\n",
      "  1605/50000: episode: 171, duration: 0.022s, episode steps: 10, steps per second: 463, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.157 [-1.956, 3.101], loss: 0.005529, mean_absolute_error: 0.952013, mean_q: 4.996445\n",
      "  1615/50000: episode: 172, duration: 0.021s, episode steps: 10, steps per second: 470, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.117 [-1.978, 2.970], loss: 0.004962, mean_absolute_error: 0.976287, mean_q: 5.161534\n",
      "  1624/50000: episode: 173, duration: 0.019s, episode steps: 9, steps per second: 478, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.155 [-1.718, 2.801], loss: 0.011236, mean_absolute_error: 0.953246, mean_q: 4.864290\n",
      "  1634/50000: episode: 174, duration: 0.021s, episode steps: 10, steps per second: 474, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.135 [-1.945, 2.982], loss: 0.007689, mean_absolute_error: 0.953309, mean_q: 4.913765\n",
      "  1643/50000: episode: 175, duration: 0.019s, episode steps: 9, steps per second: 485, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.117 [-1.790, 2.766], loss: 0.006316, mean_absolute_error: 1.006585, mean_q: 5.335149\n",
      "  1652/50000: episode: 176, duration: 0.018s, episode steps: 9, steps per second: 493, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.136 [-1.809, 2.821], loss: 0.009074, mean_absolute_error: 0.961872, mean_q: 4.947793\n",
      "  1661/50000: episode: 177, duration: 0.019s, episode steps: 9, steps per second: 483, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.138 [-1.735, 2.754], loss: 0.011521, mean_absolute_error: 0.944877, mean_q: 4.758838\n",
      "  1670/50000: episode: 178, duration: 0.018s, episode steps: 9, steps per second: 495, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.167 [-1.762, 2.888], loss: 0.008774, mean_absolute_error: 0.983731, mean_q: 5.038073\n",
      "  1679/50000: episode: 179, duration: 0.018s, episode steps: 9, steps per second: 495, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.143 [-1.751, 2.811], loss: 0.013360, mean_absolute_error: 0.989541, mean_q: 5.110738\n",
      "  1687/50000: episode: 180, duration: 0.031s, episode steps: 8, steps per second: 260, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.164 [-1.584, 2.602], loss: 0.125265, mean_absolute_error: 1.041216, mean_q: 5.071034\n",
      "  1697/50000: episode: 181, duration: 0.023s, episode steps: 10, steps per second: 439, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.121 [-1.801, 2.666], loss: 0.005831, mean_absolute_error: 1.117141, mean_q: 5.167289\n",
      "  1706/50000: episode: 182, duration: 0.021s, episode steps: 9, steps per second: 433, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.145 [-1.717, 2.801], loss: 0.003952, mean_absolute_error: 1.152377, mean_q: 5.180747\n",
      "  1716/50000: episode: 183, duration: 0.022s, episode steps: 10, steps per second: 446, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.134 [-1.745, 2.740], loss: 0.007581, mean_absolute_error: 1.135927, mean_q: 4.949812\n",
      "  1725/50000: episode: 184, duration: 0.020s, episode steps: 9, steps per second: 450, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.140 [-1.584, 2.496], loss: 0.005785, mean_absolute_error: 1.179397, mean_q: 5.221329\n",
      "  1733/50000: episode: 185, duration: 0.027s, episode steps: 8, steps per second: 301, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.153 [-1.560, 2.517], loss: 0.006834, mean_absolute_error: 1.134888, mean_q: 4.879639\n",
      "  1742/50000: episode: 186, duration: 0.020s, episode steps: 9, steps per second: 442, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.150 [-1.547, 2.479], loss: 0.097731, mean_absolute_error: 1.203005, mean_q: 5.144275\n",
      "  1752/50000: episode: 187, duration: 0.023s, episode steps: 10, steps per second: 432, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.145 [-1.737, 2.716], loss: 0.008556, mean_absolute_error: 1.260222, mean_q: 5.087250\n",
      "  1762/50000: episode: 188, duration: 0.023s, episode steps: 10, steps per second: 441, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.123 [-1.717, 2.663], loss: 0.008737, mean_absolute_error: 1.295838, mean_q: 5.091042\n",
      "  1771/50000: episode: 189, duration: 0.021s, episode steps: 9, steps per second: 423, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.128 [-1.612, 2.491], loss: 0.091864, mean_absolute_error: 1.369162, mean_q: 5.274594\n",
      "  1780/50000: episode: 190, duration: 0.028s, episode steps: 9, steps per second: 318, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.182 [-1.527, 2.543], loss: 0.012273, mean_absolute_error: 1.348878, mean_q: 4.884768\n",
      "  1789/50000: episode: 191, duration: 0.022s, episode steps: 9, steps per second: 418, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.167 [-1.528, 2.515], loss: 0.010371, mean_absolute_error: 1.459143, mean_q: 5.269032\n",
      "  1798/50000: episode: 192, duration: 0.021s, episode steps: 9, steps per second: 434, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.128 [-1.609, 2.473], loss: 0.010193, mean_absolute_error: 1.401012, mean_q: 4.920566\n",
      "  1806/50000: episode: 193, duration: 0.019s, episode steps: 8, steps per second: 423, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.146 [-1.558, 2.511], loss: 0.015465, mean_absolute_error: 1.467086, mean_q: 5.224025\n",
      "  1815/50000: episode: 194, duration: 0.022s, episode steps: 9, steps per second: 414, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.136 [-1.613, 2.474], loss: 0.008599, mean_absolute_error: 1.421265, mean_q: 5.082089\n",
      "  1825/50000: episode: 195, duration: 0.023s, episode steps: 10, steps per second: 428, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.126 [-1.787, 2.660], loss: 0.008970, mean_absolute_error: 1.381503, mean_q: 4.940854\n",
      "  1835/50000: episode: 196, duration: 0.023s, episode steps: 10, steps per second: 426, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.127 [-1.794, 2.743], loss: 0.007998, mean_absolute_error: 1.368474, mean_q: 4.995614\n",
      "  1843/50000: episode: 197, duration: 0.019s, episode steps: 8, steps per second: 415, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.154 [-1.521, 2.567], loss: 0.010695, mean_absolute_error: 1.369392, mean_q: 5.041089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1853/50000: episode: 198, duration: 0.024s, episode steps: 10, steps per second: 413, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.157 [-1.749, 2.747], loss: 0.010797, mean_absolute_error: 1.378819, mean_q: 5.116748\n",
      "  1862/50000: episode: 199, duration: 0.035s, episode steps: 9, steps per second: 258, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.152 [-1.745, 2.772], loss: 0.007662, mean_absolute_error: 1.337003, mean_q: 4.966899\n",
      "  1871/50000: episode: 200, duration: 0.022s, episode steps: 9, steps per second: 410, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.137 [-1.610, 2.473], loss: 0.007759, mean_absolute_error: 1.388993, mean_q: 5.216620\n",
      "  1881/50000: episode: 201, duration: 0.023s, episode steps: 10, steps per second: 427, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.132 [-1.724, 2.679], loss: 0.009292, mean_absolute_error: 1.323787, mean_q: 4.897491\n",
      "  1890/50000: episode: 202, duration: 0.022s, episode steps: 9, steps per second: 415, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.141 [-1.607, 2.481], loss: 0.005633, mean_absolute_error: 1.355523, mean_q: 5.105345\n",
      "  1898/50000: episode: 203, duration: 0.018s, episode steps: 8, steps per second: 441, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.151 [-1.568, 2.544], loss: 0.008482, mean_absolute_error: 1.393547, mean_q: 5.294119\n",
      "  1908/50000: episode: 204, duration: 0.024s, episode steps: 10, steps per second: 415, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.135 [-1.718, 2.653], loss: 0.007125, mean_absolute_error: 1.337395, mean_q: 5.033641\n",
      "  1918/50000: episode: 205, duration: 0.021s, episode steps: 10, steps per second: 468, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.155 [-1.731, 2.738], loss: 0.135988, mean_absolute_error: 1.389662, mean_q: 4.909740\n",
      "  1928/50000: episode: 206, duration: 0.023s, episode steps: 10, steps per second: 426, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.148 [-1.780, 2.765], loss: 0.012818, mean_absolute_error: 1.486324, mean_q: 4.935430\n",
      "  1937/50000: episode: 207, duration: 0.035s, episode steps: 9, steps per second: 259, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.169 [-1.553, 2.525], loss: 0.007245, mean_absolute_error: 1.516307, mean_q: 4.953030\n",
      "  1945/50000: episode: 208, duration: 0.022s, episode steps: 8, steps per second: 367, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.159 [-1.590, 2.568], loss: 0.008126, mean_absolute_error: 1.566804, mean_q: 5.199252\n",
      "  1954/50000: episode: 209, duration: 0.020s, episode steps: 9, steps per second: 450, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.141 [-1.571, 2.447], loss: 0.010383, mean_absolute_error: 1.501324, mean_q: 4.932310\n",
      "  1965/50000: episode: 210, duration: 0.025s, episode steps: 11, steps per second: 436, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.113 [-1.988, 2.949], loss: 0.011957, mean_absolute_error: 1.536128, mean_q: 5.107502\n",
      "  1975/50000: episode: 211, duration: 0.021s, episode steps: 10, steps per second: 473, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.138 [-1.714, 2.715], loss: 0.009926, mean_absolute_error: 1.491279, mean_q: 4.929952\n",
      "  1984/50000: episode: 212, duration: 0.019s, episode steps: 9, steps per second: 467, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.152 [-1.759, 2.809], loss: 0.007724, mean_absolute_error: 1.531924, mean_q: 5.151843\n",
      "  1994/50000: episode: 213, duration: 0.021s, episode steps: 10, steps per second: 483, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.121 [-1.787, 2.661], loss: 0.006355, mean_absolute_error: 1.450234, mean_q: 4.840984\n",
      "  2004/50000: episode: 214, duration: 0.021s, episode steps: 10, steps per second: 487, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.112 [-1.795, 2.691], loss: 0.010273, mean_absolute_error: 1.470392, mean_q: 4.930779\n",
      "  2012/50000: episode: 215, duration: 0.017s, episode steps: 8, steps per second: 472, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.166 [-1.553, 2.584], loss: 0.005340, mean_absolute_error: 1.492686, mean_q: 5.046479\n",
      "  2022/50000: episode: 216, duration: 0.021s, episode steps: 10, steps per second: 478, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.130 [-1.913, 3.009], loss: 0.008082, mean_absolute_error: 1.516845, mean_q: 5.156602\n",
      "  2030/50000: episode: 217, duration: 0.025s, episode steps: 8, steps per second: 320, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.143 [-1.595, 2.582], loss: 0.005408, mean_absolute_error: 1.494714, mean_q: 5.111796\n",
      "  2038/50000: episode: 218, duration: 0.022s, episode steps: 8, steps per second: 367, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.142 [-1.585, 2.498], loss: 0.005565, mean_absolute_error: 1.513434, mean_q: 5.211800\n",
      "  2048/50000: episode: 219, duration: 0.022s, episode steps: 10, steps per second: 452, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.140 [-1.786, 2.776], loss: 0.007856, mean_absolute_error: 1.484687, mean_q: 5.047699\n",
      "  2058/50000: episode: 220, duration: 0.021s, episode steps: 10, steps per second: 468, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.131 [-1.976, 3.079], loss: 0.005280, mean_absolute_error: 1.531654, mean_q: 5.263446\n",
      "  2067/50000: episode: 221, duration: 0.019s, episode steps: 9, steps per second: 463, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.131 [-1.778, 2.793], loss: 0.005507, mean_absolute_error: 1.445141, mean_q: 4.900294\n",
      "  2076/50000: episode: 222, duration: 0.019s, episode steps: 9, steps per second: 469, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.151 [-1.784, 2.869], loss: 0.005740, mean_absolute_error: 1.529739, mean_q: 5.248129\n",
      "  2085/50000: episode: 223, duration: 0.019s, episode steps: 9, steps per second: 474, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.164 [-1.744, 2.824], loss: 0.006324, mean_absolute_error: 1.473435, mean_q: 5.006203\n",
      "  2094/50000: episode: 224, duration: 0.019s, episode steps: 9, steps per second: 471, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.142 [-1.774, 2.810], loss: 0.009010, mean_absolute_error: 1.515376, mean_q: 5.159996\n",
      "  2102/50000: episode: 225, duration: 0.018s, episode steps: 8, steps per second: 451, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.142 [-1.578, 2.543], loss: 0.007457, mean_absolute_error: 1.412768, mean_q: 4.733140\n",
      "  2112/50000: episode: 226, duration: 0.021s, episode steps: 10, steps per second: 472, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.137 [-1.953, 3.077], loss: 0.005440, mean_absolute_error: 1.591059, mean_q: 5.473571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2122/50000: episode: 227, duration: 0.022s, episode steps: 10, steps per second: 450, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.140 [-1.960, 3.035], loss: 0.009413, mean_absolute_error: 1.510315, mean_q: 5.101686\n",
      "  2132/50000: episode: 228, duration: 0.030s, episode steps: 10, steps per second: 333, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.142 [-1.934, 3.034], loss: 0.004594, mean_absolute_error: 1.495273, mean_q: 5.102834\n",
      "  2142/50000: episode: 229, duration: 0.022s, episode steps: 10, steps per second: 459, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.120 [-1.996, 2.974], loss: 0.004987, mean_absolute_error: 1.460979, mean_q: 4.988111\n",
      "  2152/50000: episode: 230, duration: 0.021s, episode steps: 10, steps per second: 466, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.140 [-1.929, 3.064], loss: 0.007352, mean_absolute_error: 1.455153, mean_q: 4.938064\n",
      "  2162/50000: episode: 231, duration: 0.028s, episode steps: 10, steps per second: 356, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.129 [-1.905, 2.956], loss: 0.008952, mean_absolute_error: 1.477731, mean_q: 5.026740\n",
      "  2171/50000: episode: 232, duration: 0.020s, episode steps: 9, steps per second: 455, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.167 [-1.712, 2.856], loss: 0.010154, mean_absolute_error: 1.506382, mean_q: 5.126013\n",
      "  2180/50000: episode: 233, duration: 0.020s, episode steps: 9, steps per second: 461, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.154 [-1.772, 2.873], loss: 0.008035, mean_absolute_error: 1.497536, mean_q: 5.121300\n",
      "  2189/50000: episode: 234, duration: 0.020s, episode steps: 9, steps per second: 461, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.145 [-1.545, 2.503], loss: 0.070821, mean_absolute_error: 1.493954, mean_q: 4.859640\n",
      "  2198/50000: episode: 235, duration: 0.019s, episode steps: 9, steps per second: 465, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.151 [-1.736, 2.813], loss: 0.006104, mean_absolute_error: 1.556307, mean_q: 4.985181\n",
      "  2207/50000: episode: 236, duration: 0.019s, episode steps: 9, steps per second: 478, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.137 [-1.616, 2.481], loss: 0.007730, mean_absolute_error: 1.603723, mean_q: 5.105956\n",
      "  2217/50000: episode: 237, duration: 0.032s, episode steps: 10, steps per second: 308, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.141 [-1.762, 2.693], loss: 0.005763, mean_absolute_error: 1.554302, mean_q: 4.959534\n",
      "  2227/50000: episode: 238, duration: 0.022s, episode steps: 10, steps per second: 453, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.126 [-1.944, 3.019], loss: 0.007290, mean_absolute_error: 1.586652, mean_q: 5.096925\n",
      "  2235/50000: episode: 239, duration: 0.017s, episode steps: 8, steps per second: 457, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.162 [-1.519, 2.566], loss: 0.004549, mean_absolute_error: 1.559760, mean_q: 5.026484\n",
      "  2245/50000: episode: 240, duration: 0.021s, episode steps: 10, steps per second: 474, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.144 [-1.922, 2.988], loss: 0.006118, mean_absolute_error: 1.596054, mean_q: 5.140083\n",
      "  2254/50000: episode: 241, duration: 0.019s, episode steps: 9, steps per second: 472, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.137 [-1.789, 2.831], loss: 0.061974, mean_absolute_error: 1.630296, mean_q: 5.147552\n",
      "  2263/50000: episode: 242, duration: 0.018s, episode steps: 9, steps per second: 489, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.157 [-1.562, 2.491], loss: 0.007182, mean_absolute_error: 1.642596, mean_q: 5.046301\n",
      "  2272/50000: episode: 243, duration: 0.018s, episode steps: 9, steps per second: 490, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.134 [-1.600, 2.466], loss: 0.005814, mean_absolute_error: 1.665248, mean_q: 5.057521\n",
      "  2281/50000: episode: 244, duration: 0.018s, episode steps: 9, steps per second: 487, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.127 [-1.785, 2.758], loss: 0.003918, mean_absolute_error: 1.735485, mean_q: 5.331785\n",
      "  2290/50000: episode: 245, duration: 0.020s, episode steps: 9, steps per second: 456, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.146 [-1.611, 2.514], loss: 0.005106, mean_absolute_error: 1.654350, mean_q: 5.041642\n",
      "  2300/50000: episode: 246, duration: 0.035s, episode steps: 10, steps per second: 288, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.147 [-1.793, 2.784], loss: 0.007264, mean_absolute_error: 1.698869, mean_q: 5.204866\n",
      "  2309/50000: episode: 247, duration: 0.028s, episode steps: 9, steps per second: 327, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.137 [-1.598, 2.482], loss: 0.004431, mean_absolute_error: 1.581489, mean_q: 4.821700\n",
      "  2318/50000: episode: 248, duration: 0.025s, episode steps: 9, steps per second: 358, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.159 [-1.734, 2.835], loss: 0.004247, mean_absolute_error: 1.687941, mean_q: 5.199903\n",
      "  2328/50000: episode: 249, duration: 0.024s, episode steps: 10, steps per second: 417, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.158 [-1.710, 2.731], loss: 0.007580, mean_absolute_error: 1.639778, mean_q: 5.013975\n",
      "  2337/50000: episode: 250, duration: 0.021s, episode steps: 9, steps per second: 421, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.118 [-1.804, 2.790], loss: 0.006945, mean_absolute_error: 1.590247, mean_q: 4.839317\n",
      "  2346/50000: episode: 251, duration: 0.030s, episode steps: 9, steps per second: 298, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.155 [-1.755, 2.846], loss: 0.006840, mean_absolute_error: 1.626632, mean_q: 4.974172\n",
      "  2356/50000: episode: 252, duration: 0.024s, episode steps: 10, steps per second: 412, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.153 [-1.767, 2.756], loss: 0.006734, mean_absolute_error: 1.638419, mean_q: 5.007096\n",
      "  2366/50000: episode: 253, duration: 0.024s, episode steps: 10, steps per second: 422, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.142 [-1.779, 2.770], loss: 0.048490, mean_absolute_error: 1.597690, mean_q: 4.800429\n",
      "  2376/50000: episode: 254, duration: 0.040s, episode steps: 10, steps per second: 251, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.141 [-1.755, 2.753], loss: 0.005240, mean_absolute_error: 1.737828, mean_q: 5.112854\n",
      "  2386/50000: episode: 255, duration: 0.030s, episode steps: 10, steps per second: 339, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.103 [-1.774, 2.645], loss: 0.047144, mean_absolute_error: 1.795904, mean_q: 5.113402\n",
      "  2395/50000: episode: 256, duration: 0.021s, episode steps: 9, steps per second: 424, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.154 [-1.525, 2.472], loss: 0.005771, mean_absolute_error: 1.814882, mean_q: 5.062258\n",
      "  2404/50000: episode: 257, duration: 0.021s, episode steps: 9, steps per second: 439, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.122 [-1.593, 2.474], loss: 0.083174, mean_absolute_error: 1.951210, mean_q: 5.190811\n",
      "  2413/50000: episode: 258, duration: 0.020s, episode steps: 9, steps per second: 457, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.134 [-1.611, 2.457], loss: 0.008941, mean_absolute_error: 1.953115, mean_q: 4.976362\n",
      "  2423/50000: episode: 259, duration: 0.021s, episode steps: 10, steps per second: 468, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.148 [-1.724, 2.681], loss: 0.010662, mean_absolute_error: 2.090768, mean_q: 5.267400\n",
      "  2432/50000: episode: 260, duration: 0.020s, episode steps: 9, steps per second: 455, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.133 [-1.608, 2.467], loss: 0.007811, mean_absolute_error: 1.986750, mean_q: 5.002409\n",
      "  2441/50000: episode: 261, duration: 0.020s, episode steps: 9, steps per second: 456, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.125 [-1.604, 2.447], loss: 0.008223, mean_absolute_error: 1.938332, mean_q: 4.901831\n",
      "  2451/50000: episode: 262, duration: 0.021s, episode steps: 10, steps per second: 486, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.151 [-1.729, 2.706], loss: 0.008834, mean_absolute_error: 1.990705, mean_q: 5.087989\n",
      "  2460/50000: episode: 263, duration: 0.019s, episode steps: 9, steps per second: 483, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.124 [-1.590, 2.469], loss: 0.005343, mean_absolute_error: 1.975344, mean_q: 5.109900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2470/50000: episode: 264, duration: 0.033s, episode steps: 10, steps per second: 304, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.141 [-1.733, 2.744], loss: 0.008793, mean_absolute_error: 1.930189, mean_q: 4.977086\n",
      "  2480/50000: episode: 265, duration: 0.022s, episode steps: 10, steps per second: 457, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.123 [-1.785, 2.732], loss: 0.032940, mean_absolute_error: 1.966897, mean_q: 5.100577\n",
      "  2488/50000: episode: 266, duration: 0.019s, episode steps: 8, steps per second: 429, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.140 [-1.592, 2.588], loss: 0.008793, mean_absolute_error: 1.976153, mean_q: 5.033771\n",
      "  2497/50000: episode: 267, duration: 0.020s, episode steps: 9, steps per second: 440, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.129 [-1.577, 2.450], loss: 0.011481, mean_absolute_error: 2.057493, mean_q: 5.153944\n",
      "  2506/50000: episode: 268, duration: 0.036s, episode steps: 9, steps per second: 248, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.145 [-1.532, 2.422], loss: 0.034190, mean_absolute_error: 2.110388, mean_q: 5.193746\n",
      "  2514/50000: episode: 269, duration: 0.060s, episode steps: 8, steps per second: 132, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.130 [-1.611, 2.507], loss: 0.006587, mean_absolute_error: 2.149277, mean_q: 5.234556\n",
      "  2523/50000: episode: 270, duration: 0.083s, episode steps: 9, steps per second: 108, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.132 [-1.746, 2.774], loss: 0.011277, mean_absolute_error: 2.126168, mean_q: 5.136655\n",
      "  2532/50000: episode: 271, duration: 0.077s, episode steps: 9, steps per second: 117, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.146 [-1.609, 2.484], loss: 0.006568, mean_absolute_error: 1.972546, mean_q: 4.759404\n",
      "  2542/50000: episode: 272, duration: 0.027s, episode steps: 10, steps per second: 374, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.150 [-1.747, 2.720], loss: 0.027763, mean_absolute_error: 2.145351, mean_q: 5.144915\n",
      "  2552/50000: episode: 273, duration: 0.022s, episode steps: 10, steps per second: 447, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.133 [-1.761, 2.659], loss: 0.009423, mean_absolute_error: 2.081689, mean_q: 4.883786\n",
      "  2562/50000: episode: 274, duration: 0.023s, episode steps: 10, steps per second: 438, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.133 [-1.731, 2.665], loss: 0.007626, mean_absolute_error: 2.107701, mean_q: 4.979934\n",
      "  2572/50000: episode: 275, duration: 0.038s, episode steps: 10, steps per second: 266, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.132 [-1.743, 2.711], loss: 0.005977, mean_absolute_error: 2.165182, mean_q: 5.144857\n",
      "  2581/50000: episode: 276, duration: 0.097s, episode steps: 9, steps per second: 93, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.131 [-1.771, 2.806], loss: 0.008343, mean_absolute_error: 2.109414, mean_q: 5.006042\n",
      "  2591/50000: episode: 277, duration: 0.113s, episode steps: 10, steps per second: 89, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.152 [-1.941, 3.085], loss: 0.007412, mean_absolute_error: 2.201824, mean_q: 5.239224\n",
      "  2600/50000: episode: 278, duration: 0.044s, episode steps: 9, steps per second: 204, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.142 [-1.762, 2.751], loss: 0.007500, mean_absolute_error: 2.079236, mean_q: 4.933485\n",
      "  2609/50000: episode: 279, duration: 0.020s, episode steps: 9, steps per second: 443, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.163 [-1.718, 2.819], loss: 0.006224, mean_absolute_error: 2.126606, mean_q: 5.072382\n",
      "  2617/50000: episode: 280, duration: 0.017s, episode steps: 8, steps per second: 459, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.143 [-1.617, 2.535], loss: 0.007337, mean_absolute_error: 2.129189, mean_q: 5.074729\n",
      "  2627/50000: episode: 281, duration: 0.033s, episode steps: 10, steps per second: 301, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.166 [-1.921, 3.086], loss: 0.023993, mean_absolute_error: 2.108846, mean_q: 4.972541\n",
      "  2636/50000: episode: 282, duration: 0.020s, episode steps: 9, steps per second: 460, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.174 [-1.524, 2.540], loss: 0.004040, mean_absolute_error: 2.059406, mean_q: 4.794352\n",
      "  2645/50000: episode: 283, duration: 0.020s, episode steps: 9, steps per second: 455, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.160 [-1.580, 2.525], loss: 0.007970, mean_absolute_error: 2.187443, mean_q: 5.063813\n",
      "  2654/50000: episode: 284, duration: 0.026s, episode steps: 9, steps per second: 340, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.149 [-1.529, 2.448], loss: 0.024078, mean_absolute_error: 2.139100, mean_q: 4.875951\n",
      "  2664/50000: episode: 285, duration: 0.022s, episode steps: 10, steps per second: 461, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.137 [-1.730, 2.691], loss: 0.008954, mean_absolute_error: 2.162094, mean_q: 4.837706\n",
      "  2673/50000: episode: 286, duration: 0.019s, episode steps: 9, steps per second: 465, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.139 [-1.553, 2.425], loss: 0.004227, mean_absolute_error: 2.263666, mean_q: 5.090406\n",
      "  2682/50000: episode: 287, duration: 0.019s, episode steps: 9, steps per second: 486, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.168 [-1.560, 2.524], loss: 0.005786, mean_absolute_error: 2.198611, mean_q: 4.916559\n",
      "  2690/50000: episode: 288, duration: 0.017s, episode steps: 8, steps per second: 484, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.156 [-1.558, 2.590], loss: 0.006350, mean_absolute_error: 2.283178, mean_q: 5.114867\n",
      "  2700/50000: episode: 289, duration: 0.020s, episode steps: 10, steps per second: 496, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.151 [-1.719, 2.710], loss: 0.008323, mean_absolute_error: 2.268172, mean_q: 5.097602\n",
      "  2708/50000: episode: 290, duration: 0.017s, episode steps: 8, steps per second: 474, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.150 [-1.540, 2.524], loss: 0.007321, mean_absolute_error: 2.285011, mean_q: 5.142306\n",
      "  2716/50000: episode: 291, duration: 0.017s, episode steps: 8, steps per second: 457, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.156 [-1.571, 2.567], loss: 0.007528, mean_absolute_error: 2.252719, mean_q: 5.070879\n",
      "  2726/50000: episode: 292, duration: 0.031s, episode steps: 10, steps per second: 323, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.110 [-1.790, 2.643], loss: 0.004592, mean_absolute_error: 2.168546, mean_q: 4.908934\n",
      "  2735/50000: episode: 293, duration: 0.020s, episode steps: 9, steps per second: 447, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.150 [-1.746, 2.808], loss: 0.004046, mean_absolute_error: 2.289341, mean_q: 5.200017\n",
      "  2744/50000: episode: 294, duration: 0.020s, episode steps: 9, steps per second: 461, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.151 [-1.730, 2.794], loss: 0.004915, mean_absolute_error: 2.357687, mean_q: 5.348884\n",
      "  2753/50000: episode: 295, duration: 0.019s, episode steps: 9, steps per second: 468, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.170 [-1.748, 2.849], loss: 0.005964, mean_absolute_error: 2.336240, mean_q: 5.299292\n",
      "  2763/50000: episode: 296, duration: 0.021s, episode steps: 10, steps per second: 485, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.105 [-1.763, 2.620], loss: 0.007366, mean_absolute_error: 2.297920, mean_q: 5.197656\n",
      "  2772/50000: episode: 297, duration: 0.019s, episode steps: 9, steps per second: 478, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.153 [-1.759, 2.809], loss: 0.021550, mean_absolute_error: 2.251889, mean_q: 5.047280\n",
      "  2781/50000: episode: 298, duration: 0.018s, episode steps: 9, steps per second: 494, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.132 [-1.737, 2.771], loss: 0.005044, mean_absolute_error: 2.337436, mean_q: 5.176037\n",
      "  2791/50000: episode: 299, duration: 0.021s, episode steps: 10, steps per second: 485, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.126 [-1.765, 2.694], loss: 0.004622, mean_absolute_error: 2.414997, mean_q: 5.312913\n",
      "  2801/50000: episode: 300, duration: 0.021s, episode steps: 10, steps per second: 486, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.157 [-1.720, 2.715], loss: 0.005901, mean_absolute_error: 2.246331, mean_q: 4.937506\n",
      "  2810/50000: episode: 301, duration: 0.018s, episode steps: 9, steps per second: 489, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.168 [-1.728, 2.836], loss: 0.003807, mean_absolute_error: 2.383310, mean_q: 5.255035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2820/50000: episode: 302, duration: 0.021s, episode steps: 10, steps per second: 476, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.125 [-2.000, 3.079], loss: 0.006874, mean_absolute_error: 2.409254, mean_q: 5.318248\n",
      "  2830/50000: episode: 303, duration: 0.030s, episode steps: 10, steps per second: 336, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.153 [-1.938, 3.058], loss: 0.018172, mean_absolute_error: 2.377427, mean_q: 5.239278\n",
      "  2838/50000: episode: 304, duration: 0.018s, episode steps: 8, steps per second: 457, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.137 [-1.587, 2.550], loss: 0.005484, mean_absolute_error: 2.349220, mean_q: 5.108841\n",
      "  2848/50000: episode: 305, duration: 0.021s, episode steps: 10, steps per second: 468, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.138 [-1.739, 2.709], loss: 0.004385, mean_absolute_error: 2.394841, mean_q: 5.171294\n",
      "  2858/50000: episode: 306, duration: 0.021s, episode steps: 10, steps per second: 474, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.136 [-1.801, 2.753], loss: 0.004244, mean_absolute_error: 2.478205, mean_q: 5.346116\n",
      "  2868/50000: episode: 307, duration: 0.020s, episode steps: 10, steps per second: 502, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.154 [-1.759, 2.740], loss: 0.006071, mean_absolute_error: 2.429137, mean_q: 5.219774\n",
      "  2876/50000: episode: 308, duration: 0.017s, episode steps: 8, steps per second: 479, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.142 [-1.578, 2.522], loss: 0.009757, mean_absolute_error: 2.441291, mean_q: 5.217767\n",
      "  2885/50000: episode: 309, duration: 0.026s, episode steps: 9, steps per second: 347, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.138 [-1.779, 2.772], loss: 0.008058, mean_absolute_error: 2.424009, mean_q: 5.210484\n",
      "  2894/50000: episode: 310, duration: 0.020s, episode steps: 9, steps per second: 456, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.171 [-1.724, 2.837], loss: 0.007852, mean_absolute_error: 2.271895, mean_q: 4.880471\n",
      "  2902/50000: episode: 311, duration: 0.017s, episode steps: 8, steps per second: 463, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.151 [-1.545, 2.573], loss: 0.008021, mean_absolute_error: 2.277353, mean_q: 4.898797\n",
      "  2910/50000: episode: 312, duration: 0.026s, episode steps: 8, steps per second: 312, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.151 [-1.552, 2.584], loss: 0.005253, mean_absolute_error: 2.409693, mean_q: 5.170784\n",
      "  2919/50000: episode: 313, duration: 0.020s, episode steps: 9, steps per second: 446, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.119 [-1.606, 2.407], loss: 0.005122, mean_absolute_error: 2.257466, mean_q: 4.857818\n",
      "  2929/50000: episode: 314, duration: 0.023s, episode steps: 10, steps per second: 435, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.138 [-1.990, 3.139], loss: 0.005128, mean_absolute_error: 2.447539, mean_q: 5.288634\n",
      "  2939/50000: episode: 315, duration: 0.036s, episode steps: 10, steps per second: 281, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.127 [-1.742, 2.680], loss: 0.005097, mean_absolute_error: 2.380008, mean_q: 5.126907\n",
      "  2949/50000: episode: 316, duration: 0.067s, episode steps: 10, steps per second: 150, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.139 [-1.964, 3.034], loss: 0.006357, mean_absolute_error: 2.365883, mean_q: 5.092936\n",
      "  2959/50000: episode: 317, duration: 0.059s, episode steps: 10, steps per second: 171, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.148 [-1.942, 3.093], loss: 0.005749, mean_absolute_error: 2.222613, mean_q: 4.777802\n",
      "  2969/50000: episode: 318, duration: 0.070s, episode steps: 10, steps per second: 142, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.113 [-1.995, 3.031], loss: 0.004395, mean_absolute_error: 2.419604, mean_q: 5.222034\n",
      "  2978/50000: episode: 319, duration: 0.039s, episode steps: 9, steps per second: 232, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.139 [-1.800, 2.832], loss: 0.004710, mean_absolute_error: 2.215065, mean_q: 4.792049\n",
      "  2988/50000: episode: 320, duration: 0.023s, episode steps: 10, steps per second: 438, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.135 [-1.925, 2.975], loss: 0.006168, mean_absolute_error: 2.389212, mean_q: 5.158228\n",
      "  2997/50000: episode: 321, duration: 0.020s, episode steps: 9, steps per second: 448, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.141 [-1.807, 2.817], loss: 0.006234, mean_absolute_error: 2.273371, mean_q: 4.905877\n",
      "  3008/50000: episode: 322, duration: 0.023s, episode steps: 11, steps per second: 478, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.141 [-2.172, 3.312], loss: 0.005887, mean_absolute_error: 2.326790, mean_q: 5.017612\n",
      "  3017/50000: episode: 323, duration: 0.018s, episode steps: 9, steps per second: 487, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.144 [-1.739, 2.777], loss: 0.003905, mean_absolute_error: 2.329240, mean_q: 5.044447\n",
      "  3026/50000: episode: 324, duration: 0.029s, episode steps: 9, steps per second: 314, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.173 [-1.768, 2.888], loss: 0.007473, mean_absolute_error: 2.336307, mean_q: 5.037053\n",
      "  3036/50000: episode: 325, duration: 0.028s, episode steps: 10, steps per second: 353, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.112 [-1.801, 2.660], loss: 0.007372, mean_absolute_error: 2.442466, mean_q: 5.191353\n",
      "  3045/50000: episode: 326, duration: 0.020s, episode steps: 9, steps per second: 445, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.150 [-1.550, 2.464], loss: 0.009594, mean_absolute_error: 2.383900, mean_q: 5.002176\n",
      "  3053/50000: episode: 327, duration: 0.018s, episode steps: 8, steps per second: 447, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.137 [-1.535, 2.511], loss: 0.005063, mean_absolute_error: 2.437812, mean_q: 5.139262\n",
      "  3062/50000: episode: 328, duration: 0.019s, episode steps: 9, steps per second: 472, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.146 [-1.805, 2.873], loss: 0.004868, mean_absolute_error: 2.466481, mean_q: 5.206563\n",
      "  3071/50000: episode: 329, duration: 0.019s, episode steps: 9, steps per second: 486, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.151 [-1.744, 2.783], loss: 0.025217, mean_absolute_error: 2.373137, mean_q: 4.960861\n",
      "  3082/50000: episode: 330, duration: 0.022s, episode steps: 11, steps per second: 493, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.098 [-1.600, 2.264], loss: 0.005081, mean_absolute_error: 2.362718, mean_q: 4.819892\n",
      "  3091/50000: episode: 331, duration: 0.020s, episode steps: 9, steps per second: 447, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.150 [-2.818, 1.732], loss: 0.011276, mean_absolute_error: 2.473774, mean_q: 4.945955\n",
      "  3100/50000: episode: 332, duration: 0.021s, episode steps: 9, steps per second: 422, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.150 [-2.858, 1.796], loss: 0.026113, mean_absolute_error: 2.609150, mean_q: 5.110586\n",
      "  3110/50000: episode: 333, duration: 0.023s, episode steps: 10, steps per second: 433, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.152 [-3.036, 1.922], loss: 0.014716, mean_absolute_error: 2.602994, mean_q: 5.136224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3120/50000: episode: 334, duration: 0.035s, episode steps: 10, steps per second: 287, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.173 [-3.141, 1.905], loss: 0.009253, mean_absolute_error: 2.714311, mean_q: 5.378487\n",
      "  3130/50000: episode: 335, duration: 0.024s, episode steps: 10, steps per second: 414, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.132 [-3.056, 1.957], loss: 0.008492, mean_absolute_error: 2.727131, mean_q: 5.395724\n",
      "  3138/50000: episode: 336, duration: 0.019s, episode steps: 8, steps per second: 432, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.151 [-2.587, 1.567], loss: 0.007793, mean_absolute_error: 2.793782, mean_q: 5.564026\n",
      "  3147/50000: episode: 337, duration: 0.021s, episode steps: 9, steps per second: 434, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.147 [-2.871, 1.799], loss: 0.006760, mean_absolute_error: 2.810049, mean_q: 5.574044\n",
      "  3156/50000: episode: 338, duration: 0.020s, episode steps: 9, steps per second: 449, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.157 [-2.801, 1.746], loss: 0.940021, mean_absolute_error: 3.041302, mean_q: 6.008475\n",
      "  3166/50000: episode: 339, duration: 0.022s, episode steps: 10, steps per second: 446, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.151 [-3.105, 1.952], loss: 0.562149, mean_absolute_error: 2.679647, mean_q: 5.230557\n",
      "  3175/50000: episode: 340, duration: 0.019s, episode steps: 9, steps per second: 468, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.127 [-1.885, 1.177], loss: 0.775141, mean_absolute_error: 2.504121, mean_q: 5.128358\n",
      "  3186/50000: episode: 341, duration: 0.023s, episode steps: 11, steps per second: 488, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.152 [-2.100, 3.300], loss: 0.166187, mean_absolute_error: 2.473416, mean_q: 5.091548\n",
      "  3194/50000: episode: 342, duration: 0.017s, episode steps: 8, steps per second: 468, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.135 [-2.511, 1.561], loss: 0.095364, mean_absolute_error: 2.727141, mean_q: 5.342303\n",
      "  3204/50000: episode: 343, duration: 0.021s, episode steps: 10, steps per second: 482, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.170 [-3.112, 1.937], loss: 0.081842, mean_absolute_error: 3.239545, mean_q: 6.427961\n",
      "  3214/50000: episode: 344, duration: 0.033s, episode steps: 10, steps per second: 306, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.127 [-3.021, 1.936], loss: 0.817066, mean_absolute_error: 3.217387, mean_q: 6.354685\n",
      "  3223/50000: episode: 345, duration: 0.020s, episode steps: 9, steps per second: 454, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.182 [-2.867, 1.724], loss: 0.035214, mean_absolute_error: 2.990181, mean_q: 5.832850\n",
      "  3231/50000: episode: 346, duration: 0.018s, episode steps: 8, steps per second: 455, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.134 [-2.508, 1.562], loss: 1.498898, mean_absolute_error: 3.072862, mean_q: 5.883097\n",
      "  3240/50000: episode: 347, duration: 0.019s, episode steps: 9, steps per second: 474, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.151 [-2.860, 1.755], loss: 1.360064, mean_absolute_error: 3.084657, mean_q: 5.750098\n",
      "  3249/50000: episode: 348, duration: 0.018s, episode steps: 9, steps per second: 487, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.145 [-2.797, 1.761], loss: 0.898375, mean_absolute_error: 2.842279, mean_q: 5.464951\n",
      "  3258/50000: episode: 349, duration: 0.023s, episode steps: 9, steps per second: 398, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.139 [-2.792, 1.750], loss: 0.729755, mean_absolute_error: 2.976215, mean_q: 5.685495\n",
      "  3267/50000: episode: 350, duration: 0.023s, episode steps: 9, steps per second: 392, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.136 [-2.836, 1.785], loss: 0.654886, mean_absolute_error: 3.404523, mean_q: 6.622879\n",
      "  3276/50000: episode: 351, duration: 0.029s, episode steps: 9, steps per second: 312, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.146 [-2.798, 1.767], loss: 1.595418, mean_absolute_error: 3.214696, mean_q: 6.191853\n",
      "  3284/50000: episode: 352, duration: 0.026s, episode steps: 8, steps per second: 304, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.143 [-2.514, 1.548], loss: 0.672176, mean_absolute_error: 3.333659, mean_q: 6.492295\n",
      "  3294/50000: episode: 353, duration: 0.024s, episode steps: 10, steps per second: 415, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.143 [-3.006, 1.920], loss: 0.586732, mean_absolute_error: 3.326261, mean_q: 6.460101\n",
      "  3303/50000: episode: 354, duration: 0.031s, episode steps: 9, steps per second: 294, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.136 [-2.795, 1.739], loss: 1.555443, mean_absolute_error: 3.336577, mean_q: 6.435828\n",
      "  3313/50000: episode: 355, duration: 0.023s, episode steps: 10, steps per second: 428, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.114 [-2.975, 1.975], loss: 0.907911, mean_absolute_error: 3.354716, mean_q: 6.522093\n",
      "  3323/50000: episode: 356, duration: 0.024s, episode steps: 10, steps per second: 417, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.135 [-3.078, 1.991], loss: 0.106519, mean_absolute_error: 3.277586, mean_q: 6.437861\n",
      "  3332/50000: episode: 357, duration: 0.026s, episode steps: 9, steps per second: 352, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.164 [-2.843, 1.767], loss: 0.560970, mean_absolute_error: 3.727603, mean_q: 7.516685\n",
      "  3342/50000: episode: 358, duration: 0.025s, episode steps: 10, steps per second: 407, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.168 [-3.101, 1.912], loss: 1.277356, mean_absolute_error: 3.396865, mean_q: 6.634262\n",
      "  3352/50000: episode: 359, duration: 0.023s, episode steps: 10, steps per second: 437, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.121 [-3.043, 1.993], loss: 1.644836, mean_absolute_error: 3.424100, mean_q: 6.520288\n",
      "  3360/50000: episode: 360, duration: 0.020s, episode steps: 8, steps per second: 409, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.159 [-2.590, 1.559], loss: 1.206626, mean_absolute_error: 3.430794, mean_q: 6.552169\n",
      "  3369/50000: episode: 361, duration: 0.020s, episode steps: 9, steps per second: 453, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.146 [-2.837, 1.804], loss: 0.374877, mean_absolute_error: 3.530773, mean_q: 6.859377\n",
      "  3379/50000: episode: 362, duration: 0.038s, episode steps: 10, steps per second: 263, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.142 [-3.118, 2.003], loss: 2.067912, mean_absolute_error: 4.062982, mean_q: 7.934753\n",
      "  3389/50000: episode: 363, duration: 0.024s, episode steps: 10, steps per second: 419, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.134 [-2.972, 1.904], loss: 1.155994, mean_absolute_error: 3.628345, mean_q: 6.900421\n",
      "  3398/50000: episode: 364, duration: 0.021s, episode steps: 9, steps per second: 431, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.154 [-2.861, 1.751], loss: 1.176980, mean_absolute_error: 3.619082, mean_q: 6.873391\n",
      "  3408/50000: episode: 365, duration: 0.022s, episode steps: 10, steps per second: 458, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.161 [-3.094, 1.948], loss: 1.093928, mean_absolute_error: 3.647580, mean_q: 7.030383\n",
      "  3418/50000: episode: 366, duration: 0.021s, episode steps: 10, steps per second: 467, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.149 [-3.096, 1.962], loss: 1.105009, mean_absolute_error: 3.768840, mean_q: 7.283437\n",
      "  3427/50000: episode: 367, duration: 0.019s, episode steps: 9, steps per second: 480, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.172 [-2.836, 1.711], loss: 0.579053, mean_absolute_error: 3.850195, mean_q: 7.566126\n",
      "  3435/50000: episode: 368, duration: 0.017s, episode steps: 8, steps per second: 476, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.164 [-2.601, 1.532], loss: 1.297809, mean_absolute_error: 3.822916, mean_q: 7.464881\n",
      "  3445/50000: episode: 369, duration: 0.021s, episode steps: 10, steps per second: 485, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.119 [-2.980, 1.996], loss: 1.165421, mean_absolute_error: 3.662422, mean_q: 7.073745\n",
      "  3455/50000: episode: 370, duration: 0.022s, episode steps: 10, steps per second: 446, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.112 [-2.953, 1.979], loss: 0.624187, mean_absolute_error: 3.683856, mean_q: 7.142764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3463/50000: episode: 371, duration: 0.066s, episode steps: 8, steps per second: 121, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.157 [-2.564, 1.552], loss: 0.648952, mean_absolute_error: 3.577681, mean_q: 6.991995\n",
      "  3472/50000: episode: 372, duration: 0.096s, episode steps: 9, steps per second: 94, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.143 [-2.815, 1.801], loss: 0.505007, mean_absolute_error: 3.643836, mean_q: 7.199015\n",
      "  3483/50000: episode: 373, duration: 0.100s, episode steps: 11, steps per second: 109, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.147 [-3.316, 2.146], loss: 0.681721, mean_absolute_error: 3.935929, mean_q: 7.831648\n",
      "  3492/50000: episode: 374, duration: 0.035s, episode steps: 9, steps per second: 255, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.141 [-2.775, 1.751], loss: 0.534112, mean_absolute_error: 3.828513, mean_q: 7.590528\n",
      "  3500/50000: episode: 375, duration: 0.019s, episode steps: 8, steps per second: 432, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.163 [-2.576, 1.552], loss: 0.622598, mean_absolute_error: 3.848171, mean_q: 7.549652\n",
      "  3509/50000: episode: 376, duration: 0.020s, episode steps: 9, steps per second: 446, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.171 [-2.884, 1.778], loss: 0.459951, mean_absolute_error: 3.803834, mean_q: 7.473186\n",
      "  3518/50000: episode: 377, duration: 0.020s, episode steps: 9, steps per second: 453, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.140 [-2.795, 1.748], loss: 0.696560, mean_absolute_error: 4.003058, mean_q: 7.940565\n",
      "  3528/50000: episode: 378, duration: 0.021s, episode steps: 10, steps per second: 481, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.141 [-3.093, 1.948], loss: 0.336690, mean_absolute_error: 3.902723, mean_q: 7.769726\n",
      "  3537/50000: episode: 379, duration: 0.018s, episode steps: 9, steps per second: 496, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.155 [-2.865, 1.812], loss: 0.321520, mean_absolute_error: 3.737350, mean_q: 7.377063\n",
      "  3545/50000: episode: 380, duration: 0.017s, episode steps: 8, steps per second: 482, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.164 [-2.591, 1.597], loss: 0.362916, mean_absolute_error: 3.864771, mean_q: 7.711014\n",
      "  3555/50000: episode: 381, duration: 0.021s, episode steps: 10, steps per second: 478, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.132 [-3.001, 1.953], loss: 0.125651, mean_absolute_error: 3.896392, mean_q: 7.852655\n",
      "  3564/50000: episode: 382, duration: 0.019s, episode steps: 9, steps per second: 469, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.143 [-2.812, 1.797], loss: 0.273689, mean_absolute_error: 3.799351, mean_q: 7.553611\n",
      "  3574/50000: episode: 383, duration: 0.021s, episode steps: 10, steps per second: 486, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.141 [-3.051, 1.985], loss: 0.277191, mean_absolute_error: 3.786161, mean_q: 7.529199\n",
      "  3584/50000: episode: 384, duration: 0.021s, episode steps: 10, steps per second: 470, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.143 [-2.786, 1.809], loss: 0.186044, mean_absolute_error: 3.839946, mean_q: 7.651594\n",
      "  3593/50000: episode: 385, duration: 0.032s, episode steps: 9, steps per second: 283, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.144 [-2.813, 1.788], loss: 0.314975, mean_absolute_error: 4.057375, mean_q: 8.113754\n",
      "  3602/50000: episode: 386, duration: 0.026s, episode steps: 9, steps per second: 341, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.151 [-2.851, 1.790], loss: 0.256254, mean_absolute_error: 4.056827, mean_q: 8.158478\n",
      "  3611/50000: episode: 387, duration: 0.021s, episode steps: 9, steps per second: 433, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.148 [-2.784, 1.738], loss: 0.340763, mean_absolute_error: 3.599902, mean_q: 7.088486\n",
      "  3620/50000: episode: 388, duration: 0.020s, episode steps: 9, steps per second: 443, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.167 [-2.823, 1.736], loss: 0.117459, mean_absolute_error: 3.886646, mean_q: 7.826928\n",
      "  3630/50000: episode: 389, duration: 0.024s, episode steps: 10, steps per second: 425, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.110 [-2.972, 1.999], loss: 0.157514, mean_absolute_error: 4.154047, mean_q: 8.364302\n",
      "  3639/50000: episode: 390, duration: 0.021s, episode steps: 9, steps per second: 431, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.143 [-2.773, 1.739], loss: 0.161351, mean_absolute_error: 3.857354, mean_q: 7.721882\n",
      "  3648/50000: episode: 391, duration: 0.021s, episode steps: 9, steps per second: 421, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.151 [-2.851, 1.791], loss: 0.152195, mean_absolute_error: 3.693370, mean_q: 7.385379\n",
      "  3657/50000: episode: 392, duration: 0.021s, episode steps: 9, steps per second: 425, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.126 [-2.456, 1.572], loss: 0.055410, mean_absolute_error: 3.720083, mean_q: 7.467774\n",
      "  3667/50000: episode: 393, duration: 0.027s, episode steps: 10, steps per second: 367, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.151 [-2.739, 1.721], loss: 0.090235, mean_absolute_error: 3.882309, mean_q: 7.815811\n",
      "  3677/50000: episode: 394, duration: 0.044s, episode steps: 10, steps per second: 226, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.125 [-2.438, 1.516], loss: 0.073794, mean_absolute_error: 4.007423, mean_q: 8.024571\n",
      "  3686/50000: episode: 395, duration: 0.027s, episode steps: 9, steps per second: 334, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.155 [-2.480, 1.525], loss: 0.042846, mean_absolute_error: 3.888885, mean_q: 7.823090\n",
      "  3696/50000: episode: 396, duration: 0.023s, episode steps: 10, steps per second: 433, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.137 [-2.459, 1.561], loss: 0.082482, mean_absolute_error: 3.805497, mean_q: 7.615254\n",
      "  3706/50000: episode: 397, duration: 0.024s, episode steps: 10, steps per second: 414, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.145 [-2.442, 1.520], loss: 0.170916, mean_absolute_error: 3.704922, mean_q: 7.418282\n",
      "  3715/50000: episode: 398, duration: 0.021s, episode steps: 9, steps per second: 420, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.129 [-2.436, 1.558], loss: 0.038590, mean_absolute_error: 3.671407, mean_q: 7.379050\n",
      "  3725/50000: episode: 399, duration: 0.023s, episode steps: 10, steps per second: 428, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.127 [-2.510, 1.602], loss: 0.075631, mean_absolute_error: 3.694878, mean_q: 7.444582\n",
      "  3735/50000: episode: 400, duration: 0.024s, episode steps: 10, steps per second: 415, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.124 [-2.374, 1.546], loss: 0.056356, mean_absolute_error: 3.754897, mean_q: 7.560664\n",
      "  3743/50000: episode: 401, duration: 0.019s, episode steps: 8, steps per second: 415, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.153 [-2.570, 1.597], loss: 0.066166, mean_absolute_error: 3.889004, mean_q: 7.848294\n",
      "  3754/50000: episode: 402, duration: 0.026s, episode steps: 11, steps per second: 421, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.138 [-2.607, 1.726], loss: 0.033083, mean_absolute_error: 3.677944, mean_q: 7.410545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3763/50000: episode: 403, duration: 0.028s, episode steps: 9, steps per second: 321, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.149 [-2.508, 1.605], loss: 0.070926, mean_absolute_error: 3.751109, mean_q: 7.485025\n",
      "  3774/50000: episode: 404, duration: 0.029s, episode steps: 11, steps per second: 374, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.123 [-2.315, 1.549], loss: 0.105802, mean_absolute_error: 3.690132, mean_q: 7.412941\n",
      "  3782/50000: episode: 405, duration: 0.018s, episode steps: 8, steps per second: 451, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.875 [0.000, 1.000], mean observation: -0.142 [-2.209, 1.414], loss: 0.051916, mean_absolute_error: 3.756495, mean_q: 7.567923\n",
      "  3790/50000: episode: 406, duration: 0.017s, episode steps: 8, steps per second: 459, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.875 [0.000, 1.000], mean observation: -0.132 [-2.149, 1.349], loss: 0.084171, mean_absolute_error: 3.569908, mean_q: 7.190097\n",
      "  3800/50000: episode: 407, duration: 0.020s, episode steps: 10, steps per second: 492, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.124 [-2.176, 1.365], loss: 0.036828, mean_absolute_error: 3.739779, mean_q: 7.530332\n",
      "  3810/50000: episode: 408, duration: 0.027s, episode steps: 10, steps per second: 364, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.125 [-2.145, 1.340], loss: 0.096730, mean_absolute_error: 3.608670, mean_q: 7.193220\n",
      "  3818/50000: episode: 409, duration: 0.018s, episode steps: 8, steps per second: 445, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.875 [0.000, 1.000], mean observation: -0.131 [-2.232, 1.420], loss: 0.056501, mean_absolute_error: 3.636600, mean_q: 7.296984\n",
      "  3828/50000: episode: 410, duration: 0.022s, episode steps: 10, steps per second: 460, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.123 [-2.122, 1.358], loss: 0.054711, mean_absolute_error: 3.620221, mean_q: 7.251727\n",
      "  3838/50000: episode: 411, duration: 0.022s, episode steps: 10, steps per second: 464, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.122 [-2.364, 1.574], loss: 0.044068, mean_absolute_error: 3.703895, mean_q: 7.427527\n",
      "  3847/50000: episode: 412, duration: 0.019s, episode steps: 9, steps per second: 479, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.124 [-2.195, 1.396], loss: 0.083002, mean_absolute_error: 3.581421, mean_q: 7.102571\n",
      "  3856/50000: episode: 413, duration: 0.031s, episode steps: 9, steps per second: 289, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.133 [-2.200, 1.396], loss: 0.047353, mean_absolute_error: 3.676329, mean_q: 7.404806\n",
      "  3866/50000: episode: 414, duration: 0.022s, episode steps: 10, steps per second: 459, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.145 [-2.167, 1.329], loss: 0.061680, mean_absolute_error: 3.750889, mean_q: 7.507421\n",
      "  3876/50000: episode: 415, duration: 0.022s, episode steps: 10, steps per second: 448, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.143 [-2.177, 1.368], loss: 0.038372, mean_absolute_error: 3.402285, mean_q: 6.783846\n",
      "  3885/50000: episode: 416, duration: 0.023s, episode steps: 9, steps per second: 391, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.135 [-2.180, 1.364], loss: 0.112401, mean_absolute_error: 3.651118, mean_q: 7.324800\n",
      "  3896/50000: episode: 417, duration: 0.026s, episode steps: 11, steps per second: 417, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.098 [-2.174, 1.409], loss: 0.052025, mean_absolute_error: 3.478060, mean_q: 6.978613\n",
      "  3907/50000: episode: 418, duration: 0.026s, episode steps: 11, steps per second: 431, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.115 [-2.313, 1.552], loss: 0.051997, mean_absolute_error: 3.557156, mean_q: 7.093003\n",
      "  3916/50000: episode: 419, duration: 0.022s, episode steps: 9, steps per second: 416, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.157 [-2.254, 1.358], loss: 0.047956, mean_absolute_error: 3.626546, mean_q: 7.338148\n",
      "  3926/50000: episode: 420, duration: 0.023s, episode steps: 10, steps per second: 426, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.130 [-2.138, 1.358], loss: 0.145330, mean_absolute_error: 3.781455, mean_q: 7.532235\n",
      "  3935/50000: episode: 421, duration: 0.021s, episode steps: 9, steps per second: 434, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.138 [-2.166, 1.378], loss: 0.089350, mean_absolute_error: 3.714888, mean_q: 7.433142\n",
      "  3946/50000: episode: 422, duration: 0.052s, episode steps: 11, steps per second: 212, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.144 [-2.336, 1.512], loss: 0.128444, mean_absolute_error: 3.581420, mean_q: 7.141108\n",
      "  3956/50000: episode: 423, duration: 0.023s, episode steps: 10, steps per second: 439, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.126 [-2.122, 1.353], loss: 0.080550, mean_absolute_error: 3.495128, mean_q: 7.050117\n",
      "  3966/50000: episode: 424, duration: 0.022s, episode steps: 10, steps per second: 462, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.113 [-2.114, 1.388], loss: 0.099096, mean_absolute_error: 3.356580, mean_q: 6.719663\n",
      "  3976/50000: episode: 425, duration: 0.022s, episode steps: 10, steps per second: 463, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.141 [-2.115, 1.359], loss: 0.077513, mean_absolute_error: 3.520778, mean_q: 7.118679\n",
      "  3987/50000: episode: 426, duration: 0.024s, episode steps: 11, steps per second: 468, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.122 [-2.047, 1.353], loss: 0.104968, mean_absolute_error: 3.508843, mean_q: 7.033374\n",
      "  3998/50000: episode: 427, duration: 0.023s, episode steps: 11, steps per second: 485, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.100 [-1.905, 1.212], loss: 0.104492, mean_absolute_error: 3.394392, mean_q: 6.742913\n",
      "  4009/50000: episode: 428, duration: 0.022s, episode steps: 11, steps per second: 498, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.110 [-1.868, 1.179], loss: 0.098635, mean_absolute_error: 3.348722, mean_q: 6.705809\n",
      "  4020/50000: episode: 429, duration: 0.028s, episode steps: 11, steps per second: 392, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.119 [-1.877, 1.158], loss: 0.068618, mean_absolute_error: 3.496890, mean_q: 6.976672\n",
      "  4029/50000: episode: 430, duration: 0.023s, episode steps: 9, steps per second: 397, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.139 [-1.885, 1.162], loss: 0.077192, mean_absolute_error: 3.437148, mean_q: 6.898164\n",
      "  4041/50000: episode: 431, duration: 0.042s, episode steps: 12, steps per second: 286, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.109 [-1.785, 1.164], loss: 0.074555, mean_absolute_error: 3.379300, mean_q: 6.722004\n",
      "  4050/50000: episode: 432, duration: 0.046s, episode steps: 9, steps per second: 194, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.130 [-1.762, 0.996], loss: 0.149654, mean_absolute_error: 3.383687, mean_q: 6.774906\n",
      "  4061/50000: episode: 433, duration: 0.035s, episode steps: 11, steps per second: 313, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.111 [-1.784, 1.185], loss: 0.076974, mean_absolute_error: 3.291455, mean_q: 6.563606\n",
      "  4070/50000: episode: 434, duration: 0.025s, episode steps: 9, steps per second: 366, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.128 [-1.858, 1.173], loss: 0.075119, mean_absolute_error: 3.405749, mean_q: 6.818067\n",
      "  4079/50000: episode: 435, duration: 0.020s, episode steps: 9, steps per second: 446, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.123 [-1.850, 1.219], loss: 0.106031, mean_absolute_error: 3.346882, mean_q: 6.627362\n",
      "  4090/50000: episode: 436, duration: 0.038s, episode steps: 11, steps per second: 293, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.108 [-1.815, 1.182], loss: 0.077207, mean_absolute_error: 3.599648, mean_q: 7.262627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4102/50000: episode: 437, duration: 0.048s, episode steps: 12, steps per second: 247, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.110 [-1.736, 1.134], loss: 0.203871, mean_absolute_error: 3.227740, mean_q: 6.385920\n",
      "  4113/50000: episode: 438, duration: 0.045s, episode steps: 11, steps per second: 243, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.115 [-1.751, 1.211], loss: 0.131449, mean_absolute_error: 3.257507, mean_q: 6.475636\n",
      "  4122/50000: episode: 439, duration: 0.034s, episode steps: 9, steps per second: 261, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.131 [-1.729, 0.988], loss: 0.110881, mean_absolute_error: 3.302725, mean_q: 6.582163\n",
      "  4133/50000: episode: 440, duration: 0.039s, episode steps: 11, steps per second: 282, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.108 [-1.594, 0.995], loss: 0.145714, mean_absolute_error: 3.187352, mean_q: 6.353144\n",
      "  4143/50000: episode: 441, duration: 0.036s, episode steps: 10, steps per second: 275, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.123 [-1.541, 0.981], loss: 0.132778, mean_absolute_error: 3.091285, mean_q: 6.140418\n",
      "  4156/50000: episode: 442, duration: 0.044s, episode steps: 13, steps per second: 294, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.081 [-1.235, 0.796], loss: 0.065932, mean_absolute_error: 3.266777, mean_q: 6.484832\n",
      "  4171/50000: episode: 443, duration: 0.149s, episode steps: 15, steps per second: 101, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.086 [-1.213, 0.798], loss: 0.113399, mean_absolute_error: 3.326970, mean_q: 6.586590\n",
      "  4184/50000: episode: 444, duration: 0.065s, episode steps: 13, steps per second: 200, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.108 [-1.275, 0.780], loss: 0.150936, mean_absolute_error: 3.256716, mean_q: 6.425690\n",
      "  4197/50000: episode: 445, duration: 0.043s, episode steps: 13, steps per second: 301, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.096 [-1.461, 1.003], loss: 0.285918, mean_absolute_error: 3.286544, mean_q: 6.441794\n",
      "  4208/50000: episode: 446, duration: 0.043s, episode steps: 11, steps per second: 253, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.118 [-1.313, 0.764], loss: 0.187534, mean_absolute_error: 3.186417, mean_q: 6.257591\n",
      "  4219/50000: episode: 447, duration: 0.064s, episode steps: 11, steps per second: 171, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.116 [-1.312, 0.744], loss: 0.237955, mean_absolute_error: 3.448956, mean_q: 6.777578\n",
      "  4237/50000: episode: 448, duration: 0.094s, episode steps: 18, steps per second: 191, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.076 [-1.155, 0.813], loss: 0.170292, mean_absolute_error: 3.257452, mean_q: 6.442194\n",
      "  4254/50000: episode: 449, duration: 0.066s, episode steps: 17, steps per second: 257, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.095 [-0.930, 0.615], loss: 0.148316, mean_absolute_error: 3.297278, mean_q: 6.523448\n",
      "  4273/50000: episode: 450, duration: 0.075s, episode steps: 19, steps per second: 253, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.110 [-0.912, 0.539], loss: 0.219199, mean_absolute_error: 3.270462, mean_q: 6.378834\n",
      "  4296/50000: episode: 451, duration: 0.091s, episode steps: 23, steps per second: 253, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.079 [-0.883, 0.569], loss: 0.221670, mean_absolute_error: 3.288941, mean_q: 6.406741\n",
      "  4325/50000: episode: 452, duration: 0.086s, episode steps: 29, steps per second: 337, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.099 [-0.720, 0.359], loss: 0.303779, mean_absolute_error: 3.396139, mean_q: 6.575426\n",
      "  4347/50000: episode: 453, duration: 0.051s, episode steps: 22, steps per second: 430, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.108 [-0.951, 0.623], loss: 0.285423, mean_absolute_error: 3.153767, mean_q: 6.094484\n",
      "  4378/50000: episode: 454, duration: 0.060s, episode steps: 31, steps per second: 520, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.130 [-0.764, 0.352], loss: 0.376511, mean_absolute_error: 3.252857, mean_q: 6.274396\n",
      "  4400/50000: episode: 455, duration: 0.085s, episode steps: 22, steps per second: 258, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.107 [-0.688, 0.413], loss: 0.431726, mean_absolute_error: 3.401505, mean_q: 6.494948\n",
      "  4447/50000: episode: 456, duration: 0.197s, episode steps: 47, steps per second: 238, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.319 [0.000, 1.000], mean observation: -0.097 [-3.221, 3.644], loss: 0.351267, mean_absolute_error: 3.281895, mean_q: 6.302957\n",
      "  4509/50000: episode: 457, duration: 0.278s, episode steps: 62, steps per second: 223, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.118 [-0.740, 0.373], loss: 0.459681, mean_absolute_error: 3.369592, mean_q: 6.406317\n",
      "  4563/50000: episode: 458, duration: 0.257s, episode steps: 54, steps per second: 210, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.124 [-0.708, 0.215], loss: 0.597892, mean_absolute_error: 3.480361, mean_q: 6.555920\n",
      "  4609/50000: episode: 459, duration: 0.219s, episode steps: 46, steps per second: 210, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.152 [-0.749, 0.178], loss: 0.483240, mean_absolute_error: 3.679744, mean_q: 7.022729\n",
      "  4651/50000: episode: 460, duration: 0.111s, episode steps: 42, steps per second: 379, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.158 [-0.741, 0.188], loss: 0.777530, mean_absolute_error: 3.638376, mean_q: 6.876372\n",
      "  4704/50000: episode: 461, duration: 0.259s, episode steps: 53, steps per second: 205, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.113 [-0.873, 0.236], loss: 0.505460, mean_absolute_error: 3.733004, mean_q: 7.167234\n",
      "  4752/50000: episode: 462, duration: 0.263s, episode steps: 48, steps per second: 182, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.106 [-0.729, 0.211], loss: 0.617174, mean_absolute_error: 3.830771, mean_q: 7.323199\n",
      "  4804/50000: episode: 463, duration: 0.150s, episode steps: 52, steps per second: 346, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: -0.181 [-1.085, 0.205], loss: 0.507919, mean_absolute_error: 3.963941, mean_q: 7.652368\n",
      "  4854/50000: episode: 464, duration: 0.223s, episode steps: 50, steps per second: 224, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.149 [-0.771, 0.157], loss: 0.610669, mean_absolute_error: 3.982960, mean_q: 7.728628\n",
      "  4916/50000: episode: 465, duration: 0.118s, episode steps: 62, steps per second: 527, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.142 [-0.927, 0.368], loss: 0.568820, mean_absolute_error: 4.061411, mean_q: 7.910660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4967/50000: episode: 466, duration: 0.106s, episode steps: 51, steps per second: 480, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.451 [0.000, 1.000], mean observation: -0.151 [-0.884, 0.415], loss: 0.697392, mean_absolute_error: 4.320223, mean_q: 8.377643\n",
      "  5023/50000: episode: 467, duration: 0.109s, episode steps: 56, steps per second: 516, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.446 [0.000, 1.000], mean observation: -0.176 [-1.089, 0.197], loss: 0.655693, mean_absolute_error: 4.379984, mean_q: 8.510307\n",
      "  5090/50000: episode: 468, duration: 0.136s, episode steps: 67, steps per second: 492, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.139 [-0.956, 0.157], loss: 0.657748, mean_absolute_error: 4.418839, mean_q: 8.625309\n",
      "  5149/50000: episode: 469, duration: 0.111s, episode steps: 59, steps per second: 532, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.150 [-0.935, 0.171], loss: 0.802913, mean_absolute_error: 4.485634, mean_q: 8.770724\n",
      "  5222/50000: episode: 470, duration: 0.287s, episode steps: 73, steps per second: 254, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.135 [-1.075, 0.213], loss: 0.496387, mean_absolute_error: 4.636470, mean_q: 9.217533\n",
      "  5288/50000: episode: 471, duration: 0.128s, episode steps: 66, steps per second: 516, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.132 [-1.049, 0.431], loss: 0.670206, mean_absolute_error: 4.838533, mean_q: 9.571821\n",
      "  5350/50000: episode: 472, duration: 0.253s, episode steps: 62, steps per second: 245, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.154 [-1.076, 0.214], loss: 0.649459, mean_absolute_error: 4.936061, mean_q: 9.789312\n",
      "  5409/50000: episode: 473, duration: 0.112s, episode steps: 59, steps per second: 528, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.136 [-0.909, 0.389], loss: 0.691058, mean_absolute_error: 5.164345, mean_q: 10.230453\n",
      "  5486/50000: episode: 474, duration: 0.157s, episode steps: 77, steps per second: 491, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.122 [-1.279, 0.382], loss: 0.728831, mean_absolute_error: 5.105687, mean_q: 10.156374\n",
      "  5561/50000: episode: 475, duration: 0.145s, episode steps: 75, steps per second: 518, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.136 [-1.249, 0.417], loss: 0.557028, mean_absolute_error: 5.294194, mean_q: 10.600855\n",
      "  5616/50000: episode: 476, duration: 0.289s, episode steps: 55, steps per second: 191, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.436 [0.000, 1.000], mean observation: -0.221 [-1.284, 0.194], loss: 0.934429, mean_absolute_error: 5.579984, mean_q: 11.146115\n",
      "  5677/50000: episode: 477, duration: 0.223s, episode steps: 61, steps per second: 274, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.443 [0.000, 1.000], mean observation: -0.179 [-1.266, 0.401], loss: 0.650917, mean_absolute_error: 5.795013, mean_q: 11.671577\n",
      "  5734/50000: episode: 478, duration: 0.111s, episode steps: 57, steps per second: 516, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.439 [0.000, 1.000], mean observation: -0.198 [-1.260, 0.219], loss: 0.548361, mean_absolute_error: 5.794700, mean_q: 11.695239\n",
      "  5821/50000: episode: 479, duration: 0.186s, episode steps: 87, steps per second: 469, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: -0.179 [-1.630, 0.402], loss: 0.718829, mean_absolute_error: 5.986193, mean_q: 12.073511\n",
      "  5899/50000: episode: 480, duration: 0.204s, episode steps: 78, steps per second: 382, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.449 [0.000, 1.000], mean observation: -0.178 [-1.463, 0.383], loss: 0.665463, mean_absolute_error: 6.188035, mean_q: 12.479424\n",
      "  5981/50000: episode: 481, duration: 0.169s, episode steps: 82, steps per second: 486, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.439 [0.000, 1.000], mean observation: -0.256 [-1.801, 0.223], loss: 0.553268, mean_absolute_error: 6.380387, mean_q: 12.915763\n",
      "  6079/50000: episode: 482, duration: 0.198s, episode steps: 98, steps per second: 495, episode reward: 98.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.439 [0.000, 1.000], mean observation: -0.296 [-2.150, 0.426], loss: 0.428213, mean_absolute_error: 6.627463, mean_q: 13.514757\n",
      "  6170/50000: episode: 483, duration: 0.189s, episode steps: 91, steps per second: 482, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: -0.275 [-1.976, 0.422], loss: 0.451524, mean_absolute_error: 6.637799, mean_q: 13.566780\n",
      "  6261/50000: episode: 484, duration: 0.191s, episode steps: 91, steps per second: 477, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: -0.288 [-2.011, 0.385], loss: 0.498914, mean_absolute_error: 6.974544, mean_q: 14.262832\n",
      "  6356/50000: episode: 485, duration: 0.178s, episode steps: 95, steps per second: 532, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.432 [0.000, 1.000], mean observation: -0.345 [-2.362, 0.227], loss: 0.649474, mean_absolute_error: 7.100592, mean_q: 14.508822\n",
      "  6450/50000: episode: 486, duration: 0.232s, episode steps: 94, steps per second: 405, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.436 [0.000, 1.000], mean observation: -0.339 [-2.157, 0.420], loss: 0.903089, mean_absolute_error: 7.157202, mean_q: 14.646951\n",
      "  6564/50000: episode: 487, duration: 0.259s, episode steps: 114, steps per second: 440, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.439 [0.000, 1.000], mean observation: -0.438 [-2.545, 0.398], loss: 0.580684, mean_absolute_error: 7.521745, mean_q: 15.457897\n",
      "  6667/50000: episode: 488, duration: 0.195s, episode steps: 103, steps per second: 527, episode reward: 103.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.427 [0.000, 1.000], mean observation: -0.531 [-2.928, 0.286], loss: 0.806462, mean_absolute_error: 7.701509, mean_q: 15.870831\n",
      "  6789/50000: episode: 489, duration: 0.267s, episode steps: 122, steps per second: 456, episode reward: 122.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.443 [0.000, 1.000], mean observation: -0.459 [-2.572, 0.376], loss: 0.756622, mean_absolute_error: 7.996643, mean_q: 16.531935\n",
      "  6907/50000: episode: 490, duration: 0.449s, episode steps: 118, steps per second: 263, episode reward: 118.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.441 [0.000, 1.000], mean observation: -0.462 [-2.540, 0.410], loss: 0.577321, mean_absolute_error: 8.280868, mean_q: 17.194489\n",
      "  7028/50000: episode: 491, duration: 0.262s, episode steps: 121, steps per second: 461, episode reward: 121.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.446 [0.000, 1.000], mean observation: -0.459 [-2.584, 0.358], loss: 0.942648, mean_absolute_error: 8.567242, mean_q: 17.667282\n",
      "  7140/50000: episode: 492, duration: 0.222s, episode steps: 112, steps per second: 505, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.446 [0.000, 1.000], mean observation: -0.493 [-2.411, 0.359], loss: 0.775575, mean_absolute_error: 8.959222, mean_q: 18.537680\n",
      "  7248/50000: episode: 493, duration: 0.218s, episode steps: 108, steps per second: 495, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: -0.514 [-2.773, 0.288], loss: 0.631736, mean_absolute_error: 8.970156, mean_q: 18.622025\n",
      "  7363/50000: episode: 494, duration: 0.292s, episode steps: 115, steps per second: 394, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: -0.485 [-2.766, 0.302], loss: 1.251828, mean_absolute_error: 9.196886, mean_q: 18.981857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7480/50000: episode: 495, duration: 0.252s, episode steps: 117, steps per second: 464, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.436 [0.000, 1.000], mean observation: -0.471 [-2.911, 0.399], loss: 1.283955, mean_absolute_error: 9.260300, mean_q: 19.218445\n",
      "  7595/50000: episode: 496, duration: 0.246s, episode steps: 115, steps per second: 467, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: -0.476 [-3.552, 1.092], loss: 1.135483, mean_absolute_error: 9.567020, mean_q: 19.810492\n",
      "  7715/50000: episode: 497, duration: 0.274s, episode steps: 120, steps per second: 438, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.433 [0.000, 1.000], mean observation: -0.463 [-2.916, 0.579], loss: 1.290864, mean_absolute_error: 9.651331, mean_q: 20.085245\n",
      "  7828/50000: episode: 498, duration: 0.237s, episode steps: 113, steps per second: 476, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.407 [0.000, 1.000], mean observation: -0.475 [-3.857, 1.813], loss: 0.878977, mean_absolute_error: 9.773899, mean_q: 20.597601\n",
      "  7945/50000: episode: 499, duration: 0.240s, episode steps: 117, steps per second: 487, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.419 [0.000, 1.000], mean observation: -0.478 [-3.506, 0.985], loss: 1.152878, mean_absolute_error: 10.082108, mean_q: 21.158428\n",
      "  8055/50000: episode: 500, duration: 0.218s, episode steps: 110, steps per second: 504, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.436 [0.000, 1.000], mean observation: -0.407 [-2.577, 0.361], loss: 1.056680, mean_absolute_error: 10.453150, mean_q: 22.013296\n",
      "  8163/50000: episode: 501, duration: 0.221s, episode steps: 108, steps per second: 489, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: -0.486 [-2.560, 0.376], loss: 1.122299, mean_absolute_error: 10.846780, mean_q: 22.785376\n",
      "  8277/50000: episode: 502, duration: 0.254s, episode steps: 114, steps per second: 448, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.439 [0.000, 1.000], mean observation: -0.454 [-2.567, 0.372], loss: 1.428255, mean_absolute_error: 11.153031, mean_q: 23.301489\n",
      "  8397/50000: episode: 503, duration: 0.240s, episode steps: 120, steps per second: 500, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: -0.447 [-2.549, 0.388], loss: 1.095333, mean_absolute_error: 11.277712, mean_q: 23.723295\n",
      "  8512/50000: episode: 504, duration: 0.258s, episode steps: 115, steps per second: 445, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.443 [0.000, 1.000], mean observation: -0.380 [-2.325, 0.431], loss: 1.292722, mean_absolute_error: 11.634744, mean_q: 24.379940\n",
      "  8638/50000: episode: 505, duration: 0.270s, episode steps: 126, steps per second: 466, episode reward: 126.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.382 [-2.573, 0.360], loss: 1.144804, mean_absolute_error: 11.895370, mean_q: 24.856855\n",
      "  8763/50000: episode: 506, duration: 0.265s, episode steps: 125, steps per second: 472, episode reward: 125.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: -0.447 [-2.680, 0.438], loss: 1.507874, mean_absolute_error: 12.061643, mean_q: 25.126158\n",
      "  8887/50000: episode: 507, duration: 0.263s, episode steps: 124, steps per second: 471, episode reward: 124.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.450 [-2.591, 0.538], loss: 1.597431, mean_absolute_error: 12.418707, mean_q: 25.964676\n",
      "  9008/50000: episode: 508, duration: 0.247s, episode steps: 121, steps per second: 490, episode reward: 121.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.464 [-2.713, 0.410], loss: 1.235560, mean_absolute_error: 12.591208, mean_q: 26.506491\n",
      "  9126/50000: episode: 509, duration: 0.262s, episode steps: 118, steps per second: 450, episode reward: 118.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.441 [0.000, 1.000], mean observation: -0.479 [-2.730, 0.394], loss: 1.335881, mean_absolute_error: 12.737265, mean_q: 26.708105\n",
      "  9249/50000: episode: 510, duration: 0.258s, episode steps: 123, steps per second: 477, episode reward: 123.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.439 [0.000, 1.000], mean observation: -0.461 [-2.703, 0.422], loss: 1.861092, mean_absolute_error: 12.976767, mean_q: 27.206076\n",
      "  9373/50000: episode: 511, duration: 0.262s, episode steps: 124, steps per second: 474, episode reward: 124.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.449 [-2.692, 0.622], loss: 0.966212, mean_absolute_error: 13.217938, mean_q: 27.899584\n",
      "  9489/50000: episode: 512, duration: 0.246s, episode steps: 116, steps per second: 472, episode reward: 116.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: -0.486 [-2.754, 0.368], loss: 1.760386, mean_absolute_error: 13.661997, mean_q: 28.770288\n",
      "  9598/50000: episode: 513, duration: 0.225s, episode steps: 109, steps per second: 485, episode reward: 109.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.431 [0.000, 1.000], mean observation: -0.508 [-2.777, 0.305], loss: 1.302974, mean_absolute_error: 13.537386, mean_q: 28.586699\n",
      "  9712/50000: episode: 514, duration: 0.246s, episode steps: 114, steps per second: 464, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.439 [0.000, 1.000], mean observation: -0.488 [-2.770, 0.269], loss: 1.098254, mean_absolute_error: 13.874184, mean_q: 29.422331\n",
      "  9819/50000: episode: 515, duration: 0.243s, episode steps: 107, steps per second: 441, episode reward: 107.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.430 [0.000, 1.000], mean observation: -0.518 [-2.777, 0.351], loss: 1.581986, mean_absolute_error: 14.086767, mean_q: 29.698471\n",
      "  9925/50000: episode: 516, duration: 0.230s, episode steps: 106, steps per second: 462, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.425 [0.000, 1.000], mean observation: -0.524 [-2.927, 0.256], loss: 1.704670, mean_absolute_error: 14.327230, mean_q: 30.148079\n",
      " 10039/50000: episode: 517, duration: 0.237s, episode steps: 114, steps per second: 482, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.430 [0.000, 1.000], mean observation: -0.489 [-2.912, 0.398], loss: 1.663125, mean_absolute_error: 14.406937, mean_q: 30.449535\n",
      " 10149/50000: episode: 518, duration: 0.229s, episode steps: 110, steps per second: 480, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.418 [0.000, 1.000], mean observation: -0.495 [-3.316, 0.907], loss: 1.402916, mean_absolute_error: 14.385927, mean_q: 30.519169\n",
      " 10260/50000: episode: 519, duration: 0.226s, episode steps: 111, steps per second: 492, episode reward: 111.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.432 [0.000, 1.000], mean observation: -0.498 [-2.730, 0.399], loss: 1.463839, mean_absolute_error: 14.769921, mean_q: 31.185905\n",
      " 10376/50000: episode: 520, duration: 0.248s, episode steps: 116, steps per second: 468, episode reward: 116.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: -0.480 [-2.597, 0.344], loss: 1.201000, mean_absolute_error: 15.205413, mean_q: 31.995804\n",
      " 10488/50000: episode: 521, duration: 0.230s, episode steps: 112, steps per second: 487, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.446 [0.000, 1.000], mean observation: -0.500 [-2.596, 0.344], loss: 1.282020, mean_absolute_error: 15.331650, mean_q: 32.414562\n",
      " 10603/50000: episode: 522, duration: 0.231s, episode steps: 115, steps per second: 498, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: -0.488 [-2.962, 0.351], loss: 1.196957, mean_absolute_error: 15.348052, mean_q: 32.577576\n",
      " 10717/50000: episode: 523, duration: 0.239s, episode steps: 114, steps per second: 477, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.439 [0.000, 1.000], mean observation: -0.491 [-2.782, 0.278], loss: 1.537889, mean_absolute_error: 15.856549, mean_q: 33.531986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10837/50000: episode: 524, duration: 0.252s, episode steps: 120, steps per second: 475, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.433 [0.000, 1.000], mean observation: -0.464 [-2.937, 0.377], loss: 1.164411, mean_absolute_error: 15.924143, mean_q: 33.719894\n",
      " 10958/50000: episode: 525, duration: 0.255s, episode steps: 121, steps per second: 474, episode reward: 121.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.462 [-2.945, 0.400], loss: 1.159465, mean_absolute_error: 15.677879, mean_q: 33.278290\n",
      " 11068/50000: episode: 526, duration: 0.222s, episode steps: 110, steps per second: 496, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.418 [0.000, 1.000], mean observation: -0.504 [-3.329, 0.943], loss: 1.926086, mean_absolute_error: 15.784049, mean_q: 33.418446\n",
      " 11188/50000: episode: 527, duration: 0.237s, episode steps: 120, steps per second: 507, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.433 [0.000, 1.000], mean observation: -0.464 [-2.896, 0.615], loss: 1.183388, mean_absolute_error: 16.083950, mean_q: 34.288860\n",
      " 11309/50000: episode: 528, duration: 0.251s, episode steps: 121, steps per second: 482, episode reward: 121.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.465 [-2.708, 0.420], loss: 1.258864, mean_absolute_error: 16.221928, mean_q: 34.538349\n",
      " 11414/50000: episode: 529, duration: 0.227s, episode steps: 105, steps per second: 463, episode reward: 105.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: -0.529 [-2.942, 0.399], loss: 1.518045, mean_absolute_error: 16.380718, mean_q: 34.846401\n",
      " 11527/50000: episode: 530, duration: 0.233s, episode steps: 113, steps per second: 484, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.434 [0.000, 1.000], mean observation: -0.495 [-2.783, 0.361], loss: 1.352325, mean_absolute_error: 16.897888, mean_q: 35.937889\n",
      " 11632/50000: episode: 531, duration: 0.226s, episode steps: 105, steps per second: 465, episode reward: 105.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.524 [-2.790, 0.308], loss: 1.238459, mean_absolute_error: 17.015457, mean_q: 36.182987\n",
      " 11746/50000: episode: 532, duration: 0.252s, episode steps: 114, steps per second: 453, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.430 [0.000, 1.000], mean observation: -0.479 [-2.882, 0.628], loss: 1.289092, mean_absolute_error: 16.979145, mean_q: 36.114456\n",
      " 11868/50000: episode: 533, duration: 0.259s, episode steps: 122, steps per second: 470, episode reward: 122.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.434 [0.000, 1.000], mean observation: -0.452 [-2.951, 0.369], loss: 1.546200, mean_absolute_error: 17.015818, mean_q: 36.065586\n",
      " 11986/50000: episode: 534, duration: 0.240s, episode steps: 118, steps per second: 492, episode reward: 118.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.441 [0.000, 1.000], mean observation: -0.473 [-2.720, 0.605], loss: 1.556064, mean_absolute_error: 17.395233, mean_q: 37.002010\n",
      " 12103/50000: episode: 535, duration: 0.246s, episode steps: 117, steps per second: 475, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.436 [0.000, 1.000], mean observation: -0.481 [-2.905, 0.409], loss: 1.440158, mean_absolute_error: 17.139858, mean_q: 36.594387\n",
      " 12216/50000: episode: 536, duration: 0.253s, episode steps: 113, steps per second: 447, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.434 [0.000, 1.000], mean observation: -0.495 [-2.757, 0.377], loss: 1.398785, mean_absolute_error: 17.468338, mean_q: 37.229939\n",
      " 12326/50000: episode: 537, duration: 0.238s, episode steps: 110, steps per second: 462, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.436 [0.000, 1.000], mean observation: -0.506 [-2.776, 0.336], loss: 0.988148, mean_absolute_error: 17.785322, mean_q: 37.958019\n",
      " 12442/50000: episode: 538, duration: 0.237s, episode steps: 116, steps per second: 489, episode reward: 116.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: -0.483 [-2.733, 0.595], loss: 1.285123, mean_absolute_error: 17.943933, mean_q: 38.235809\n",
      " 12558/50000: episode: 539, duration: 0.245s, episode steps: 116, steps per second: 474, episode reward: 116.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.489 [-2.591, 0.555], loss: 1.640700, mean_absolute_error: 17.876940, mean_q: 38.110352\n",
      " 12677/50000: episode: 540, duration: 0.241s, episode steps: 119, steps per second: 494, episode reward: 119.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.437 [0.000, 1.000], mean observation: -0.467 [-2.771, 0.363], loss: 1.619902, mean_absolute_error: 17.965618, mean_q: 38.305981\n",
      " 12792/50000: episode: 541, duration: 0.245s, episode steps: 115, steps per second: 468, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: -0.478 [-2.705, 0.431], loss: 1.154757, mean_absolute_error: 18.277948, mean_q: 39.161392\n",
      " 12913/50000: episode: 542, duration: 0.262s, episode steps: 121, steps per second: 462, episode reward: 121.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.446 [0.000, 1.000], mean observation: -0.461 [-2.702, 0.623], loss: 1.305808, mean_absolute_error: 18.315359, mean_q: 39.233776\n",
      " 13036/50000: episode: 543, duration: 0.258s, episode steps: 123, steps per second: 477, episode reward: 123.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.450 [-2.774, 0.596], loss: 1.533934, mean_absolute_error: 18.181486, mean_q: 38.966026\n",
      " 13154/50000: episode: 544, duration: 0.251s, episode steps: 118, steps per second: 470, episode reward: 118.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.441 [0.000, 1.000], mean observation: -0.472 [-2.774, 0.393], loss: 1.588751, mean_absolute_error: 18.501278, mean_q: 39.525227\n",
      " 13263/50000: episode: 545, duration: 0.235s, episode steps: 109, steps per second: 463, episode reward: 109.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.431 [0.000, 1.000], mean observation: -0.506 [-2.776, 0.390], loss: 1.212806, mean_absolute_error: 18.409597, mean_q: 39.696838\n",
      " 13372/50000: episode: 546, duration: 0.222s, episode steps: 109, steps per second: 491, episode reward: 109.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.431 [0.000, 1.000], mean observation: -0.501 [-2.746, 0.385], loss: 1.412724, mean_absolute_error: 18.679752, mean_q: 40.065979\n",
      " 13485/50000: episode: 547, duration: 0.237s, episode steps: 113, steps per second: 477, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.434 [0.000, 1.000], mean observation: -0.483 [-2.734, 0.397], loss: 1.492812, mean_absolute_error: 18.750612, mean_q: 40.222252\n",
      " 13599/50000: episode: 548, duration: 0.240s, episode steps: 114, steps per second: 475, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.439 [0.000, 1.000], mean observation: -0.482 [-2.711, 0.426], loss: 1.348135, mean_absolute_error: 19.082390, mean_q: 40.762180\n",
      " 13721/50000: episode: 549, duration: 0.265s, episode steps: 122, steps per second: 460, episode reward: 122.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.434 [0.000, 1.000], mean observation: -0.456 [-2.876, 0.632], loss: 1.172524, mean_absolute_error: 19.002798, mean_q: 40.577709\n",
      " 13848/50000: episode: 550, duration: 0.274s, episode steps: 127, steps per second: 463, episode reward: 127.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.441 [0.000, 1.000], mean observation: -0.439 [-2.891, 0.616], loss: 1.221000, mean_absolute_error: 19.166866, mean_q: 40.989853\n",
      " 13968/50000: episode: 551, duration: 0.253s, episode steps: 120, steps per second: 473, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: -0.457 [-2.749, 0.385], loss: 1.144179, mean_absolute_error: 19.560009, mean_q: 41.877686\n",
      " 14080/50000: episode: 552, duration: 0.246s, episode steps: 112, steps per second: 456, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.490 [-2.734, 0.350], loss: 1.394679, mean_absolute_error: 19.136715, mean_q: 40.628559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 14192/50000: episode: 553, duration: 0.223s, episode steps: 112, steps per second: 502, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: -0.492 [-2.895, 0.418], loss: 1.220190, mean_absolute_error: 19.349148, mean_q: 41.362965\n",
      " 14300/50000: episode: 554, duration: 0.232s, episode steps: 108, steps per second: 465, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: -0.505 [-2.725, 0.407], loss: 1.332970, mean_absolute_error: 19.295675, mean_q: 41.158905\n",
      " 14418/50000: episode: 555, duration: 0.264s, episode steps: 118, steps per second: 446, episode reward: 118.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.463 [-2.544, 0.409], loss: 1.419351, mean_absolute_error: 19.377140, mean_q: 41.242573\n",
      " 14536/50000: episode: 556, duration: 0.249s, episode steps: 118, steps per second: 475, episode reward: 118.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.441 [0.000, 1.000], mean observation: -0.471 [-2.727, 0.404], loss: 1.461540, mean_absolute_error: 19.867046, mean_q: 42.239357\n",
      " 14652/50000: episode: 557, duration: 0.248s, episode steps: 116, steps per second: 468, episode reward: 116.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: -0.470 [-2.750, 0.380], loss: 0.922056, mean_absolute_error: 19.956476, mean_q: 42.563358\n",
      " 14779/50000: episode: 558, duration: 0.254s, episode steps: 127, steps per second: 500, episode reward: 127.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.441 [0.000, 1.000], mean observation: -0.436 [-2.714, 0.417], loss: 1.618542, mean_absolute_error: 20.185234, mean_q: 42.831970\n",
      " 14895/50000: episode: 559, duration: 0.246s, episode steps: 116, steps per second: 471, episode reward: 116.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: -0.471 [-2.693, 0.437], loss: 1.659349, mean_absolute_error: 19.644915, mean_q: 41.902313\n",
      " 15004/50000: episode: 560, duration: 0.213s, episode steps: 109, steps per second: 511, episode reward: 109.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.431 [0.000, 1.000], mean observation: -0.506 [-2.703, 0.296], loss: 1.003623, mean_absolute_error: 20.310204, mean_q: 43.287357\n",
      " 15133/50000: episode: 561, duration: 0.274s, episode steps: 129, steps per second: 471, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: -0.428 [-2.748, 0.578], loss: 1.360313, mean_absolute_error: 20.472281, mean_q: 43.551701\n",
      " 15250/50000: episode: 562, duration: 0.252s, episode steps: 117, steps per second: 464, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.436 [0.000, 1.000], mean observation: -0.470 [-2.760, 0.564], loss: 1.727358, mean_absolute_error: 20.398951, mean_q: 43.223354\n",
      " 15354/50000: episode: 563, duration: 0.197s, episode steps: 104, steps per second: 528, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.433 [0.000, 1.000], mean observation: -0.524 [-2.760, 0.292], loss: 0.979465, mean_absolute_error: 20.452860, mean_q: 43.394356\n",
      " 15460/50000: episode: 564, duration: 0.239s, episode steps: 106, steps per second: 443, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.434 [0.000, 1.000], mean observation: -0.521 [-2.788, 0.365], loss: 1.027379, mean_absolute_error: 20.451239, mean_q: 43.560406\n",
      " 15572/50000: episode: 565, duration: 0.222s, episode steps: 112, steps per second: 505, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.492 [-2.731, 0.397], loss: 0.994744, mean_absolute_error: 20.298508, mean_q: 42.960152\n",
      " 15678/50000: episode: 566, duration: 0.212s, episode steps: 106, steps per second: 501, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.434 [0.000, 1.000], mean observation: -0.518 [-2.765, 0.367], loss: 2.024349, mean_absolute_error: 20.677958, mean_q: 43.766251\n",
      " 15787/50000: episode: 567, duration: 0.245s, episode steps: 109, steps per second: 444, episode reward: 109.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.431 [0.000, 1.000], mean observation: -0.502 [-2.736, 0.328], loss: 1.303717, mean_absolute_error: 20.419138, mean_q: 43.346104\n",
      " 15912/50000: episode: 568, duration: 0.265s, episode steps: 125, steps per second: 472, episode reward: 125.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: -0.444 [-2.753, 0.571], loss: 1.622820, mean_absolute_error: 20.681271, mean_q: 44.056622\n",
      " 16036/50000: episode: 569, duration: 0.274s, episode steps: 124, steps per second: 452, episode reward: 124.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.455 [-2.721, 0.598], loss: 1.511903, mean_absolute_error: 20.614788, mean_q: 43.842411\n",
      " 16155/50000: episode: 570, duration: 0.261s, episode steps: 119, steps per second: 456, episode reward: 119.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.437 [0.000, 1.000], mean observation: -0.462 [-2.730, 0.400], loss: 1.192282, mean_absolute_error: 20.627216, mean_q: 43.945366\n",
      " 16280/50000: episode: 571, duration: 0.283s, episode steps: 125, steps per second: 441, episode reward: 125.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: -0.449 [-2.700, 0.618], loss: 1.027725, mean_absolute_error: 20.737547, mean_q: 44.054478\n",
      " 16393/50000: episode: 572, duration: 0.238s, episode steps: 113, steps per second: 475, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: -0.489 [-2.577, 0.362], loss: 1.197198, mean_absolute_error: 20.434446, mean_q: 43.764648\n",
      " 16514/50000: episode: 573, duration: 0.257s, episode steps: 121, steps per second: 471, episode reward: 121.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.457 [-2.741, 0.578], loss: 1.201826, mean_absolute_error: 20.942577, mean_q: 44.627430\n",
      " 16626/50000: episode: 574, duration: 0.230s, episode steps: 112, steps per second: 487, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.501 [-2.743, 0.381], loss: 0.955046, mean_absolute_error: 21.324280, mean_q: 45.259907\n",
      " 16746/50000: episode: 575, duration: 0.269s, episode steps: 120, steps per second: 447, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: -0.463 [-2.777, 0.544], loss: 1.546306, mean_absolute_error: 21.213692, mean_q: 45.006180\n",
      " 16874/50000: episode: 576, duration: 0.263s, episode steps: 128, steps per second: 488, episode reward: 128.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.445 [0.000, 1.000], mean observation: -0.438 [-2.756, 0.368], loss: 1.533394, mean_absolute_error: 20.856491, mean_q: 44.215393\n",
      " 16986/50000: episode: 577, duration: 0.232s, episode steps: 112, steps per second: 483, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.489 [-2.701, 0.426], loss: 0.762077, mean_absolute_error: 20.974979, mean_q: 44.672626\n",
      " 17101/50000: episode: 578, duration: 0.251s, episode steps: 115, steps per second: 459, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: -0.483 [-2.739, 0.383], loss: 1.188734, mean_absolute_error: 21.477917, mean_q: 45.586201\n",
      " 17209/50000: episode: 579, duration: 0.232s, episode steps: 108, steps per second: 466, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: -0.513 [-2.773, 0.314], loss: 0.798741, mean_absolute_error: 21.136715, mean_q: 44.923290\n",
      " 17326/50000: episode: 580, duration: 0.246s, episode steps: 117, steps per second: 475, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.436 [0.000, 1.000], mean observation: -0.477 [-2.692, 0.433], loss: 1.322290, mean_absolute_error: 20.906944, mean_q: 44.338573\n",
      " 17451/50000: episode: 581, duration: 0.246s, episode steps: 125, steps per second: 508, episode reward: 125.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: -0.449 [-2.713, 0.608], loss: 1.155950, mean_absolute_error: 21.185360, mean_q: 45.113682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 17573/50000: episode: 582, duration: 0.289s, episode steps: 122, steps per second: 422, episode reward: 122.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.443 [0.000, 1.000], mean observation: -0.459 [-2.714, 0.605], loss: 0.681188, mean_absolute_error: 21.327692, mean_q: 45.482388\n",
      " 17701/50000: episode: 583, duration: 0.291s, episode steps: 128, steps per second: 439, episode reward: 128.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.442 [-2.887, 0.425], loss: 1.323289, mean_absolute_error: 20.884495, mean_q: 44.395004\n",
      " 17830/50000: episode: 584, duration: 0.265s, episode steps: 129, steps per second: 487, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: -0.430 [-2.767, 0.555], loss: 1.106584, mean_absolute_error: 21.451406, mean_q: 45.618847\n",
      " 17943/50000: episode: 585, duration: 0.225s, episode steps: 113, steps per second: 501, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.434 [0.000, 1.000], mean observation: -0.497 [-2.901, 0.344], loss: 1.289512, mean_absolute_error: 21.295097, mean_q: 45.237598\n",
      " 18069/50000: episode: 586, duration: 0.255s, episode steps: 126, steps per second: 494, episode reward: 126.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.441 [-2.732, 0.594], loss: 1.023977, mean_absolute_error: 21.169180, mean_q: 44.971119\n",
      " 18177/50000: episode: 587, duration: 0.253s, episode steps: 108, steps per second: 426, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: -0.507 [-2.768, 0.308], loss: 0.747851, mean_absolute_error: 21.462894, mean_q: 45.574482\n",
      " 18288/50000: episode: 588, duration: 0.238s, episode steps: 111, steps per second: 467, episode reward: 111.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.432 [0.000, 1.000], mean observation: -0.491 [-2.736, 0.391], loss: 1.077306, mean_absolute_error: 21.200945, mean_q: 44.852859\n",
      " 18404/50000: episode: 589, duration: 0.248s, episode steps: 116, steps per second: 468, episode reward: 116.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: -0.475 [-2.560, 0.275], loss: 0.586723, mean_absolute_error: 21.570576, mean_q: 45.905277\n",
      " 18533/50000: episode: 590, duration: 0.265s, episode steps: 129, steps per second: 487, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: -0.433 [-2.757, 0.368], loss: 0.841123, mean_absolute_error: 21.911488, mean_q: 46.320930\n",
      " 18647/50000: episode: 591, duration: 0.235s, episode steps: 114, steps per second: 486, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.439 [0.000, 1.000], mean observation: -0.485 [-2.700, 0.422], loss: 0.689583, mean_absolute_error: 21.911901, mean_q: 46.364220\n",
      " 18764/50000: episode: 592, duration: 0.231s, episode steps: 117, steps per second: 506, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.436 [0.000, 1.000], mean observation: -0.476 [-2.689, 0.430], loss: 1.128185, mean_absolute_error: 21.773376, mean_q: 45.979282\n",
      " 18883/50000: episode: 593, duration: 0.264s, episode steps: 119, steps per second: 451, episode reward: 119.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.437 [0.000, 1.000], mean observation: -0.472 [-2.874, 0.354], loss: 0.737269, mean_absolute_error: 21.730637, mean_q: 45.885593\n",
      " 19007/50000: episode: 594, duration: 0.283s, episode steps: 124, steps per second: 438, episode reward: 124.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.449 [-2.588, 0.356], loss: 0.792511, mean_absolute_error: 21.565130, mean_q: 45.384651\n",
      " 19130/50000: episode: 595, duration: 0.254s, episode steps: 123, steps per second: 483, episode reward: 123.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.439 [0.000, 1.000], mean observation: -0.458 [-2.782, 0.537], loss: 1.184199, mean_absolute_error: 21.823719, mean_q: 45.966854\n",
      " 19247/50000: episode: 596, duration: 0.232s, episode steps: 117, steps per second: 504, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.436 [0.000, 1.000], mean observation: -0.472 [-2.760, 0.359], loss: 1.015538, mean_absolute_error: 21.207195, mean_q: 44.776997\n",
      " 19369/50000: episode: 597, duration: 0.252s, episode steps: 122, steps per second: 483, episode reward: 122.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.443 [0.000, 1.000], mean observation: -0.458 [-2.590, 0.358], loss: 1.222085, mean_absolute_error: 21.505848, mean_q: 45.580448\n",
      " 19476/50000: episode: 598, duration: 0.239s, episode steps: 107, steps per second: 447, episode reward: 107.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.430 [0.000, 1.000], mean observation: -0.515 [-2.742, 0.337], loss: 0.814928, mean_absolute_error: 21.546387, mean_q: 45.609722\n",
      " 19599/50000: episode: 599, duration: 0.261s, episode steps: 123, steps per second: 472, episode reward: 123.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.453 [-2.585, 0.367], loss: 0.902134, mean_absolute_error: 21.584343, mean_q: 45.717102\n",
      " 19719/50000: episode: 600, duration: 0.269s, episode steps: 120, steps per second: 446, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: -0.468 [-2.727, 0.312], loss: 0.883417, mean_absolute_error: 21.729668, mean_q: 46.072655\n",
      " 19821/50000: episode: 601, duration: 0.194s, episode steps: 102, steps per second: 525, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.431 [0.000, 1.000], mean observation: -0.539 [-2.779, 0.379], loss: 1.095936, mean_absolute_error: 21.241474, mean_q: 45.075340\n",
      " 19944/50000: episode: 602, duration: 0.292s, episode steps: 123, steps per second: 421, episode reward: 123.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.439 [0.000, 1.000], mean observation: -0.445 [-2.724, 0.407], loss: 0.902197, mean_absolute_error: 21.859629, mean_q: 46.234024\n",
      " 20072/50000: episode: 603, duration: 0.270s, episode steps: 128, steps per second: 473, episode reward: 128.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.445 [0.000, 1.000], mean observation: -0.435 [-2.706, 0.428], loss: 0.544827, mean_absolute_error: 21.613905, mean_q: 45.760132\n",
      " 20182/50000: episode: 604, duration: 0.225s, episode steps: 110, steps per second: 490, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.436 [0.000, 1.000], mean observation: -0.490 [-2.697, 0.383], loss: 0.842396, mean_absolute_error: 21.270008, mean_q: 44.951389\n",
      " 20308/50000: episode: 605, duration: 0.273s, episode steps: 126, steps per second: 462, episode reward: 126.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.448 [-2.765, 0.366], loss: 0.968962, mean_absolute_error: 21.829720, mean_q: 46.098774\n",
      " 20426/50000: episode: 606, duration: 0.259s, episode steps: 118, steps per second: 455, episode reward: 118.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.432 [0.000, 1.000], mean observation: -0.474 [-2.956, 0.396], loss: 1.209291, mean_absolute_error: 21.761248, mean_q: 45.775524\n",
      " 20536/50000: episode: 607, duration: 0.247s, episode steps: 110, steps per second: 446, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.436 [0.000, 1.000], mean observation: -0.491 [-2.751, 0.408], loss: 1.037349, mean_absolute_error: 21.903593, mean_q: 46.203156\n",
      " 20662/50000: episode: 608, duration: 0.268s, episode steps: 126, steps per second: 470, episode reward: 126.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.437 [0.000, 1.000], mean observation: -0.437 [-2.887, 0.619], loss: 1.315810, mean_absolute_error: 21.808535, mean_q: 45.949455\n",
      " 20772/50000: episode: 609, duration: 0.206s, episode steps: 110, steps per second: 534, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.427 [0.000, 1.000], mean observation: -0.507 [-2.912, 0.390], loss: 0.757317, mean_absolute_error: 21.347443, mean_q: 44.851524\n",
      " 20884/50000: episode: 610, duration: 0.264s, episode steps: 112, steps per second: 425, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.489 [-2.765, 0.394], loss: 0.941648, mean_absolute_error: 21.565275, mean_q: 45.322502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 21001/50000: episode: 611, duration: 0.257s, episode steps: 117, steps per second: 456, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.436 [0.000, 1.000], mean observation: -0.479 [-2.782, 0.405], loss: 1.179300, mean_absolute_error: 21.857342, mean_q: 45.874111\n",
      " 21124/50000: episode: 612, duration: 0.243s, episode steps: 123, steps per second: 507, episode reward: 123.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.439 [0.000, 1.000], mean observation: -0.452 [-2.725, 0.364], loss: 1.635453, mean_absolute_error: 21.608488, mean_q: 45.350998\n",
      " 21241/50000: episode: 613, duration: 0.244s, episode steps: 117, steps per second: 479, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.436 [0.000, 1.000], mean observation: -0.482 [-2.767, 0.358], loss: 0.709570, mean_absolute_error: 21.721283, mean_q: 45.763447\n",
      " 21355/50000: episode: 614, duration: 0.231s, episode steps: 114, steps per second: 493, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.488 [-2.712, 0.378], loss: 0.650917, mean_absolute_error: 21.249710, mean_q: 44.868130\n",
      " 21464/50000: episode: 615, duration: 0.228s, episode steps: 109, steps per second: 478, episode reward: 109.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.431 [0.000, 1.000], mean observation: -0.504 [-2.710, 0.420], loss: 0.640900, mean_absolute_error: 21.491419, mean_q: 45.271790\n",
      " 21572/50000: episode: 616, duration: 0.215s, episode steps: 108, steps per second: 503, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: -0.516 [-2.598, 0.379], loss: 0.713210, mean_absolute_error: 21.484861, mean_q: 45.394733\n",
      " 21676/50000: episode: 617, duration: 0.197s, episode steps: 104, steps per second: 529, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.433 [0.000, 1.000], mean observation: -0.523 [-2.779, 0.361], loss: 0.824924, mean_absolute_error: 21.314686, mean_q: 45.207859\n",
      " 21789/50000: episode: 618, duration: 0.229s, episode steps: 113, steps per second: 494, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.434 [0.000, 1.000], mean observation: -0.491 [-2.740, 0.392], loss: 0.548035, mean_absolute_error: 21.391455, mean_q: 45.403027\n",
      " 21896/50000: episode: 619, duration: 0.212s, episode steps: 107, steps per second: 505, episode reward: 107.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.430 [0.000, 1.000], mean observation: -0.510 [-2.737, 0.408], loss: 0.979800, mean_absolute_error: 21.307766, mean_q: 44.948517\n",
      " 22001/50000: episode: 620, duration: 0.225s, episode steps: 105, steps per second: 466, episode reward: 105.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: -0.515 [-2.788, 0.366], loss: 0.756143, mean_absolute_error: 20.973433, mean_q: 44.368382\n",
      " 22115/50000: episode: 621, duration: 0.231s, episode steps: 114, steps per second: 495, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.439 [0.000, 1.000], mean observation: -0.482 [-2.742, 0.390], loss: 1.098291, mean_absolute_error: 21.040644, mean_q: 44.426556\n",
      " 22237/50000: episode: 622, duration: 0.306s, episode steps: 122, steps per second: 398, episode reward: 122.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.443 [0.000, 1.000], mean observation: -0.452 [-2.757, 0.566], loss: 0.616260, mean_absolute_error: 21.143147, mean_q: 44.779236\n",
      " 22352/50000: episode: 623, duration: 0.247s, episode steps: 115, steps per second: 466, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: -0.484 [-2.760, 0.389], loss: 0.699711, mean_absolute_error: 21.597870, mean_q: 45.582966\n",
      " 22466/50000: episode: 624, duration: 0.237s, episode steps: 114, steps per second: 481, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.430 [0.000, 1.000], mean observation: -0.481 [-2.972, 0.397], loss: 0.809519, mean_absolute_error: 21.549452, mean_q: 45.595753\n",
      " 22587/50000: episode: 625, duration: 0.256s, episode steps: 121, steps per second: 472, episode reward: 121.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.460 [-2.784, 0.537], loss: 1.009611, mean_absolute_error: 21.415468, mean_q: 44.937271\n",
      " 22701/50000: episode: 626, duration: 0.256s, episode steps: 114, steps per second: 445, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.439 [0.000, 1.000], mean observation: -0.480 [-2.738, 0.425], loss: 1.178009, mean_absolute_error: 21.430101, mean_q: 45.054317\n",
      " 22819/50000: episode: 627, duration: 0.264s, episode steps: 118, steps per second: 448, episode reward: 118.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.424 [0.000, 1.000], mean observation: -0.473 [-3.357, 0.950], loss: 0.758503, mean_absolute_error: 21.291851, mean_q: 44.805382\n",
      " 22927/50000: episode: 628, duration: 0.247s, episode steps: 108, steps per second: 437, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: -0.503 [-2.771, 0.419], loss: 1.090460, mean_absolute_error: 21.247032, mean_q: 44.781185\n",
      " 23054/50000: episode: 629, duration: 0.268s, episode steps: 127, steps per second: 474, episode reward: 127.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.433 [0.000, 1.000], mean observation: -0.447 [-3.157, 0.608], loss: 0.837414, mean_absolute_error: 21.163359, mean_q: 44.830620\n",
      " 23163/50000: episode: 630, duration: 0.234s, episode steps: 109, steps per second: 465, episode reward: 109.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: -0.508 [-2.763, 0.396], loss: 0.923314, mean_absolute_error: 21.187920, mean_q: 44.795971\n",
      " 23286/50000: episode: 631, duration: 0.246s, episode steps: 123, steps per second: 499, episode reward: 123.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.439 [0.000, 1.000], mean observation: -0.453 [-2.730, 0.399], loss: 0.783195, mean_absolute_error: 20.954142, mean_q: 44.345982\n",
      " 23407/50000: episode: 632, duration: 0.277s, episode steps: 121, steps per second: 437, episode reward: 121.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.458 [-2.717, 0.412], loss: 0.936136, mean_absolute_error: 21.071213, mean_q: 44.641850\n",
      " 23514/50000: episode: 633, duration: 0.226s, episode steps: 107, steps per second: 473, episode reward: 107.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.430 [0.000, 1.000], mean observation: -0.517 [-2.769, 0.293], loss: 0.913550, mean_absolute_error: 21.072685, mean_q: 44.589523\n",
      " 23646/50000: episode: 634, duration: 0.292s, episode steps: 132, steps per second: 452, episode reward: 132.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.439 [0.000, 1.000], mean observation: -0.417 [-2.907, 0.406], loss: 1.003240, mean_absolute_error: 20.955122, mean_q: 44.288723\n",
      " 23765/50000: episode: 635, duration: 0.239s, episode steps: 119, steps per second: 498, episode reward: 119.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.437 [0.000, 1.000], mean observation: -0.471 [-2.897, 0.346], loss: 0.685976, mean_absolute_error: 20.735277, mean_q: 43.941414\n",
      " 23879/50000: episode: 636, duration: 0.242s, episode steps: 114, steps per second: 471, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.439 [0.000, 1.000], mean observation: -0.493 [-2.771, 0.354], loss: 1.022922, mean_absolute_error: 21.179426, mean_q: 44.844227\n",
      " 23997/50000: episode: 637, duration: 0.258s, episode steps: 118, steps per second: 458, episode reward: 118.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.441 [0.000, 1.000], mean observation: -0.474 [-2.739, 0.297], loss: 0.744151, mean_absolute_error: 21.114614, mean_q: 44.870907\n",
      " 24122/50000: episode: 638, duration: 0.250s, episode steps: 125, steps per second: 501, episode reward: 125.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: -0.441 [-2.704, 0.430], loss: 0.706563, mean_absolute_error: 21.252460, mean_q: 45.162510\n",
      " 24258/50000: episode: 639, duration: 0.277s, episode steps: 136, steps per second: 491, episode reward: 136.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.441 [0.000, 1.000], mean observation: -0.409 [-2.913, 0.401], loss: 1.023145, mean_absolute_error: 21.162085, mean_q: 44.952961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 24371/50000: episode: 640, duration: 0.224s, episode steps: 113, steps per second: 505, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: -0.487 [-2.687, 0.310], loss: 0.485596, mean_absolute_error: 21.248356, mean_q: 45.206360\n",
      " 24498/50000: episode: 641, duration: 0.292s, episode steps: 127, steps per second: 435, episode reward: 127.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.449 [0.000, 1.000], mean observation: -0.436 [-2.722, 0.413], loss: 0.813776, mean_absolute_error: 21.026432, mean_q: 44.606052\n",
      " 24619/50000: episode: 642, duration: 0.275s, episode steps: 121, steps per second: 440, episode reward: 121.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.461 [-2.746, 0.374], loss: 0.528965, mean_absolute_error: 21.074703, mean_q: 44.624710\n",
      " 24743/50000: episode: 643, duration: 0.273s, episode steps: 124, steps per second: 454, episode reward: 124.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: -0.448 [-2.879, 0.439], loss: 0.972979, mean_absolute_error: 21.078049, mean_q: 44.619339\n",
      " 24855/50000: episode: 644, duration: 0.221s, episode steps: 112, steps per second: 507, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.501 [-2.728, 0.370], loss: 0.912945, mean_absolute_error: 20.922964, mean_q: 44.433647\n",
      " 24982/50000: episode: 645, duration: 0.262s, episode steps: 127, steps per second: 484, episode reward: 127.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.444 [-2.743, 0.587], loss: 0.913733, mean_absolute_error: 21.037519, mean_q: 44.594925\n",
      " 25114/50000: episode: 646, duration: 0.265s, episode steps: 132, steps per second: 498, episode reward: 132.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.422 [-2.590, 0.348], loss: 0.443003, mean_absolute_error: 20.875217, mean_q: 44.402298\n",
      " 25242/50000: episode: 647, duration: 0.275s, episode steps: 128, steps per second: 465, episode reward: 128.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.445 [0.000, 1.000], mean observation: -0.434 [-2.883, 0.431], loss: 0.759425, mean_absolute_error: 20.904762, mean_q: 44.359783\n",
      " 25359/50000: episode: 648, duration: 0.233s, episode steps: 117, steps per second: 502, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.436 [0.000, 1.000], mean observation: -0.473 [-2.767, 0.367], loss: 0.568952, mean_absolute_error: 20.682909, mean_q: 43.837929\n",
      " 25485/50000: episode: 649, duration: 0.283s, episode steps: 126, steps per second: 445, episode reward: 126.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.439 [-2.706, 0.426], loss: 0.623982, mean_absolute_error: 20.827484, mean_q: 44.388168\n",
      " 25604/50000: episode: 650, duration: 0.257s, episode steps: 119, steps per second: 464, episode reward: 119.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.437 [0.000, 1.000], mean observation: -0.468 [-2.766, 0.369], loss: 0.813897, mean_absolute_error: 20.822975, mean_q: 44.154655\n",
      " 25717/50000: episode: 651, duration: 0.227s, episode steps: 113, steps per second: 498, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.434 [0.000, 1.000], mean observation: -0.487 [-2.755, 0.343], loss: 0.578345, mean_absolute_error: 20.593781, mean_q: 43.627193\n",
      " 25846/50000: episode: 652, duration: 0.272s, episode steps: 129, steps per second: 474, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: -0.439 [-2.770, 0.355], loss: 0.783652, mean_absolute_error: 20.854919, mean_q: 44.244293\n",
      " 25954/50000: episode: 653, duration: 0.230s, episode steps: 108, steps per second: 469, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: -0.508 [-2.706, 0.406], loss: 0.565551, mean_absolute_error: 20.316723, mean_q: 43.058857\n",
      " 26081/50000: episode: 654, duration: 0.275s, episode steps: 127, steps per second: 462, episode reward: 127.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.441 [0.000, 1.000], mean observation: -0.435 [-2.742, 0.441], loss: 0.870531, mean_absolute_error: 20.947023, mean_q: 44.402859\n",
      " 26218/50000: episode: 655, duration: 0.274s, episode steps: 137, steps per second: 500, episode reward: 137.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.445 [0.000, 1.000], mean observation: -0.401 [-2.740, 0.512], loss: 0.761365, mean_absolute_error: 20.843220, mean_q: 44.006607\n",
      " 26326/50000: episode: 656, duration: 0.208s, episode steps: 108, steps per second: 520, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: -0.505 [-2.883, 0.389], loss: 0.631799, mean_absolute_error: 21.009144, mean_q: 44.363804\n",
      " 26460/50000: episode: 657, duration: 0.272s, episode steps: 134, steps per second: 493, episode reward: 134.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: -0.415 [-2.710, 0.610], loss: 0.637496, mean_absolute_error: 21.052719, mean_q: 44.337948\n",
      " 26567/50000: episode: 658, duration: 0.227s, episode steps: 107, steps per second: 471, episode reward: 107.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.430 [0.000, 1.000], mean observation: -0.511 [-2.900, 0.403], loss: 0.865248, mean_absolute_error: 21.152985, mean_q: 44.553959\n",
      " 26682/50000: episode: 659, duration: 0.254s, episode steps: 115, steps per second: 452, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: -0.484 [-2.919, 0.340], loss: 0.436174, mean_absolute_error: 21.157276, mean_q: 44.408009\n",
      " 26803/50000: episode: 660, duration: 0.238s, episode steps: 121, steps per second: 508, episode reward: 121.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.446 [0.000, 1.000], mean observation: -0.462 [-2.749, 0.388], loss: 0.676700, mean_absolute_error: 21.099745, mean_q: 44.317604\n",
      " 26914/50000: episode: 661, duration: 0.232s, episode steps: 111, steps per second: 479, episode reward: 111.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.432 [0.000, 1.000], mean observation: -0.499 [-2.732, 0.504], loss: 0.643153, mean_absolute_error: 20.749252, mean_q: 43.530846\n",
      " 27042/50000: episode: 662, duration: 0.261s, episode steps: 128, steps per second: 490, episode reward: 128.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.445 [0.000, 1.000], mean observation: -0.435 [-2.747, 0.417], loss: 0.713293, mean_absolute_error: 20.864420, mean_q: 43.896461\n",
      " 27162/50000: episode: 663, duration: 0.246s, episode steps: 120, steps per second: 489, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: -0.468 [-2.739, 0.463], loss: 1.099805, mean_absolute_error: 20.847368, mean_q: 44.020931\n",
      " 27282/50000: episode: 664, duration: 0.258s, episode steps: 120, steps per second: 466, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: -0.466 [-2.976, 0.465], loss: 0.861103, mean_absolute_error: 20.964998, mean_q: 44.187565\n",
      " 27384/50000: episode: 665, duration: 0.193s, episode steps: 102, steps per second: 529, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.431 [0.000, 1.000], mean observation: -0.534 [-2.798, 0.469], loss: 0.399223, mean_absolute_error: 20.949625, mean_q: 44.045643\n",
      " 27491/50000: episode: 666, duration: 0.227s, episode steps: 107, steps per second: 472, episode reward: 107.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.439 [0.000, 1.000], mean observation: -0.516 [-2.752, 0.340], loss: 0.491449, mean_absolute_error: 20.823700, mean_q: 43.801773\n",
      " 27625/50000: episode: 667, duration: 0.297s, episode steps: 134, steps per second: 452, episode reward: 134.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: -0.409 [-2.936, 0.581], loss: 0.561352, mean_absolute_error: 20.833923, mean_q: 43.914238\n",
      " 27748/50000: episode: 668, duration: 0.245s, episode steps: 123, steps per second: 502, episode reward: 123.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.431 [0.000, 1.000], mean observation: -0.449 [-3.078, 0.556], loss: 0.356311, mean_absolute_error: 20.761318, mean_q: 43.656273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 27878/50000: episode: 669, duration: 0.267s, episode steps: 130, steps per second: 488, episode reward: 130.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.432 [-2.879, 0.486], loss: 0.483286, mean_absolute_error: 21.027750, mean_q: 44.078281\n",
      " 27979/50000: episode: 670, duration: 0.199s, episode steps: 101, steps per second: 507, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.490 [-2.797, 0.537], loss: 0.845321, mean_absolute_error: 20.539057, mean_q: 43.174641\n",
      " 28092/50000: episode: 671, duration: 0.225s, episode steps: 113, steps per second: 503, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.451 [0.000, 1.000], mean observation: -0.498 [-2.949, 0.566], loss: 0.302139, mean_absolute_error: 20.737209, mean_q: 43.775749\n",
      " 28212/50000: episode: 672, duration: 0.246s, episode steps: 120, steps per second: 488, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: -0.472 [-2.789, 0.399], loss: 1.279612, mean_absolute_error: 20.267908, mean_q: 42.671452\n",
      " 28325/50000: episode: 673, duration: 0.223s, episode steps: 113, steps per second: 507, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.451 [0.000, 1.000], mean observation: -0.486 [-2.533, 0.388], loss: 0.791327, mean_absolute_error: 20.428940, mean_q: 43.160122\n",
      " 28440/50000: episode: 674, duration: 0.236s, episode steps: 115, steps per second: 487, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.426 [0.000, 1.000], mean observation: -0.480 [-3.174, 0.747], loss: 0.690157, mean_absolute_error: 20.883886, mean_q: 44.120014\n",
      " 28559/50000: episode: 675, duration: 0.238s, episode steps: 119, steps per second: 500, episode reward: 119.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.437 [0.000, 1.000], mean observation: -0.469 [-2.941, 0.547], loss: 0.655297, mean_absolute_error: 20.668938, mean_q: 43.543449\n",
      " 28674/50000: episode: 676, duration: 0.239s, episode steps: 115, steps per second: 482, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.426 [0.000, 1.000], mean observation: -0.484 [-3.153, 0.622], loss: 0.520233, mean_absolute_error: 20.494352, mean_q: 43.335419\n",
      " 28798/50000: episode: 677, duration: 0.256s, episode steps: 124, steps per second: 484, episode reward: 124.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.461 [-2.777, 0.518], loss: 0.828065, mean_absolute_error: 20.866907, mean_q: 43.887447\n",
      " 28920/50000: episode: 678, duration: 0.257s, episode steps: 122, steps per second: 474, episode reward: 122.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.434 [0.000, 1.000], mean observation: -0.462 [-2.923, 0.558], loss: 0.362630, mean_absolute_error: 20.492720, mean_q: 43.108242\n",
      " 29045/50000: episode: 679, duration: 0.269s, episode steps: 125, steps per second: 465, episode reward: 125.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.454 [-2.910, 0.517], loss: 0.437982, mean_absolute_error: 20.812515, mean_q: 43.802570\n",
      " 29175/50000: episode: 680, duration: 0.273s, episode steps: 130, steps per second: 477, episode reward: 130.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.408 [-2.727, 0.572], loss: 0.591885, mean_absolute_error: 20.440128, mean_q: 43.065849\n",
      " 29301/50000: episode: 681, duration: 0.275s, episode steps: 126, steps per second: 457, episode reward: 126.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.445 [-2.935, 0.634], loss: 0.415778, mean_absolute_error: 20.736134, mean_q: 43.498608\n",
      " 29440/50000: episode: 682, duration: 0.309s, episode steps: 139, steps per second: 450, episode reward: 139.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.395 [-2.885, 0.537], loss: 0.416972, mean_absolute_error: 20.802555, mean_q: 43.620399\n",
      " 29569/50000: episode: 683, duration: 0.299s, episode steps: 129, steps per second: 432, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.418 [-2.752, 0.545], loss: 0.755164, mean_absolute_error: 20.546776, mean_q: 43.093254\n",
      " 29700/50000: episode: 684, duration: 0.259s, episode steps: 131, steps per second: 506, episode reward: 131.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.423 [-2.928, 0.582], loss: 0.400058, mean_absolute_error: 20.773251, mean_q: 43.562622\n",
      " 29807/50000: episode: 685, duration: 0.227s, episode steps: 107, steps per second: 472, episode reward: 107.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.411 [0.000, 1.000], mean observation: -0.513 [-3.541, 1.139], loss: 0.762722, mean_absolute_error: 20.748940, mean_q: 43.417370\n",
      " 29943/50000: episode: 686, duration: 0.303s, episode steps: 136, steps per second: 449, episode reward: 136.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.419 [-2.737, 0.583], loss: 0.662937, mean_absolute_error: 21.054680, mean_q: 44.188751\n",
      " 30062/50000: episode: 687, duration: 0.262s, episode steps: 119, steps per second: 455, episode reward: 119.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.470 [-2.708, 0.579], loss: 0.603771, mean_absolute_error: 20.833691, mean_q: 43.633636\n",
      " 30188/50000: episode: 688, duration: 0.282s, episode steps: 126, steps per second: 446, episode reward: 126.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.450 [-2.991, 0.706], loss: 0.527583, mean_absolute_error: 20.821598, mean_q: 43.744858\n",
      " 30302/50000: episode: 689, duration: 0.240s, episode steps: 114, steps per second: 474, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.436 [-2.618, 0.551], loss: 0.763836, mean_absolute_error: 20.394279, mean_q: 42.944618\n",
      " 30410/50000: episode: 690, duration: 0.221s, episode steps: 108, steps per second: 489, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.454 [0.000, 1.000], mean observation: -0.462 [-2.576, 0.462], loss: 0.293096, mean_absolute_error: 20.818230, mean_q: 43.875584\n",
      " 30526/50000: episode: 691, duration: 0.241s, episode steps: 116, steps per second: 481, episode reward: 116.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: -0.466 [-2.741, 0.514], loss: 0.468295, mean_absolute_error: 20.473425, mean_q: 43.233707\n",
      " 30629/50000: episode: 692, duration: 0.196s, episode steps: 103, steps per second: 526, episode reward: 103.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.437 [0.000, 1.000], mean observation: -0.458 [-2.419, 0.452], loss: 1.033199, mean_absolute_error: 20.787716, mean_q: 43.777988\n",
      " 30737/50000: episode: 693, duration: 0.231s, episode steps: 108, steps per second: 467, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.508 [-2.792, 0.531], loss: 0.539518, mean_absolute_error: 20.514137, mean_q: 43.577847\n",
      " 30853/50000: episode: 694, duration: 0.238s, episode steps: 116, steps per second: 488, episode reward: 116.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: -0.459 [-2.728, 0.386], loss: 0.345132, mean_absolute_error: 20.501987, mean_q: 43.194717\n",
      " 30984/50000: episode: 695, duration: 0.275s, episode steps: 131, steps per second: 476, episode reward: 131.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.434 [-2.923, 0.488], loss: 0.611404, mean_absolute_error: 20.609032, mean_q: 43.401524\n",
      " 31117/50000: episode: 696, duration: 0.281s, episode steps: 133, steps per second: 473, episode reward: 133.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.416 [-2.801, 0.612], loss: 0.381791, mean_absolute_error: 20.621914, mean_q: 43.325771\n",
      " 31225/50000: episode: 697, duration: 0.208s, episode steps: 108, steps per second: 518, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: -0.509 [-3.323, 0.931], loss: 0.658555, mean_absolute_error: 20.553780, mean_q: 43.023022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 31348/50000: episode: 698, duration: 0.272s, episode steps: 123, steps per second: 452, episode reward: 123.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.442 [-2.732, 0.614], loss: 0.320656, mean_absolute_error: 20.719416, mean_q: 43.426407\n",
      " 31457/50000: episode: 699, duration: 0.233s, episode steps: 109, steps per second: 467, episode reward: 109.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.500 [-2.773, 0.535], loss: 0.700437, mean_absolute_error: 20.595369, mean_q: 43.134693\n",
      " 31587/50000: episode: 700, duration: 0.268s, episode steps: 130, steps per second: 485, episode reward: 130.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.431 [0.000, 1.000], mean observation: -0.436 [-3.278, 0.772], loss: 0.683236, mean_absolute_error: 20.552193, mean_q: 43.009995\n",
      " 31713/50000: episode: 701, duration: 0.281s, episode steps: 126, steps per second: 448, episode reward: 126.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.440 [-2.760, 0.643], loss: 0.607244, mean_absolute_error: 20.778421, mean_q: 43.429043\n",
      " 31829/50000: episode: 702, duration: 0.249s, episode steps: 116, steps per second: 465, episode reward: 116.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: -0.364 [-2.176, 0.442], loss: 0.636179, mean_absolute_error: 20.628012, mean_q: 43.076424\n",
      " 31967/50000: episode: 703, duration: 0.301s, episode steps: 138, steps per second: 458, episode reward: 138.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.388 [-2.724, 0.633], loss: 0.534184, mean_absolute_error: 20.857496, mean_q: 43.617950\n",
      " 32097/50000: episode: 704, duration: 0.259s, episode steps: 130, steps per second: 503, episode reward: 130.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.454 [0.000, 1.000], mean observation: -0.433 [-3.090, 0.672], loss: 0.510531, mean_absolute_error: 20.719933, mean_q: 43.347641\n",
      " 32195/50000: episode: 705, duration: 0.207s, episode steps: 98, steps per second: 472, episode reward: 98.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.439 [0.000, 1.000], mean observation: -0.490 [-2.418, 0.467], loss: 0.468593, mean_absolute_error: 20.623671, mean_q: 43.069138\n",
      " 32307/50000: episode: 706, duration: 0.265s, episode steps: 112, steps per second: 423, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.446 [0.000, 1.000], mean observation: -0.481 [-2.609, 0.503], loss: 0.725093, mean_absolute_error: 20.631174, mean_q: 43.280022\n",
      " 32420/50000: episode: 707, duration: 0.229s, episode steps: 113, steps per second: 494, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.451 [0.000, 1.000], mean observation: -0.359 [-2.231, 0.504], loss: 0.286348, mean_absolute_error: 20.975428, mean_q: 43.761681\n",
      " 32547/50000: episode: 708, duration: 0.282s, episode steps: 127, steps per second: 450, episode reward: 127.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.425 [0.000, 1.000], mean observation: -0.440 [-3.508, 1.230], loss: 0.612188, mean_absolute_error: 20.628630, mean_q: 43.256771\n",
      " 32667/50000: episode: 709, duration: 0.258s, episode steps: 120, steps per second: 466, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.470 [-2.933, 0.713], loss: 0.621029, mean_absolute_error: 20.806755, mean_q: 43.464176\n",
      " 32797/50000: episode: 710, duration: 0.279s, episode steps: 130, steps per second: 465, episode reward: 130.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.431 [0.000, 1.000], mean observation: -0.439 [-3.350, 0.884], loss: 0.763274, mean_absolute_error: 20.658087, mean_q: 43.225769\n",
      " 32918/50000: episode: 711, duration: 0.241s, episode steps: 121, steps per second: 502, episode reward: 121.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.446 [0.000, 1.000], mean observation: -0.405 [-2.575, 0.541], loss: 0.351154, mean_absolute_error: 20.619608, mean_q: 42.964249\n",
      " 33038/50000: episode: 712, duration: 0.277s, episode steps: 120, steps per second: 434, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.425 [0.000, 1.000], mean observation: -0.462 [-3.288, 0.842], loss: 0.218800, mean_absolute_error: 20.863638, mean_q: 43.779160\n",
      " 33171/50000: episode: 713, duration: 0.276s, episode steps: 133, steps per second: 482, episode reward: 133.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: -0.410 [-3.484, 1.217], loss: 0.340320, mean_absolute_error: 20.962652, mean_q: 43.687458\n",
      " 33288/50000: episode: 714, duration: 0.247s, episode steps: 117, steps per second: 474, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.422 [-2.580, 0.509], loss: 0.495961, mean_absolute_error: 21.149340, mean_q: 44.083187\n",
      " 33407/50000: episode: 715, duration: 0.242s, episode steps: 119, steps per second: 491, episode reward: 119.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.420 [0.000, 1.000], mean observation: -0.473 [-3.489, 1.276], loss: 0.853361, mean_absolute_error: 21.062458, mean_q: 43.863266\n",
      " 33514/50000: episode: 716, duration: 0.203s, episode steps: 107, steps per second: 528, episode reward: 107.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.402 [0.000, 1.000], mean observation: -0.493 [-3.945, 2.057], loss: 0.613306, mean_absolute_error: 20.896915, mean_q: 43.604298\n",
      " 33639/50000: episode: 717, duration: 0.279s, episode steps: 125, steps per second: 448, episode reward: 125.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.424 [0.000, 1.000], mean observation: -0.444 [-3.564, 1.315], loss: 0.707264, mean_absolute_error: 21.025078, mean_q: 43.861954\n",
      " 33762/50000: episode: 718, duration: 0.263s, episode steps: 123, steps per second: 467, episode reward: 123.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.431 [0.000, 1.000], mean observation: -0.455 [-3.118, 0.731], loss: 0.664089, mean_absolute_error: 21.004791, mean_q: 43.936214\n",
      " 33881/50000: episode: 719, duration: 0.236s, episode steps: 119, steps per second: 505, episode reward: 119.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.445 [0.000, 1.000], mean observation: -0.427 [-2.397, 0.549], loss: 1.009672, mean_absolute_error: 21.105814, mean_q: 43.865402\n",
      " 34025/50000: episode: 720, duration: 0.298s, episode steps: 144, steps per second: 483, episode reward: 144.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.377 [-2.612, 0.583], loss: 0.676755, mean_absolute_error: 20.818827, mean_q: 43.523235\n",
      " 34132/50000: episode: 721, duration: 0.223s, episode steps: 107, steps per second: 480, episode reward: 107.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.449 [0.000, 1.000], mean observation: -0.463 [-2.361, 0.460], loss: 0.677336, mean_absolute_error: 20.923687, mean_q: 43.772530\n",
      " 34244/50000: episode: 722, duration: 0.231s, episode steps: 112, steps per second: 485, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.402 [0.000, 1.000], mean observation: -0.468 [-4.056, 2.194], loss: 0.460398, mean_absolute_error: 20.582199, mean_q: 43.186104\n",
      " 34358/50000: episode: 723, duration: 0.232s, episode steps: 114, steps per second: 492, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.442 [-2.577, 0.595], loss: 0.458674, mean_absolute_error: 20.634985, mean_q: 43.121883\n",
      " 34496/50000: episode: 724, duration: 0.286s, episode steps: 138, steps per second: 483, episode reward: 138.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.362 [-2.598, 0.551], loss: 0.475276, mean_absolute_error: 20.867022, mean_q: 43.655483\n",
      " 34620/50000: episode: 725, duration: 0.255s, episode steps: 124, steps per second: 487, episode reward: 124.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.411 [0.000, 1.000], mean observation: -0.415 [-4.159, 2.588], loss: 0.956654, mean_absolute_error: 20.528484, mean_q: 43.080803\n",
      " 34734/50000: episode: 726, duration: 0.240s, episode steps: 114, steps per second: 474, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: -0.485 [-3.736, 1.628], loss: 0.465199, mean_absolute_error: 20.985199, mean_q: 43.878311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 34848/50000: episode: 727, duration: 0.294s, episode steps: 114, steps per second: 388, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.466 [-2.709, 0.596], loss: 0.810673, mean_absolute_error: 20.760344, mean_q: 43.471405\n",
      " 34973/50000: episode: 728, duration: 0.256s, episode steps: 125, steps per second: 487, episode reward: 125.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.422 [-2.781, 0.641], loss: 0.266628, mean_absolute_error: 21.123447, mean_q: 44.144131\n",
      " 35105/50000: episode: 729, duration: 0.288s, episode steps: 132, steps per second: 458, episode reward: 132.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.371 [-2.565, 0.603], loss: 0.446112, mean_absolute_error: 20.860273, mean_q: 43.712959\n",
      " 35240/50000: episode: 730, duration: 0.275s, episode steps: 135, steps per second: 490, episode reward: 135.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.390 [-2.735, 0.663], loss: 0.638603, mean_absolute_error: 20.811584, mean_q: 43.495152\n",
      " 35379/50000: episode: 731, duration: 0.315s, episode steps: 139, steps per second: 441, episode reward: 139.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.432 [0.000, 1.000], mean observation: -0.395 [-3.555, 1.310], loss: 0.339638, mean_absolute_error: 20.735741, mean_q: 43.461781\n",
      " 35528/50000: episode: 732, duration: 0.327s, episode steps: 149, steps per second: 455, episode reward: 149.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.373 [-2.908, 0.764], loss: 0.589634, mean_absolute_error: 21.173714, mean_q: 44.158104\n",
      " 35645/50000: episode: 733, duration: 0.254s, episode steps: 117, steps per second: 461, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.416 [-2.597, 0.630], loss: 0.537922, mean_absolute_error: 20.649057, mean_q: 43.130909\n",
      " 35772/50000: episode: 734, duration: 0.250s, episode steps: 127, steps per second: 508, episode reward: 127.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.412 [-2.560, 0.654], loss: 0.478956, mean_absolute_error: 21.218790, mean_q: 44.247810\n",
      " 35906/50000: episode: 735, duration: 0.288s, episode steps: 134, steps per second: 465, episode reward: 134.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.407 [-2.709, 0.604], loss: 0.812639, mean_absolute_error: 21.246386, mean_q: 44.214245\n",
      " 36016/50000: episode: 736, duration: 0.259s, episode steps: 110, steps per second: 424, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.409 [0.000, 1.000], mean observation: -0.499 [-3.735, 1.603], loss: 0.264973, mean_absolute_error: 20.909500, mean_q: 43.755665\n",
      " 36138/50000: episode: 737, duration: 0.245s, episode steps: 122, steps per second: 499, episode reward: 122.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.464 [-2.792, 0.576], loss: 0.421726, mean_absolute_error: 20.932413, mean_q: 43.846470\n",
      " 36274/50000: episode: 738, duration: 0.288s, episode steps: 136, steps per second: 472, episode reward: 136.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.449 [0.000, 1.000], mean observation: -0.374 [-2.542, 0.568], loss: 0.503322, mean_absolute_error: 21.158703, mean_q: 44.182079\n",
      " 36406/50000: episode: 739, duration: 0.275s, episode steps: 132, steps per second: 479, episode reward: 132.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.343 [-2.384, 0.531], loss: 0.425607, mean_absolute_error: 21.215883, mean_q: 44.162598\n",
      " 36529/50000: episode: 740, duration: 0.275s, episode steps: 123, steps per second: 447, episode reward: 123.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.447 [-2.753, 0.635], loss: 0.577237, mean_absolute_error: 20.986637, mean_q: 43.810337\n",
      " 36649/50000: episode: 741, duration: 0.238s, episode steps: 120, steps per second: 504, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.407 [-2.565, 0.523], loss: 0.528568, mean_absolute_error: 21.083336, mean_q: 44.032402\n",
      " 36770/50000: episode: 742, duration: 0.270s, episode steps: 121, steps per second: 449, episode reward: 121.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: -0.447 [-3.548, 1.374], loss: 0.597281, mean_absolute_error: 20.937229, mean_q: 43.571003\n",
      " 36898/50000: episode: 743, duration: 0.271s, episode steps: 128, steps per second: 472, episode reward: 128.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.461 [0.000, 1.000], mean observation: -0.439 [-2.737, 0.665], loss: 0.578700, mean_absolute_error: 21.283325, mean_q: 44.328255\n",
      " 37034/50000: episode: 744, duration: 0.289s, episode steps: 136, steps per second: 471, episode reward: 136.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.426 [0.000, 1.000], mean observation: -0.407 [-3.724, 1.520], loss: 0.427922, mean_absolute_error: 21.208168, mean_q: 44.105274\n",
      " 37138/50000: episode: 745, duration: 0.195s, episode steps: 104, steps per second: 534, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: -0.399 [-2.217, 0.473], loss: 0.207794, mean_absolute_error: 21.214809, mean_q: 44.107361\n",
      " 37266/50000: episode: 746, duration: 0.264s, episode steps: 128, steps per second: 485, episode reward: 128.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.461 [0.000, 1.000], mean observation: -0.432 [-2.898, 0.700], loss: 0.340049, mean_absolute_error: 20.858007, mean_q: 43.387989\n",
      " 37398/50000: episode: 747, duration: 0.277s, episode steps: 132, steps per second: 477, episode reward: 132.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.427 [-2.750, 0.657], loss: 0.429217, mean_absolute_error: 21.503252, mean_q: 44.810966\n",
      " 37516/50000: episode: 748, duration: 0.234s, episode steps: 118, steps per second: 503, episode reward: 118.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.449 [0.000, 1.000], mean observation: -0.474 [-2.710, 0.627], loss: 0.392218, mean_absolute_error: 21.394566, mean_q: 44.411907\n",
      " 37633/50000: episode: 749, duration: 0.257s, episode steps: 117, steps per second: 456, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.419 [0.000, 1.000], mean observation: -0.458 [-3.521, 1.537], loss: 0.590483, mean_absolute_error: 21.383257, mean_q: 44.410385\n",
      " 37756/50000: episode: 750, duration: 0.263s, episode steps: 123, steps per second: 468, episode reward: 123.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.467 [-2.721, 0.539], loss: 0.252557, mean_absolute_error: 21.236073, mean_q: 44.167950\n",
      " 37856/50000: episode: 751, duration: 0.205s, episode steps: 100, steps per second: 488, episode reward: 100.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.395 [-2.050, 0.393], loss: 0.645027, mean_absolute_error: 21.120325, mean_q: 43.972572\n",
      " 37991/50000: episode: 752, duration: 0.311s, episode steps: 135, steps per second: 434, episode reward: 135.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.430 [0.000, 1.000], mean observation: -0.418 [-3.467, 1.299], loss: 0.851569, mean_absolute_error: 21.355759, mean_q: 44.480209\n",
      " 38113/50000: episode: 753, duration: 0.243s, episode steps: 122, steps per second: 502, episode reward: 122.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.418 [0.000, 1.000], mean observation: -0.449 [-3.699, 1.663], loss: 0.602049, mean_absolute_error: 21.258276, mean_q: 44.414375\n",
      " 38240/50000: episode: 754, duration: 0.287s, episode steps: 127, steps per second: 443, episode reward: 127.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.423 [-2.614, 0.600], loss: 0.538702, mean_absolute_error: 21.204876, mean_q: 44.413940\n",
      " 38357/50000: episode: 755, duration: 0.242s, episode steps: 117, steps per second: 483, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.493 [-2.613, 0.540], loss: 0.426833, mean_absolute_error: 21.244505, mean_q: 44.441124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 38460/50000: episode: 756, duration: 0.219s, episode steps: 103, steps per second: 471, episode reward: 103.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.437 [0.000, 1.000], mean observation: -0.461 [-2.397, 0.520], loss: 0.568909, mean_absolute_error: 21.193327, mean_q: 44.318810\n",
      " 38570/50000: episode: 757, duration: 0.237s, episode steps: 110, steps per second: 465, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.509 [-2.790, 0.607], loss: 0.581715, mean_absolute_error: 21.447126, mean_q: 44.708195\n",
      " 38708/50000: episode: 758, duration: 0.273s, episode steps: 138, steps per second: 506, episode reward: 138.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.428 [0.000, 1.000], mean observation: -0.402 [-3.745, 1.729], loss: 0.487327, mean_absolute_error: 21.098536, mean_q: 43.867142\n",
      " 38859/50000: episode: 759, duration: 0.328s, episode steps: 151, steps per second: 461, episode reward: 151.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.384 [-2.754, 0.662], loss: 0.540510, mean_absolute_error: 21.176237, mean_q: 44.135639\n",
      " 38980/50000: episode: 760, duration: 0.259s, episode steps: 121, steps per second: 468, episode reward: 121.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.418 [-2.595, 0.602], loss: 0.546979, mean_absolute_error: 21.135323, mean_q: 44.021122\n",
      " 39095/50000: episode: 761, duration: 0.247s, episode steps: 115, steps per second: 465, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.487 [-2.584, 0.546], loss: 0.500412, mean_absolute_error: 21.087290, mean_q: 44.005005\n",
      " 39233/50000: episode: 762, duration: 0.275s, episode steps: 138, steps per second: 501, episode reward: 138.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.413 [-2.713, 0.697], loss: 0.398969, mean_absolute_error: 21.280888, mean_q: 44.365406\n",
      " 39347/50000: episode: 763, duration: 0.241s, episode steps: 114, steps per second: 472, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.479 [-2.380, 0.584], loss: 0.517817, mean_absolute_error: 21.191658, mean_q: 44.153702\n",
      " 39465/50000: episode: 764, duration: 0.236s, episode steps: 118, steps per second: 499, episode reward: 118.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.438 [-2.343, 0.464], loss: 0.637511, mean_absolute_error: 21.421370, mean_q: 44.396332\n",
      " 39584/50000: episode: 765, duration: 0.248s, episode steps: 119, steps per second: 480, episode reward: 119.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.448 [-2.338, 0.487], loss: 0.366033, mean_absolute_error: 21.440424, mean_q: 44.567993\n",
      " 39698/50000: episode: 766, duration: 0.224s, episode steps: 114, steps per second: 510, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: -0.481 [-3.334, 1.302], loss: 0.550336, mean_absolute_error: 21.635712, mean_q: 44.793861\n",
      " 39819/50000: episode: 767, duration: 0.257s, episode steps: 121, steps per second: 471, episode reward: 121.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.446 [0.000, 1.000], mean observation: -0.459 [-2.562, 0.525], loss: 0.283864, mean_absolute_error: 21.405910, mean_q: 44.264175\n",
      " 39944/50000: episode: 768, duration: 0.253s, episode steps: 125, steps per second: 494, episode reward: 125.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.458 [-2.604, 0.553], loss: 0.496143, mean_absolute_error: 21.478008, mean_q: 44.681866\n",
      " 40068/50000: episode: 769, duration: 0.260s, episode steps: 124, steps per second: 476, episode reward: 124.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.430 [-2.353, 0.428], loss: 0.605825, mean_absolute_error: 21.793905, mean_q: 45.208233\n",
      " 40220/50000: episode: 770, duration: 0.298s, episode steps: 152, steps per second: 510, episode reward: 152.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.461 [0.000, 1.000], mean observation: -0.363 [-2.746, 0.716], loss: 0.318524, mean_absolute_error: 21.734301, mean_q: 45.128204\n",
      " 40345/50000: episode: 771, duration: 0.261s, episode steps: 125, steps per second: 479, episode reward: 125.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: -0.448 [-2.607, 0.528], loss: 0.416965, mean_absolute_error: 21.590872, mean_q: 44.620659\n",
      " 40481/50000: episode: 772, duration: 0.282s, episode steps: 136, steps per second: 482, episode reward: 136.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.441 [0.000, 1.000], mean observation: -0.423 [-2.947, 0.588], loss: 0.561160, mean_absolute_error: 21.699192, mean_q: 44.802547\n",
      " 40615/50000: episode: 773, duration: 0.297s, episode steps: 134, steps per second: 451, episode reward: 134.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.416 [-2.422, 0.480], loss: 0.924393, mean_absolute_error: 21.227072, mean_q: 43.962124\n",
      " 40743/50000: episode: 774, duration: 0.268s, episode steps: 128, steps per second: 478, episode reward: 128.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.445 [0.000, 1.000], mean observation: -0.433 [-2.736, 0.551], loss: 0.334195, mean_absolute_error: 21.864233, mean_q: 45.384087\n",
      " 40890/50000: episode: 775, duration: 0.307s, episode steps: 147, steps per second: 479, episode reward: 147.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.449 [0.000, 1.000], mean observation: -0.384 [-2.699, 0.487], loss: 0.552369, mean_absolute_error: 21.532501, mean_q: 44.604107\n",
      " 41090/50000: episode: 776, duration: 0.417s, episode steps: 200, steps per second: 480, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.110 [-1.854, 0.363], loss: 0.472446, mean_absolute_error: 21.651030, mean_q: 44.935635\n",
      " 41221/50000: episode: 777, duration: 0.288s, episode steps: 131, steps per second: 454, episode reward: 131.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.415 [-2.416, 0.536], loss: 1.170722, mean_absolute_error: 21.943087, mean_q: 45.404415\n",
      " 41348/50000: episode: 778, duration: 0.271s, episode steps: 127, steps per second: 469, episode reward: 127.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.398 [-2.216, 0.420], loss: 0.353347, mean_absolute_error: 21.904106, mean_q: 45.309383\n",
      " 41479/50000: episode: 779, duration: 0.274s, episode steps: 131, steps per second: 478, episode reward: 131.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.443 [0.000, 1.000], mean observation: -0.431 [-2.793, 0.517], loss: 0.624093, mean_absolute_error: 21.990519, mean_q: 45.630322\n",
      " 41590/50000: episode: 780, duration: 0.223s, episode steps: 111, steps per second: 497, episode reward: 111.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.441 [0.000, 1.000], mean observation: -0.487 [-2.609, 0.551], loss: 0.727840, mean_absolute_error: 21.429012, mean_q: 44.556984\n",
      " 41704/50000: episode: 781, duration: 0.261s, episode steps: 114, steps per second: 436, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.483 [-2.596, 0.413], loss: 0.472637, mean_absolute_error: 21.627068, mean_q: 44.946213\n",
      " 41829/50000: episode: 782, duration: 0.281s, episode steps: 125, steps per second: 445, episode reward: 125.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: -0.439 [-2.732, 0.523], loss: 0.601098, mean_absolute_error: 21.793978, mean_q: 45.437435\n",
      " 41934/50000: episode: 783, duration: 0.245s, episode steps: 105, steps per second: 429, episode reward: 105.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.489 [-2.395, 0.383], loss: 0.230391, mean_absolute_error: 21.799038, mean_q: 45.222023\n",
      " 42054/50000: episode: 784, duration: 0.241s, episode steps: 120, steps per second: 497, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.471 [-2.400, 0.395], loss: 0.371928, mean_absolute_error: 21.967695, mean_q: 45.688663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 42185/50000: episode: 785, duration: 0.272s, episode steps: 131, steps per second: 481, episode reward: 131.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.416 [-2.403, 0.434], loss: 0.516194, mean_absolute_error: 21.918652, mean_q: 45.452831\n",
      " 42317/50000: episode: 786, duration: 0.329s, episode steps: 132, steps per second: 402, episode reward: 132.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.413 [-2.603, 0.526], loss: 0.248688, mean_absolute_error: 21.856146, mean_q: 45.482384\n",
      " 42443/50000: episode: 787, duration: 0.253s, episode steps: 126, steps per second: 499, episode reward: 126.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.422 [-2.335, 0.434], loss: 0.213520, mean_absolute_error: 21.899530, mean_q: 45.459278\n",
      " 42552/50000: episode: 788, duration: 0.223s, episode steps: 109, steps per second: 488, episode reward: 109.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: -0.488 [-2.571, 0.486], loss: 0.194651, mean_absolute_error: 22.034098, mean_q: 45.507122\n",
      " 42671/50000: episode: 789, duration: 0.246s, episode steps: 119, steps per second: 483, episode reward: 119.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.445 [0.000, 1.000], mean observation: -0.462 [-2.425, 0.386], loss: 0.371022, mean_absolute_error: 22.105606, mean_q: 45.693264\n",
      " 42801/50000: episode: 790, duration: 0.256s, episode steps: 130, steps per second: 508, episode reward: 130.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.446 [0.000, 1.000], mean observation: -0.422 [-2.544, 0.427], loss: 0.244636, mean_absolute_error: 22.101818, mean_q: 45.725197\n",
      " 42952/50000: episode: 791, duration: 0.306s, episode steps: 151, steps per second: 494, episode reward: 151.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.359 [-2.529, 0.623], loss: 0.778544, mean_absolute_error: 21.835232, mean_q: 45.185444\n",
      " 43081/50000: episode: 792, duration: 0.248s, episode steps: 129, steps per second: 519, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.437 [-2.431, 0.378], loss: 0.345648, mean_absolute_error: 21.826534, mean_q: 45.202061\n",
      " 43215/50000: episode: 793, duration: 0.280s, episode steps: 134, steps per second: 478, episode reward: 134.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.367 [-2.187, 0.426], loss: 0.745415, mean_absolute_error: 22.024166, mean_q: 45.690338\n",
      " 43356/50000: episode: 794, duration: 0.278s, episode steps: 141, steps per second: 507, episode reward: 141.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.454 [0.000, 1.000], mean observation: -0.374 [-2.315, 0.431], loss: 0.358110, mean_absolute_error: 22.059332, mean_q: 45.720215\n",
      " 43487/50000: episode: 795, duration: 0.280s, episode steps: 131, steps per second: 469, episode reward: 131.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.443 [0.000, 1.000], mean observation: -0.441 [-2.765, 0.480], loss: 0.503076, mean_absolute_error: 21.983450, mean_q: 45.644165\n",
      " 43631/50000: episode: 796, duration: 0.290s, episode steps: 144, steps per second: 497, episode reward: 144.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.391 [-2.423, 0.497], loss: 0.731660, mean_absolute_error: 22.224495, mean_q: 45.993034\n",
      " 43754/50000: episode: 797, duration: 0.252s, episode steps: 123, steps per second: 488, episode reward: 123.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.446 [-2.435, 0.425], loss: 0.636515, mean_absolute_error: 22.112179, mean_q: 45.667828\n",
      " 43874/50000: episode: 798, duration: 0.297s, episode steps: 120, steps per second: 403, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: -0.461 [-2.758, 0.652], loss: 0.437318, mean_absolute_error: 22.564369, mean_q: 46.669075\n",
      " 44027/50000: episode: 799, duration: 0.316s, episode steps: 153, steps per second: 484, episode reward: 153.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.358 [-2.419, 0.450], loss: 0.481628, mean_absolute_error: 22.096275, mean_q: 45.667210\n",
      " 44142/50000: episode: 800, duration: 0.278s, episode steps: 115, steps per second: 413, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.461 [0.000, 1.000], mean observation: -0.481 [-2.408, 0.530], loss: 0.499451, mean_absolute_error: 22.481260, mean_q: 46.371162\n",
      " 44298/50000: episode: 801, duration: 0.320s, episode steps: 156, steps per second: 487, episode reward: 156.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.354 [-2.416, 0.435], loss: 0.609879, mean_absolute_error: 22.377815, mean_q: 46.077095\n",
      " 44422/50000: episode: 802, duration: 0.242s, episode steps: 124, steps per second: 512, episode reward: 124.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.447 [-2.436, 0.457], loss: 0.503503, mean_absolute_error: 22.615568, mean_q: 46.497650\n",
      " 44559/50000: episode: 803, duration: 0.298s, episode steps: 137, steps per second: 460, episode reward: 137.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.398 [-2.422, 0.482], loss: 0.399076, mean_absolute_error: 22.446898, mean_q: 46.108883\n",
      " 44679/50000: episode: 804, duration: 0.253s, episode steps: 120, steps per second: 474, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: -0.464 [-2.599, 0.380], loss: 0.332015, mean_absolute_error: 22.974825, mean_q: 47.055294\n",
      " 44792/50000: episode: 805, duration: 0.225s, episode steps: 113, steps per second: 502, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: -0.482 [-2.433, 0.424], loss: 0.519378, mean_absolute_error: 22.703228, mean_q: 46.536762\n",
      " 44917/50000: episode: 806, duration: 0.268s, episode steps: 125, steps per second: 466, episode reward: 125.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: -0.437 [-2.526, 0.399], loss: 0.386365, mean_absolute_error: 22.817802, mean_q: 46.641537\n",
      " 45034/50000: episode: 807, duration: 0.235s, episode steps: 117, steps per second: 497, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.468 [-2.518, 0.380], loss: 0.674957, mean_absolute_error: 23.005276, mean_q: 47.117489\n",
      " 45172/50000: episode: 808, duration: 0.292s, episode steps: 138, steps per second: 473, episode reward: 138.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.406 [-2.443, 0.397], loss: 0.393605, mean_absolute_error: 22.861332, mean_q: 46.748848\n",
      " 45351/50000: episode: 809, duration: 0.344s, episode steps: 179, steps per second: 520, episode reward: 179.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.317 [-2.437, 0.526], loss: 0.471447, mean_absolute_error: 22.874250, mean_q: 46.795235\n",
      " 45551/50000: episode: 810, duration: 0.432s, episode steps: 200, steps per second: 463, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: 0.054 [-0.729, 0.529], loss: 0.623720, mean_absolute_error: 22.985567, mean_q: 47.000050\n",
      " 45700/50000: episode: 811, duration: 0.326s, episode steps: 149, steps per second: 458, episode reward: 149.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.377 [-2.371, 0.424], loss: 0.334601, mean_absolute_error: 22.964096, mean_q: 47.035728\n",
      " 45832/50000: episode: 812, duration: 0.302s, episode steps: 132, steps per second: 437, episode reward: 132.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.421 [-2.521, 0.392], loss: 0.481392, mean_absolute_error: 23.026966, mean_q: 47.078896\n",
      " 46032/50000: episode: 813, duration: 0.404s, episode steps: 200, steps per second: 495, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.076 [-0.417, 0.501], loss: 0.667891, mean_absolute_error: 23.098503, mean_q: 47.187790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 46232/50000: episode: 814, duration: 0.436s, episode steps: 200, steps per second: 459, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.038 [-0.770, 0.352], loss: 0.504121, mean_absolute_error: 23.179398, mean_q: 47.323368\n",
      " 46390/50000: episode: 815, duration: 0.328s, episode steps: 158, steps per second: 481, episode reward: 158.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.358 [-2.436, 0.432], loss: 0.405849, mean_absolute_error: 23.438757, mean_q: 47.794392\n",
      " 46590/50000: episode: 816, duration: 0.422s, episode steps: 200, steps per second: 474, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.261 [-2.391, 0.463], loss: 0.935883, mean_absolute_error: 23.413336, mean_q: 47.639725\n",
      " 46718/50000: episode: 817, duration: 0.269s, episode steps: 128, steps per second: 475, episode reward: 128.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.415 [-2.222, 0.447], loss: 0.478900, mean_absolute_error: 23.958157, mean_q: 48.647530\n",
      " 46849/50000: episode: 818, duration: 0.275s, episode steps: 131, steps per second: 477, episode reward: 131.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.429 [-2.552, 0.434], loss: 0.573805, mean_absolute_error: 23.451313, mean_q: 47.580860\n",
      " 46971/50000: episode: 819, duration: 0.246s, episode steps: 122, steps per second: 496, episode reward: 122.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.443 [0.000, 1.000], mean observation: -0.453 [-2.769, 0.600], loss: 0.759056, mean_absolute_error: 23.642376, mean_q: 47.946751\n",
      " 47171/50000: episode: 820, duration: 0.442s, episode steps: 200, steps per second: 453, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.063 [-0.870, 1.138], loss: 0.565788, mean_absolute_error: 23.655254, mean_q: 47.996647\n",
      " 47329/50000: episode: 821, duration: 0.305s, episode steps: 158, steps per second: 518, episode reward: 158.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.356 [-2.548, 0.396], loss: 0.793922, mean_absolute_error: 23.641506, mean_q: 48.011143\n",
      " 47500/50000: episode: 822, duration: 0.361s, episode steps: 171, steps per second: 474, episode reward: 171.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.332 [-2.588, 0.371], loss: 0.358177, mean_absolute_error: 23.701828, mean_q: 48.138420\n",
      " 47658/50000: episode: 823, duration: 0.340s, episode steps: 158, steps per second: 465, episode reward: 158.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.354 [-2.741, 0.512], loss: 0.369134, mean_absolute_error: 23.645359, mean_q: 47.966957\n",
      " 47795/50000: episode: 824, duration: 0.298s, episode steps: 137, steps per second: 460, episode reward: 137.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.445 [0.000, 1.000], mean observation: -0.406 [-2.744, 0.588], loss: 0.216790, mean_absolute_error: 24.020159, mean_q: 48.692524\n",
      " 47949/50000: episode: 825, duration: 0.358s, episode steps: 154, steps per second: 430, episode reward: 154.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.363 [-2.588, 0.366], loss: 0.747492, mean_absolute_error: 24.080870, mean_q: 48.798203\n",
      " 48070/50000: episode: 826, duration: 0.238s, episode steps: 121, steps per second: 508, episode reward: 121.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.468 [-2.945, 0.577], loss: 0.317921, mean_absolute_error: 23.978392, mean_q: 48.525757\n",
      " 48201/50000: episode: 827, duration: 0.278s, episode steps: 131, steps per second: 471, episode reward: 131.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.427 [-2.421, 0.450], loss: 0.691332, mean_absolute_error: 24.148378, mean_q: 48.942875\n",
      " 48333/50000: episode: 828, duration: 0.278s, episode steps: 132, steps per second: 475, episode reward: 132.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.418 [-2.585, 0.417], loss: 0.514545, mean_absolute_error: 23.904163, mean_q: 48.467819\n",
      " 48472/50000: episode: 829, duration: 0.296s, episode steps: 139, steps per second: 469, episode reward: 139.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.405 [-2.587, 0.531], loss: 0.969157, mean_absolute_error: 24.055637, mean_q: 48.736702\n",
      " 48608/50000: episode: 830, duration: 0.284s, episode steps: 136, steps per second: 479, episode reward: 136.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.449 [0.000, 1.000], mean observation: -0.417 [-2.574, 0.409], loss: 0.465475, mean_absolute_error: 24.133654, mean_q: 48.956593\n",
      " 48724/50000: episode: 831, duration: 0.245s, episode steps: 116, steps per second: 473, episode reward: 116.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: -0.474 [-2.581, 0.374], loss: 0.262229, mean_absolute_error: 24.217518, mean_q: 49.099041\n",
      " 48861/50000: episode: 832, duration: 0.285s, episode steps: 137, steps per second: 480, episode reward: 137.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.405 [-2.563, 0.506], loss: 0.456457, mean_absolute_error: 24.170895, mean_q: 48.956985\n",
      " 48964/50000: episode: 833, duration: 0.242s, episode steps: 103, steps per second: 426, episode reward: 103.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.329 [-1.996, 0.276], loss: 0.344861, mean_absolute_error: 24.007391, mean_q: 48.539238\n",
      " 49099/50000: episode: 834, duration: 0.299s, episode steps: 135, steps per second: 451, episode reward: 135.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.352 [-2.429, 0.609], loss: 1.510778, mean_absolute_error: 24.130093, mean_q: 48.828972\n",
      " 49267/50000: episode: 835, duration: 0.337s, episode steps: 168, steps per second: 498, episode reward: 168.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.332 [-2.437, 0.600], loss: 0.598230, mean_absolute_error: 24.293522, mean_q: 49.171730\n",
      " 49412/50000: episode: 836, duration: 0.314s, episode steps: 145, steps per second: 462, episode reward: 145.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: -0.387 [-2.922, 0.654], loss: 0.389513, mean_absolute_error: 24.627069, mean_q: 49.930103\n",
      " 49560/50000: episode: 837, duration: 0.304s, episode steps: 148, steps per second: 487, episode reward: 148.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.391 [-0.955, 2.817], loss: 0.388798, mean_absolute_error: 24.557987, mean_q: 49.678501\n",
      " 49760/50000: episode: 838, duration: 0.423s, episode steps: 200, steps per second: 473, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.285 [-0.930, 2.993], loss: 0.852529, mean_absolute_error: 24.209953, mean_q: 48.921959\n",
      " 49927/50000: episode: 839, duration: 0.361s, episode steps: 167, steps per second: 462, episode reward: 167.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.449 [0.000, 1.000], mean observation: -0.325 [-3.111, 0.815], loss: 0.842333, mean_absolute_error: 24.300631, mean_q: 49.053272\n",
      "done, took 111.096 seconds\n"
     ]
    }
   ],
   "source": [
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "history = dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 123.000, steps: 123\n",
      "Episode 2: reward: 121.000, steps: 121\n",
      "Episode 3: reward: 200.000, steps: 200\n",
      "Episode 4: reward: 149.000, steps: 149\n",
      "Episode 5: reward: 138.000, steps: 138\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "dqn.test(env, nb_episodes=5, visualize=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()\n",
    "y = history.history['episode_reward']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title('DQN_rewards')\n",
    "ax.set_xlabel('episodes')\n",
    "ax.set_ylabel('rewards')\n",
    "ax.plot(y)\n",
    "plt.savefig('CartPole_DQN_rewards')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
